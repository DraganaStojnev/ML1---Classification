{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install pandas\n",
    "# !{sys.executable} -m pip install numpy\n",
    "# !{sys.executable} -m pip install matplotlib\n",
    "# !{sys.executable} -m pip install sklearn\n",
    "# !{sys.executable} -m pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importovanje potrebnih paketa za rad sa algoritmima nadgledanog učenja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split, KFold, RepeatedKFold\n",
    "from sklearn.linear_model import LogisticRegression, LassoCV, Ridge\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score, classification_report,r2_score, mean_squared_error, roc_curve, auc\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.stats import iqr\n",
    "from sklearn.feature_selection import RFE, SelectFromModel\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Učitavanje dataset-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>label_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...         0.00        0.000   \n",
       "1             0.00            0.94  ...         0.00        0.132   \n",
       "2             0.64            0.25  ...         0.01        0.143   \n",
       "3             0.31            0.63  ...         0.00        0.137   \n",
       "4             0.31            0.63  ...         0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "3          0.0        0.137        0.000        0.000   \n",
       "4          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  label_spam  \n",
       "0                       278           1  \n",
       "1                      1028           1  \n",
       "2                      2259           1  \n",
       "3                       191           1  \n",
       "4                       191           1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"dataset.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deskriptivna analiza dataset-a\n",
    "### - MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_freq_make                  0.104553\n",
       "word_freq_address               0.213015\n",
       "word_freq_all                   0.280656\n",
       "word_freq_3d                    0.065425\n",
       "word_freq_our                   0.312223\n",
       "word_freq_over                  0.095901\n",
       "word_freq_remove                0.114208\n",
       "word_freq_internet              0.105295\n",
       "word_freq_order                 0.090067\n",
       "word_freq_mail                  0.239413\n",
       "word_freq_receive               0.059824\n",
       "word_freq_will                  0.541702\n",
       "word_freq_people                0.093930\n",
       "word_freq_report                0.058626\n",
       "word_freq_addresses             0.049205\n",
       "word_freq_free                  0.248848\n",
       "word_freq_business              0.142586\n",
       "word_freq_email                 0.184745\n",
       "word_freq_you                   1.662100\n",
       "word_freq_credit                0.085577\n",
       "word_freq_your                  0.809761\n",
       "word_freq_font                  0.121202\n",
       "word_freq_000                   0.101645\n",
       "word_freq_money                 0.094269\n",
       "word_freq_hp                    0.549504\n",
       "word_freq_hpl                   0.265384\n",
       "word_freq_george                0.767305\n",
       "word_freq_650                   0.124845\n",
       "word_freq_lab                   0.098915\n",
       "word_freq_labs                  0.102852\n",
       "word_freq_telnet                0.064753\n",
       "word_freq_857                   0.047048\n",
       "word_freq_data                  0.097229\n",
       "word_freq_415                   0.047835\n",
       "word_freq_85                    0.105412\n",
       "word_freq_technology            0.097477\n",
       "word_freq_1999                  0.136953\n",
       "word_freq_parts                 0.013201\n",
       "word_freq_pm                    0.078629\n",
       "word_freq_direct                0.064834\n",
       "word_freq_cs                    0.043667\n",
       "word_freq_meeting               0.132339\n",
       "word_freq_original              0.046099\n",
       "word_freq_project               0.079196\n",
       "word_freq_re                    0.301224\n",
       "word_freq_edu                   0.179824\n",
       "word_freq_table                 0.005444\n",
       "word_freq_conference            0.031869\n",
       "char_freq_;                     0.038575\n",
       "char_freq_(                     0.139030\n",
       "char_freq_[                     0.016976\n",
       "char_freq_!                     0.269071\n",
       "char_freq_$                     0.075811\n",
       "char_freq_#                     0.044238\n",
       "capital_run_length_average      5.191515\n",
       "capital_run_length_longest     52.172789\n",
       "capital_run_length_total      283.289285\n",
       "label_spam                      0.394045\n",
       "dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mean\n",
    "dataset.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1e7993b40c8>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = np.zeros((25))\n",
    "plt.scatter(random.sample(list(dataset['capital_run_length_longest']),25), y)\n",
    "plt.scatter(dataset['capital_run_length_longest'].mean(),0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - MEDIAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_freq_make                 0.000\n",
       "word_freq_address              0.000\n",
       "word_freq_all                  0.000\n",
       "word_freq_3d                   0.000\n",
       "word_freq_our                  0.000\n",
       "word_freq_over                 0.000\n",
       "word_freq_remove               0.000\n",
       "word_freq_internet             0.000\n",
       "word_freq_order                0.000\n",
       "word_freq_mail                 0.000\n",
       "word_freq_receive              0.000\n",
       "word_freq_will                 0.100\n",
       "word_freq_people               0.000\n",
       "word_freq_report               0.000\n",
       "word_freq_addresses            0.000\n",
       "word_freq_free                 0.000\n",
       "word_freq_business             0.000\n",
       "word_freq_email                0.000\n",
       "word_freq_you                  1.310\n",
       "word_freq_credit               0.000\n",
       "word_freq_your                 0.220\n",
       "word_freq_font                 0.000\n",
       "word_freq_000                  0.000\n",
       "word_freq_money                0.000\n",
       "word_freq_hp                   0.000\n",
       "word_freq_hpl                  0.000\n",
       "word_freq_george               0.000\n",
       "word_freq_650                  0.000\n",
       "word_freq_lab                  0.000\n",
       "word_freq_labs                 0.000\n",
       "word_freq_telnet               0.000\n",
       "word_freq_857                  0.000\n",
       "word_freq_data                 0.000\n",
       "word_freq_415                  0.000\n",
       "word_freq_85                   0.000\n",
       "word_freq_technology           0.000\n",
       "word_freq_1999                 0.000\n",
       "word_freq_parts                0.000\n",
       "word_freq_pm                   0.000\n",
       "word_freq_direct               0.000\n",
       "word_freq_cs                   0.000\n",
       "word_freq_meeting              0.000\n",
       "word_freq_original             0.000\n",
       "word_freq_project              0.000\n",
       "word_freq_re                   0.000\n",
       "word_freq_edu                  0.000\n",
       "word_freq_table                0.000\n",
       "word_freq_conference           0.000\n",
       "char_freq_;                    0.000\n",
       "char_freq_(                    0.065\n",
       "char_freq_[                    0.000\n",
       "char_freq_!                    0.000\n",
       "char_freq_$                    0.000\n",
       "char_freq_#                    0.000\n",
       "capital_run_length_average     2.276\n",
       "capital_run_length_longest    15.000\n",
       "capital_run_length_total      95.000\n",
       "label_spam                     0.000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Median\n",
    "dataset.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1e7991cca88>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = np.zeros((25))\n",
    "plt.scatter(random.sample(list(dataset['capital_run_length_total']),25), y)\n",
    "plt.scatter(dataset['capital_run_length_total'].median(),0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - MODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>label_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0             0.0                0.0            0.0           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0            0.0             0.0               0.0                 0.0   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0              0.0             0.0  ...          0.0          0.0   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                         1.0                           1   \n",
       "\n",
       "   capital_run_length_total  label_spam  \n",
       "0                         5           0  \n",
       "\n",
       "[1 rows x 58 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mode\n",
    "dataset.mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - STANDARD DEVIATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_freq_make                  0.305358\n",
       "word_freq_address               1.290575\n",
       "word_freq_all                   0.504143\n",
       "word_freq_3d                    1.395151\n",
       "word_freq_our                   0.672513\n",
       "word_freq_over                  0.273824\n",
       "word_freq_remove                0.391441\n",
       "word_freq_internet              0.401071\n",
       "word_freq_order                 0.278616\n",
       "word_freq_mail                  0.644755\n",
       "word_freq_receive               0.201545\n",
       "word_freq_will                  0.861698\n",
       "word_freq_people                0.301036\n",
       "word_freq_report                0.335184\n",
       "word_freq_addresses             0.258843\n",
       "word_freq_free                  0.825792\n",
       "word_freq_business              0.444055\n",
       "word_freq_email                 0.531122\n",
       "word_freq_you                   1.775481\n",
       "word_freq_credit                0.509767\n",
       "word_freq_your                  1.200810\n",
       "word_freq_font                  1.025756\n",
       "word_freq_000                   0.350286\n",
       "word_freq_money                 0.442636\n",
       "word_freq_hp                    1.671349\n",
       "word_freq_hpl                   0.886955\n",
       "word_freq_george                3.367292\n",
       "word_freq_650                   0.538576\n",
       "word_freq_lab                   0.593327\n",
       "word_freq_labs                  0.456682\n",
       "word_freq_telnet                0.403393\n",
       "word_freq_857                   0.328559\n",
       "word_freq_data                  0.555907\n",
       "word_freq_415                   0.329445\n",
       "word_freq_85                    0.532260\n",
       "word_freq_technology            0.402623\n",
       "word_freq_1999                  0.423451\n",
       "word_freq_parts                 0.220651\n",
       "word_freq_pm                    0.434672\n",
       "word_freq_direct                0.349916\n",
       "word_freq_cs                    0.361205\n",
       "word_freq_meeting               0.766819\n",
       "word_freq_original              0.223812\n",
       "word_freq_project               0.621976\n",
       "word_freq_re                    1.011687\n",
       "word_freq_edu                   0.911119\n",
       "word_freq_table                 0.076274\n",
       "word_freq_conference            0.285735\n",
       "char_freq_;                     0.243471\n",
       "char_freq_(                     0.270355\n",
       "char_freq_[                     0.109394\n",
       "char_freq_!                     0.815672\n",
       "char_freq_$                     0.245882\n",
       "char_freq_#                     0.429342\n",
       "capital_run_length_average     31.729449\n",
       "capital_run_length_longest    194.891310\n",
       "capital_run_length_total      606.347851\n",
       "label_spam                      0.488698\n",
       "dtype: float64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Standard deviation\n",
    "dataset.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - VARIANCE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_freq_make                     0.093243\n",
       "word_freq_address                  1.665584\n",
       "word_freq_all                      0.254160\n",
       "word_freq_3d                       1.946447\n",
       "word_freq_our                      0.452273\n",
       "word_freq_over                     0.074980\n",
       "word_freq_remove                   0.153226\n",
       "word_freq_internet                 0.160858\n",
       "word_freq_order                    0.077627\n",
       "word_freq_mail                     0.415710\n",
       "word_freq_receive                  0.040620\n",
       "word_freq_will                     0.742524\n",
       "word_freq_people                   0.090623\n",
       "word_freq_report                   0.112348\n",
       "word_freq_addresses                0.067000\n",
       "word_freq_free                     0.681932\n",
       "word_freq_business                 0.197185\n",
       "word_freq_email                    0.282091\n",
       "word_freq_you                      3.152332\n",
       "word_freq_credit                   0.259862\n",
       "word_freq_your                     1.441944\n",
       "word_freq_font                     1.052175\n",
       "word_freq_000                      0.122701\n",
       "word_freq_money                    0.195926\n",
       "word_freq_hp                       2.793409\n",
       "word_freq_hpl                      0.786690\n",
       "word_freq_george                  11.338654\n",
       "word_freq_650                      0.290064\n",
       "word_freq_lab                      0.352036\n",
       "word_freq_labs                     0.208558\n",
       "word_freq_telnet                   0.162726\n",
       "word_freq_857                      0.107951\n",
       "word_freq_data                     0.309033\n",
       "word_freq_415                      0.108534\n",
       "word_freq_85                       0.283301\n",
       "word_freq_technology               0.162105\n",
       "word_freq_1999                     0.179311\n",
       "word_freq_parts                    0.048687\n",
       "word_freq_pm                       0.188940\n",
       "word_freq_direct                   0.122441\n",
       "word_freq_cs                       0.130469\n",
       "word_freq_meeting                  0.588012\n",
       "word_freq_original                 0.050092\n",
       "word_freq_project                  0.386854\n",
       "word_freq_re                       1.023511\n",
       "word_freq_edu                      0.830138\n",
       "word_freq_table                    0.005818\n",
       "word_freq_conference               0.081644\n",
       "char_freq_;                        0.059278\n",
       "char_freq_(                        0.073092\n",
       "char_freq_[                        0.011967\n",
       "char_freq_!                        0.665320\n",
       "char_freq_$                        0.060458\n",
       "char_freq_#                        0.184335\n",
       "capital_run_length_average      1006.757917\n",
       "capital_run_length_longest     37982.622529\n",
       "capital_run_length_total      367657.716079\n",
       "label_spam                         0.238825\n",
       "dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variance\n",
    "dataset.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - INTERQUARTILE RANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_freq_make: 0.0\n",
      "word_freq_address: 0.0\n",
      "word_freq_all: 0.42\n",
      "word_freq_3d: 0.0\n",
      "word_freq_our: 0.38\n",
      "word_freq_over: 0.0\n",
      "word_freq_remove: 0.0\n",
      "word_freq_internet: 0.0\n",
      "word_freq_order: 0.0\n",
      "word_freq_mail: 0.16\n",
      "word_freq_receive: 0.0\n",
      "word_freq_will: 0.8\n",
      "word_freq_people: 0.0\n",
      "word_freq_report: 0.0\n",
      "word_freq_addresses: 0.0\n",
      "word_freq_free: 0.1\n",
      "word_freq_business: 0.0\n",
      "word_freq_email: 0.0\n",
      "word_freq_you: 2.64\n",
      "word_freq_credit: 0.0\n",
      "word_freq_your: 1.27\n",
      "word_freq_font: 0.0\n",
      "word_freq_000: 0.0\n",
      "word_freq_money: 0.0\n",
      "word_freq_hp: 0.0\n",
      "word_freq_hpl: 0.0\n",
      "word_freq_george: 0.0\n",
      "word_freq_650: 0.0\n",
      "word_freq_lab: 0.0\n",
      "word_freq_labs: 0.0\n",
      "word_freq_telnet: 0.0\n",
      "word_freq_857: 0.0\n",
      "word_freq_data: 0.0\n",
      "word_freq_415: 0.0\n",
      "word_freq_85: 0.0\n",
      "word_freq_technology: 0.0\n",
      "word_freq_1999: 0.0\n",
      "word_freq_parts: 0.0\n",
      "word_freq_pm: 0.0\n",
      "word_freq_direct: 0.0\n",
      "word_freq_cs: 0.0\n",
      "word_freq_meeting: 0.0\n",
      "word_freq_original: 0.0\n",
      "word_freq_project: 0.0\n",
      "word_freq_re: 0.11\n",
      "word_freq_edu: 0.0\n",
      "word_freq_table: 0.0\n",
      "word_freq_conference: 0.0\n",
      "char_freq_;: 0.0\n",
      "char_freq_(: 0.188\n",
      "char_freq_[: 0.0\n",
      "char_freq_!: 0.315\n",
      "char_freq_$: 0.052000000000000005\n",
      "char_freq_#: 0.0\n",
      "capital_run_length_average: 2.1180000000000003\n",
      "capital_run_length_longest: 37.0\n",
      "capital_run_length_total: 231.0\n",
      "label_spam: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Interquartile Range\n",
    "for column in dataset.columns:\n",
    "    print(column + \": \"+ str(iqr(dataset[column])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - SKEWNESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_freq_make                 5.675639\n",
       "word_freq_address             10.086811\n",
       "word_freq_all                  3.009249\n",
       "word_freq_3d                  26.227744\n",
       "word_freq_our                  4.747126\n",
       "word_freq_over                 5.956953\n",
       "word_freq_remove               6.765580\n",
       "word_freq_internet             9.724848\n",
       "word_freq_order                5.226067\n",
       "word_freq_mail                 8.487810\n",
       "word_freq_receive              5.510250\n",
       "word_freq_will                 2.867354\n",
       "word_freq_people               6.955548\n",
       "word_freq_report              11.754645\n",
       "word_freq_addresses            6.971041\n",
       "word_freq_free                10.763594\n",
       "word_freq_business             5.688642\n",
       "word_freq_email                5.413754\n",
       "word_freq_you                  1.591674\n",
       "word_freq_credit              14.602587\n",
       "word_freq_your                 2.435527\n",
       "word_freq_font                 9.975441\n",
       "word_freq_000                  5.713775\n",
       "word_freq_money               14.687028\n",
       "word_freq_hp                   5.716843\n",
       "word_freq_hpl                  6.350012\n",
       "word_freq_george               5.744493\n",
       "word_freq_650                  6.606534\n",
       "word_freq_lab                 11.370232\n",
       "word_freq_labs                 6.636015\n",
       "word_freq_telnet              12.669081\n",
       "word_freq_857                 10.549184\n",
       "word_freq_data                13.190056\n",
       "word_freq_415                 10.475181\n",
       "word_freq_85                  15.230811\n",
       "word_freq_technology           7.673461\n",
       "word_freq_1999                 5.323492\n",
       "word_freq_parts               28.263216\n",
       "word_freq_pm                  12.056912\n",
       "word_freq_direct               9.147029\n",
       "word_freq_cs                  12.587900\n",
       "word_freq_meeting              9.455755\n",
       "word_freq_original             7.629228\n",
       "word_freq_project             18.771515\n",
       "word_freq_re                   9.146093\n",
       "word_freq_edu                 10.122663\n",
       "word_freq_table               19.867691\n",
       "word_freq_conference          19.720446\n",
       "char_freq_;                   13.708621\n",
       "char_freq_(                   13.583755\n",
       "char_freq_[                   21.083545\n",
       "char_freq_!                   18.658004\n",
       "char_freq_$                   11.163141\n",
       "char_freq_#                   31.062064\n",
       "capital_run_length_average    23.761923\n",
       "capital_run_length_longest    30.764993\n",
       "capital_run_length_total       8.709850\n",
       "label_spam                     0.433811\n",
       "dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Skewness\n",
    "dataset.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - KURTOSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_freq_make                  49.305064\n",
       "word_freq_address              105.647472\n",
       "word_freq_all                   13.308743\n",
       "word_freq_3d                   726.451538\n",
       "word_freq_our                   37.941169\n",
       "word_freq_over                  68.445258\n",
       "word_freq_remove                75.413439\n",
       "word_freq_internet             169.162876\n",
       "word_freq_order                 46.940256\n",
       "word_freq_mail                 161.214641\n",
       "word_freq_receive               39.650945\n",
       "word_freq_will                  12.550747\n",
       "word_freq_people                84.941822\n",
       "word_freq_report               229.201271\n",
       "word_freq_addresses             57.727676\n",
       "word_freq_free                 196.424975\n",
       "word_freq_business              45.673775\n",
       "word_freq_email                 47.961674\n",
       "word_freq_you                    5.257394\n",
       "word_freq_credit               383.001882\n",
       "word_freq_your                   9.009506\n",
       "word_freq_font                 109.142325\n",
       "word_freq_000                   46.807860\n",
       "word_freq_money                302.056409\n",
       "word_freq_hp                    43.603634\n",
       "word_freq_hpl                   63.900394\n",
       "word_freq_george                34.204476\n",
       "word_freq_650                   58.373022\n",
       "word_freq_lab                  175.248251\n",
       "word_freq_labs                  52.006798\n",
       "word_freq_telnet               254.232509\n",
       "word_freq_857                  127.376529\n",
       "word_freq_data                 296.088338\n",
       "word_freq_415                  125.943332\n",
       "word_freq_85                   449.374271\n",
       "word_freq_technology            81.207276\n",
       "word_freq_1999                  42.621043\n",
       "word_freq_parts                912.045722\n",
       "word_freq_pm                   215.718144\n",
       "word_freq_direct                99.386561\n",
       "word_freq_cs                   193.619294\n",
       "word_freq_meeting              115.705974\n",
       "word_freq_original              78.572357\n",
       "word_freq_project              479.830907\n",
       "word_freq_re                   128.864672\n",
       "word_freq_edu                  150.899817\n",
       "word_freq_table                459.433463\n",
       "word_freq_conference           537.493007\n",
       "char_freq_;                    213.068194\n",
       "char_freq_(                    393.415158\n",
       "char_freq_[                    618.475610\n",
       "char_freq_!                    607.455685\n",
       "char_freq_$                    199.953692\n",
       "char_freq_#                   1218.493647\n",
       "capital_run_length_average     670.368705\n",
       "capital_run_length_longest    1480.642050\n",
       "capital_run_length_total       145.829814\n",
       "label_spam                      -1.812596\n",
       "dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kurtosis\n",
    "dataset.kurt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deskriptivna analiza - pregled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>label_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.395151</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>42.810000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "count     4601.000000        4601.000000    4601.000000   4601.000000   \n",
       "mean         0.104553           0.213015       0.280656      0.065425   \n",
       "std          0.305358           1.290575       0.504143      1.395151   \n",
       "min          0.000000           0.000000       0.000000      0.000000   \n",
       "25%          0.000000           0.000000       0.000000      0.000000   \n",
       "50%          0.000000           0.000000       0.000000      0.000000   \n",
       "75%          0.000000           0.000000       0.420000      0.000000   \n",
       "max          4.540000          14.280000       5.100000     42.810000   \n",
       "\n",
       "       word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "count    4601.000000     4601.000000       4601.000000         4601.000000   \n",
       "mean        0.312223        0.095901          0.114208            0.105295   \n",
       "std         0.672513        0.273824          0.391441            0.401071   \n",
       "min         0.000000        0.000000          0.000000            0.000000   \n",
       "25%         0.000000        0.000000          0.000000            0.000000   \n",
       "50%         0.000000        0.000000          0.000000            0.000000   \n",
       "75%         0.380000        0.000000          0.000000            0.000000   \n",
       "max        10.000000        5.880000          7.270000           11.110000   \n",
       "\n",
       "       word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "count      4601.000000     4601.000000  ...  4601.000000  4601.000000   \n",
       "mean          0.090067        0.239413  ...     0.038575     0.139030   \n",
       "std           0.278616        0.644755  ...     0.243471     0.270355   \n",
       "min           0.000000        0.000000  ...     0.000000     0.000000   \n",
       "25%           0.000000        0.000000  ...     0.000000     0.000000   \n",
       "50%           0.000000        0.000000  ...     0.000000     0.065000   \n",
       "75%           0.000000        0.160000  ...     0.000000     0.188000   \n",
       "max           5.260000       18.180000  ...     4.385000     9.752000   \n",
       "\n",
       "       char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.016976     0.269071     0.075811     0.044238   \n",
       "std       0.109394     0.815672     0.245882     0.429342   \n",
       "min       0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.315000     0.052000     0.000000   \n",
       "max       4.081000    32.478000     6.003000    19.829000   \n",
       "\n",
       "       capital_run_length_average  capital_run_length_longest  \\\n",
       "count                 4601.000000                 4601.000000   \n",
       "mean                     5.191515                   52.172789   \n",
       "std                     31.729449                  194.891310   \n",
       "min                      1.000000                    1.000000   \n",
       "25%                      1.588000                    6.000000   \n",
       "50%                      2.276000                   15.000000   \n",
       "75%                      3.706000                   43.000000   \n",
       "max                   1102.500000                 9989.000000   \n",
       "\n",
       "       capital_run_length_total   label_spam  \n",
       "count               4601.000000  4601.000000  \n",
       "mean                 283.289285     0.394045  \n",
       "std                  606.347851     0.488698  \n",
       "min                    1.000000     0.000000  \n",
       "25%                   35.000000     0.000000  \n",
       "50%                   95.000000     0.000000  \n",
       "75%                  266.000000     1.000000  \n",
       "max                15841.000000     1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descriptive analysis\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raspodela podataka po klasama (SPAM or NOT SPAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#NOT SPAM:  2788\n",
      "#SPAM:  1813\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ0AAAD4CAYAAAA3kTv/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAANX0lEQVR4nO3cf6zd9V3H8eeLViAMxo+1IQ3g2k0Ioiy1lo4FBGaWMagKM1EWZ2Cbk5iwbMvCIpMEURMpLk6zqCwYB3PD4aJbhiFzMAZBCb9aKC2/Whh0KsOxBmREZ2X07R/n2+Tm5t7b25a+z723z0dyc8/9nu+55/Pu96RPvt9zaKoKSZI6HDTuBUiSDhxGR5LUxuhIktoYHUlSG6MjSWqzeNwLmMuWLFlSy5cvH/cyJGle2bBhw/aqWjrVfUZnBsuXL2f9+vXjXoYkzStJvjvdfV5ekyS1MTqSpDZGR5LUxuhIktoYHUlSG6MjSWpjdCRJbYyOJKmN/3PoDDY/9zLLr7h13MuQ5p1t69aOewmaozzTkSS1MTqSpDZGR5LUxuhIktoYHUlSG6MjSWpjdCRJbYyOJKmN0ZEktTE6kqQ2RkeS1MboSJLaGB1JUhujI0lqY3QkSW2MjiSpjdGRJLUxOpKkNkZHktTG6EiS2hgdSVIboyNJamN0JEltjI4kqY3RkSS1MTqSpDZGR5LUxuhIktoYHUlSG6MjSWpjdCRJbYyOJKmN0ZEktTE6kqQ2RkeS1MboSJLaGB1JUhujI0lqY3QkSW2MjiSpzeJxLyDJNcBtwJHAT1fVNWNekiRpP5kLZzpvB+4DzgbuHvNaJEn70diik+TTSTYBpwH3Ah8GrktyVZKPJnk8yaYkNw/7X53ki0nuTfJUkt8eth+e5I4kDyXZnOSCYfvyJE8muTHJ1iQ3JXlXknuGx68Z1+ySdKAa2+W1qvpkkq8AFwOfAO6qqjMAknwPWFFVO5IcNeFhbwNOB94APJzkVuAF4L1V9cMkS4D7ktwy7P9TwK8BHwIeBH4DOBP4FeD3gAsnryvJpcClAIveuPR1nVmSDnTjvry2CngEOBl4YsL2TcBNSX4T+PGE7V+vqh9V1XbgTmANEOCPh7OmbwHHAccO+z9bVZuraifwGHBHVRWwGVg+1YKq6vqqWl1VqxcdduTrNackiTGd6SRZCdwIHA9sBw4bbc5G4B3AWuAs4JeBK5OcOjy0Jv2qAt4PLAV+vqpeTbINOHS4f8eEfXdO+Hknc+BDFJJ0oBnLmU5VbayqlcBW4BTg28C5w7YdwAlVdSfwu4w+1Xb48NALkhya5E3AOYwumR0JvDAE553AmztnkSTN3tj+az/JUuClqtqZ5OSqeny4axHwpSRHMrp09tmq+q8kMLrsdiewBPijqvpekpuAf0qyGVgPPNk+jCRpVsb5QYIfMLqMRlWdPmH7q4ze7J/Kpqq6eNLv2c7oktxUfnbCfh+YcHvbxPskST3G/UECSdIBZN68mV5VV497DZKkfeOZjiSpjdGRJLUxOpKkNkZHktTG6EiS2hgdSVIboyNJamN0JEltjI4kqY3RkSS1MTqSpDZGR5LUxuhIktoYHUlSG6MjSWpjdCRJbYyOJKmN0ZEktTE6kqQ2RkeS1MboSJLaGB1JUhujI0lqY3QkSW2MjiSpjdGRJLUxOpKkNkZHktTG6EiS2hgdSVIboyNJamN0JEltjI4kqY3RkSS1MTqSpDZGR5LUZvG4FzCXnXrckaxft3bcy5CkBcMzHUlSG6MjSWpjdCRJbYyOJKmN0ZEktTE6kqQ2RkeS1MboSJLaGB1JUhujI0lqY3QkSW2MjiSpjdGRJLUxOpKkNkZHktTG6EiS2hgdSVIboyNJamN0JEltjI4kqY3RkSS1WTzuBcxlm597meVX3DruZUhSq23r1u633+2ZjiSpjdGRJLUxOpKkNkZHktTG6EiS2hgdSVIboyNJamN0JEltjI4kqY3RkSS1MTqSpDZGR5LUxuhIktoYHUlSG6MjSWpjdCRJbYyOJKmN0ZEktTE6kqQ2RkeS1MboSJLaGB1JUhujI0lqY3QkSW2MjiSpjdGRJLUxOpKkNkZHktTG6EiS2hgdSVIboyNJamN0JEltjI4kqY3RkSS1MTqSpDZGR5LUxuhIktoYHUlSG6MjSWpjdCRJbYyOJKnNHkUnyTVJ3pnkwiSf2psnTLIyyfl781hJ0vy2p2c6bwfuA84G7t7L51wJGB1JOgDNKjpJPp1kE3AacC/wYeC6JFcluSvJtUkeSLI1yS8Mjzk0yQ1JNid5eDhDOhj4Q+CiJBuTXDTN85093L9xeOwRSc5JcneSW5NsSfK5JAcN+1+XZH2Sx5L8wYTfs204O9s43L8qyTeTfCfJ70zz3JcO+65/7X9e3pM/S0nSbiyezU5V9ckkXwEuBj4B3FVVZwAk+UVgcVWtGS6b/T7wLuCy0UPr1CQnA7cBJwFXAaur6iMzPOXlwGVVdU+Sw4H/HbavAU4Bvgv8M/CrwD8AV1bVi0kWAXckeVtVbRoe829VtTLJnwE3AmcAhwKPAp+bYtbrgesBDll2Ys3mz0eSNDt7cnltFfAIcDLwxKT7vjp83wAsH26fCXwJoKqeZBSKk2b5XPcAn0nyUeCoqvrxsP2Bqnqmql4Dvjw8B8CvJ3kIeBj4GUZh2uWW4ftm4P6qeqWqfgDsSHLULNcjSXod7PZMJ8lKRmcIxwPbgcNGm7MReMew247h+2uz+Z27U1XrktzK6L2fe5Kcu+uuybsmWcHozOi0qnopyY2MzmR22bW2nRNu7/p5n9cqSZq93Z7pVNXGqloJbGV0BvFt4NyqWllVP5rhof8CvB8gyUnATwJbgFeAI2Z6ziRvrarNVXUt8CCjsyuANUlWDO/lXAT8K/BG4L+Bl5McC5y3u5kkSeMx2w8SLAVeqqqdwMlV9fgsHvZXwEFJNgN/D3ygqnYAdwKnzPRBAuDjSR4dPrzwKvCNYfuDwF8wurz3LPC1qnqE0WW1J4G/Y3RpTpI0B6VqfrxXnuQc4PKq+qWu5zxk2Ym17JI/73o6SZoTtq1bu0+PT7KhqlZPdZ//IoEkqc1Y30hP8kHgY5M231NVl03et6ruAu5qWJYkaT8Za3Sq6gbghnGuQZLUx8trkqQ2RkeS1MboSJLaGB1JUhujI0lqY3QkSW2MjiSpjdGRJLUxOpKkNkZHktTG6EiS2hgdSVIboyNJamN0JEltjI4kqY3RkSS1MTqSpDZGR5LUxuhIktoYHUlSG6MjSWpjdCRJbYyOJKmN0ZEktTE6kqQ2RkeS1MboSJLaGB1JUhujI0lqY3QkSW2MjiSpjdGRJLUxOpKkNkZHktRm8bgXMJedetyRrF+3dtzLkKQFwzMdSVIboyNJamN0JEltjI4kqY3RkSS1MTqSpDZGR5LUxuhIktoYHUlSm1TVuNcwZyV5Bdgy7nXsZ0uA7eNexH52IMwIB8aczjg/vLmqlk51h/8Mzsy2VNXqcS9if0qy3hkXhgNhTmec/7y8JklqY3QkSW2MzsyuH/cCGjjjwnEgzOmM85wfJJAktfFMR5LUxuhIktoYnWkkeU+SLUmeTnLFuNezL5JsS7I5ycYk64dtxyS5PclTw/ejh+1J8tlh7k1JVo139VNL8vkkLyR5dMK2PZ4pySXD/k8luWQcs0xnmhmvTvLccCw3Jjl/wn2fGmbckuTcCdvn7Gs5yQlJ7kzyeJLHknxs2L5gjuUMMy6oYzlrVeXXpC9gEfAd4C3AwcAjwCnjXtc+zLMNWDJp258AVwy3rwCuHW6fD3wDCHA6cP+41z/NTGcBq4BH93Ym4BjgmeH70cPto8c9225mvBq4fIp9Txlep4cAK4bX76K5/loGlgGrhttHAFuHWRbMsZxhxgV1LGf75ZnO1NYAT1fVM1X1f8DNwAVjXtPr7QLgC8PtLwAXTtj+tzVyH3BUkmVjWN+Mqupu4MVJm/d0pnOB26vqxap6CbgdeM9+X/wsTTPjdC4Abq6qHVX1LPA0o9fxnH4tV9XzVfXQcPsV4AngOBbQsZxhxunMy2M5W0ZnascB/z7h5/9g5hfJXFfAbUk2JLl02HZsVT0/3P5P4Njh9nyefU9nmq+zfmS4tPT5XZedWAAzJlkO/BxwPwv0WE6aERbosZyJ0TkwnFlVq4DzgMuSnDXxzhqd0y+oz84vxJkG1wFvBVYCzwN/OtbVvE6SHA78I/DxqvrhxPsWyrGcYsYFeSx3x+hM7TnghAk/Hz9sm5eq6rnh+wvA1xidpn9/12Wz4fsLw+7zefY9nWnezVpV36+q16pqJ/DXjI4lzOMZk/wEo7+Mb6qqrw6bF9SxnGrGhXgsZ8PoTO1B4MQkK5IcDLwPuGXMa9orSd6Q5Ihdt4F3A48ymmfXJ3wuAb4+3L4FuHj4lNDpwMsTLnPMdXs60zeBdyc5eri08e5h25w16f219zI6ljCa8X1JDkmyAjgReIA5/lpOEuBvgCeq6jMT7lowx3K6GRfasZy1cX+SYa5+MfqUzFZGnxa5ctzr2Yc53sLoUy6PAI/tmgV4E3AH8BTwLeCYYXuAvxzm3gysHvcM08z1ZUaXJF5ldG37t/ZmJuBDjN6ofRr44LjnmsWMXxxm2MToL5xlE/a/cphxC3DefHgtA2cyunS2Cdg4fJ2/kI7lDDMuqGM52y//GRxJUhsvr0mS2hgdSVIboyNJamN0JEltjI4kqY3RkSS1MTqSpDb/D8HQWGxRg0udAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "not_spam_count, spam_count = dataset['label_spam'].value_counts()\n",
    "plt.barh([\"#not_spam\",\"#spam\"], [not_spam_count, spam_count])\n",
    "print(\"#NOT SPAM:  \"+str(not_spam_count))\n",
    "print(\"#SPAM:  \"+ str(spam_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balansiranje dataseta - UNDERSAMPLING\n",
    "#### Izbacivanje random uzoraka iz klase sa većim brojem instanci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#NOT SPAM:  1813\n",
      "#SPAM:  1813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 2 artists>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Solving unbalanced dataset \n",
    "#1 undersampling\n",
    "\n",
    "df_majority_US = dataset[dataset.label_spam == 0]\n",
    "df_minority_US = dataset[dataset.label_spam == 1]\n",
    "\n",
    "df_majority_undersampled = resample(df_majority_US, replace = False, n_samples = 1813, random_state = 123)\n",
    "df_undersampled = pd.concat([df_majority_undersampled, df_minority_US])\n",
    "undersampled_not_spam, undersampled_spam = df_undersampled .label_spam.value_counts()\n",
    "print(\"#NOT SPAM:  \"+str(undersampled_not_spam))\n",
    "print(\"#SPAM:  \"+ str(undersampled_spam))\n",
    "plt.barh([\"#not_spam\",\"#spam\"], [undersampled_not_spam, undersampled_spam])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balansiranje stabla - OVERSAMPLING\n",
    "\n",
    "#### Dodavanje duplikata iz klase sa manjim brojem instanci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#NOT SPAM:  2788\n",
      "#SPAM:  2788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 2 artists>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ0AAAD4CAYAAAA3kTv/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAANXUlEQVR4nO3cf6zd9V3H8eeLViAMxo+1IQ3g2k0Ioiy1lo4FBGaWMagKM1EWZ2Cbk5iwbMvCIpMEURMpLk6zqCwYB3PD4aJbhiFzMAZBCb9aKC2/Whh0KsOxBmREZ2X07R/n2+Tm5t7b25a+z723z0dyc8/9nu+55/Pu96RPvt9zaKoKSZI6HDTuBUiSDhxGR5LUxuhIktoYHUlSG6MjSWqzeNwLmMuWLFlSy5cvH/cyJGle2bBhw/aqWjrVfUZnBsuXL2f9+vXjXoYkzStJvjvdfV5ekyS1MTqSpDZGR5LUxuhIktoYHUlSG6MjSWpjdCRJbYyOJKmN/3PoDDY/9zLLr7h13MuQpFbb1q3db7/bMx1JUhujI0lqY3QkSW2MjiSpjdGRJLUxOpKkNkZHktTG6EiS2hgdSVIboyNJamN0JEltjI4kqY3RkSS1MTqSpDZGR5LUxuhIktoYHUlSG6MjSWpjdCRJbYyOJKmN0ZEktTE6kqQ2RkeS1MboSJLaGB1JUhujI0lqY3QkSW2MjiSpjdGRJLUxOpKkNkZHktTG6EiS2hgdSVIboyNJamN0JEltjI4kqY3RkSS1MTqSpDZGR5LUxuhIktosHvcCklwD3AYcCfx0VV0z5iVJkvaTuXCm83bgPuBs4O4xr0WStB+NLTpJPp1kE3AacC/wYeC6JFcl+WiSx5NsSnLzsP/VSb6Y5N4kTyX57WH74UnuSPJQks1JLhi2L0/yZJIbk2xNclOSdyW5Z3j8mnHNLkkHqrFdXquqTyb5CnAx8Angrqo6AyDJ94AVVbUjyVETHvY24HTgDcDDSW4FXgDeW1U/TLIEuC/JLcP+PwX8GvAh4EHgN4AzgV8Bfg+4cPK6klwKXAqw6I1LX9eZJelAN+7La6uAR4CTgScmbN8E3JTkN4EfT9j+9ar6UVVtB+4E1gAB/ng4a/oWcBxw7LD/s1W1uap2Ao8Bd1RVAZuB5VMtqKqur6rVVbV60WFHvl5zSpIY05lOkpXAjcDxwHbgsNHmbATeAawFzgJ+GbgyyanDQ2vSryrg/cBS4Oer6tUk24BDh/t3TNh354SfdzIHPkQhSQeasZzpVNXGqloJbAVOAb4NnDts2wGcUFV3Ar/L6FNthw8PvSDJoUneBJzD6JLZkcALQ3DeCby5cxZJ0uyN7b/2kywFXqqqnUlOrqrHh7sWAV9KciSjS2efrar/SgKjy253AkuAP6qq7yW5CfinJJuB9cCT7cNIkmZlnB8k+AGjy2hU1ekTtr/K6M3+qWyqqosn/Z7tjC7JTeVnJ+z3gQm3t028T5LUY9wfJJAkHUDmzZvpVXX1uNcgSdo3nulIktoYHUlSG6MjSWpjdCRJbYyOJKmN0ZEktTE6kqQ2RkeS1MboSJLaGB1JUhujI0lqY3QkSW2MjiSpjdGRJLUxOpKkNkZHktTG6EiS2hgdSVIboyNJamN0JEltjI4kqY3RkSS1MTqSpDZGR5LUxuhIktoYHUlSG6MjSWpjdCRJbYyOJKmN0ZEktTE6kqQ2RkeS1MboSJLaGB1JUhujI0lqY3QkSW0Wj3sBc9mpxx3J+nVrx70MSVowPNORJLUxOpKkNkZHktTG6EiS2hgdSVIboyNJamN0JEltjI4kqY3RkSS1MTqSpDZGR5LUxuhIktoYHUlSG6MjSWpjdCRJbYyOJKmN0ZEktTE6kqQ2RkeS1MboSJLaGB1JUpvF417AXLb5uZdZfsWt416GJLXatm7tfvvdnulIktoYHUlSG6MjSWpjdCRJbYyOJKmN0ZEktTE6kqQ2RkeS1MboSJLaGB1JUhujI0lqY3QkSW2MjiSpjdGRJLUxOpKkNkZHktTG6EiS2hgdSVIboyNJamN0JEltjI4kqY3RkSS1MTqSpDZGR5LUxuhIktoYHUlSG6MjSWpjdCRJbYyOJKmN0ZEktTE6kqQ2RkeS1MboSJLaGB1JUhujI0lqY3QkSW2MjiSpjdGRJLUxOpKkNkZHktTG6EiS2uxRdJJck+SdSS5M8qm9ecIkK5OcvzePlSTNb3t6pvN24D7gbODuvXzOlYDRkaQD0Kyik+TTSTYBpwH3Ah8GrktyVZK7klyb5IEkW5P8wvCYQ5PckGRzkoeHM6SDgT8ELkqyMclF0zzf2cP9G4fHHpHknCR3J7k1yZYkn0ty0LD/dUnWJ3ksyR9M+D3bhrOzjcP9q5J8M8l3kvzONM996bDv+tf+5+U9+bOUJO3G4tnsVFWfTPIV4GLgE8BdVXUGQJJfBBZX1ZrhstnvA+8CLhs9tE5NcjJwG3AScBWwuqo+MsNTXg5cVlX3JDkc+N9h+xrgFOC7wD8Dvwr8A3BlVb2YZBFwR5K3VdWm4TH/VlUrk/wZcCNwBnAo8CjwuSlmvR64HuCQZSfWbP58JEmzsyeX11YBjwAnA09Muu+rw/cNwPLh9pnAlwCq6klGoThpls91D/CZJB8FjqqqHw/bH6iqZ6rqNeDLw3MA/HqSh4CHgZ9hFKZdbhm+bwbur6pXquoHwI4kR81yPZKk18Fuz3SSrGR0hnA8sB04bLQ5G4F3DLvtGL6/NpvfuTtVtS7JrYze+7knybm77pq8a5IVjM6MTquql5LcyOhMZpdda9s54faun/d5rZKk2dvtmU5VbayqlcBWRmcQ3wbOraqVVfWjGR76L8D7AZKcBPwksAV4BThipudM8taq2lxV1wIPMjq7AliTZMXwXs5FwL8CbwT+G3g5ybHAebubSZI0HrP9IMFS4KWq2gmcXFWPz+JhfwUclGQz8PfAB6pqB3AncMpMHyQAPp7k0eHDC68C3xi2Pwj8BaPLe88CX6uqRxhdVnsS+DtGl+YkSXNQqubHe+VJzgEur6pf6nrOQ5adWMsu+fOup5OkOWHburX79PgkG6pq9VT3+S8SSJLajPWN9CQfBD42afM9VXXZ5H2r6i7groZlSZL2k7FGp6puAG4Y5xokSX28vCZJamN0JEltjI4kqY3RkSS1MTqSpDZGR5LUxuhIktoYHUlSG6MjSWpjdCRJbYyOJKmN0ZEktTE6kqQ2RkeS1MboSJLaGB1JUhujI0lqY3QkSW2MjiSpjdGRJLUxOpKkNkZHktTG6EiS2hgdSVIboyNJamN0JEltjI4kqY3RkSS1MTqSpDZGR5LUxuhIktoYHUlSG6MjSWpjdCRJbRaPewFz2anHHcn6dWvHvQxJWjA805EktTE6kqQ2RkeS1MboSJLaGB1JUhujI0lqY3QkSW2MjiSpjdGRJLVJVY17DXNWkleALeNex362BNg+7kXsZwfCjHBgzOmM88Obq2rpVHf4z+DMbEtVrR73IvanJOudcWE4EOZ0xvnPy2uSpDZGR5LUxujM7PpxL6CBMy4cB8KczjjP+UECSVIbz3QkSW2MjiSpjdGZRpL3JNmS5OkkV4x7PfsiybYkm5NsTLJ+2HZMktuTPDV8P3rYniSfHebelGTVeFc/tSSfT/JCkkcnbNvjmZJcMuz/VJJLxjHLdKaZ8eokzw3HcmOS8yfc96lhxi1Jzp2wfc6+lpOckOTOJI8neSzJx4btC+ZYzjDjgjqWs1ZVfk36AhYB3wHeAhwMPAKcMu517cM824Alk7b9CXDFcPsK4Nrh9vnAN4AApwP3j3v908x0FrAKeHRvZwKOAZ4Zvh893D563LPtZsargcun2PeU4XV6CLBieP0umuuvZWAZsGq4fQSwdZhlwRzLGWZcUMdytl+e6UxtDfB0VT1TVf8H3AxcMOY1vd4uAL4w3P4CcOGE7X9bI/cBRyVZNob1zaiq7gZenLR5T2c6F7i9ql6sqpeA24H37PfFz9I0M07nAuDmqtpRVc8CTzN6Hc/p13JVPV9VDw23XwGeAI5jAR3LGWaczrw8lrNldKZ2HPDvE37+D2Z+kcx1BdyWZEOSS4dtx1bV88Pt/wSOHW7P59n3dKb5OutHhktLn9912YkFMGOS5cDPAfezQI/lpBlhgR7LmRidA8OZVbUKOA+4LMlZE++s0Tn9gvrs/EKcaXAd8FZgJfA88KdjXc3rJMnhwD8CH6+qH068b6EcyylmXJDHcneMztSeA06Y8PPxw7Z5qaqeG76/AHyN0Wn693ddNhu+vzDsPp9n39OZ5t2sVfX9qnqtqnYCf83oWMI8njHJTzD6y/imqvrqsHlBHcupZlyIx3I2jM7UHgROTLIiycHA+4BbxrymvZLkDUmO2HUbeDfwKKN5dn3C5xLg68PtW4CLh08JnQ68POEyx1y3pzN9E3h3kqOHSxvvHrbNWZPeX3svo2MJoxnfl+SQJCuAE4EHmOOv5SQB/gZ4oqo+M+GuBXMsp5txoR3LWRv3Jxnm6hejT8lsZfRpkSvHvZ59mOMtjD7l8gjw2K5ZgDcBdwBPAd8Cjhm2B/jLYe7NwOpxzzDNXF9mdEniVUbXtn9rb2YCPsTojdqngQ+Oe65ZzPjFYYZNjP7CWTZh/yuHGbcA582H1zJwJqNLZ5uAjcPX+QvpWM4w44I6lrP98p/BkSS18fKaJKmN0ZEktTE6kqQ2RkeS1MboSJLaGB1JUhujI0lq8//IY1hs4akmfAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#2 oversampling\n",
    "df_majority_OS = dataset[dataset.label_spam==0]\n",
    "df_minority_OS = dataset[dataset.label_spam==1]\n",
    "df_minority_upsampled = resample(df_minority_OS, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=2788,    # to match majority class\n",
    "                                 random_state=123)\n",
    "df_upsampled = pd.concat([df_majority_OS, df_minority_upsampled])\n",
    "oversampled_not_spam, oversampled_spam = df_upsampled.label_spam.value_counts()\n",
    "print(\"#NOT SPAM:  \"+str(oversampled_not_spam))\n",
    "print(\"#SPAM:  \"+ str(oversampled_spam))\n",
    "plt.barh([\"#not_spam\",\"#spam\"], [oversampled_not_spam, oversampled_spam])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Priprema dataseta za podelu na train i test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bez balansiranja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imbalanced = dataset.drop(\"label_spam\",axis = 1).values\n",
    "y_imbalanced = dataset[\"label_spam\"].values\n",
    "\n",
    "X_train_imbalanced,X_test_imbalanced, y_train_imbalanced, y_test_imbalanced = train_test_split(X_imbalanced,y_imbalanced,test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_undersampled = df_undersampled.drop(\"label_spam\",axis = 1).values\n",
    "y_undersampled = df_undersampled[\"label_spam\"].values\n",
    "\n",
    "X_train_undersampled,X_test_undersampled, y_train_undersampled, y_test_undersampled = train_test_split(X_undersampled,y_undersampled,test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_upsampled = df_upsampled.drop(\"label_spam\",axis = 1).values\n",
    "y_upsampled = df_upsampled[\"label_spam\"].values\n",
    "\n",
    "X_train_upsampled,X_test_upsampled, y_train_upsampled, y_test_upsampled = train_test_split(X_upsampled,y_upsampled,test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmi za predikciju\n",
    "### LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bez balansiranja i cross validacije"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model_imbalanced = LogisticRegression(max_iter = 3000)\n",
    "logistic_regression_model_imbalanced.fit(X_train_imbalanced,y_train_imbalanced)\n",
    "prediction_imbalanced = logistic_regression_model_imbalanced.predict(X_test_imbalanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Različite metrike za evaluaciju prediktivnog algoritma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_imbalanced = confusion_matrix(y_true=y_test_imbalanced,y_pred=prediction_imbalanced)\n",
    "TP_imbalanced = conf_matrix_imbalanced[1,1]\n",
    "TN_imbalanced = conf_matrix_imbalanced[0,0]\n",
    "FP_imbalanced = conf_matrix_imbalanced[0,1]\n",
    "FN_imbalanced = conf_matrix_imbalanced[1,0]\n",
    "sensitivity_imbalanced = TP_imbalanced/(TP_imbalanced+FN_imbalanced)\n",
    "specificity_imbalanced = TN_imbalanced/(TN_imbalanced+FP_imbalanced)\n",
    "accuracy_imbalanced = accuracy_score(y_test_imbalanced,prediction_imbalanced)*100\n",
    "f1_imbalanced = f1_score(y_test_imbalanced, prediction_imbalanced)\n",
    "precision_imbalanced = precision_score(y_test_imbalanced, prediction_imbalanced)\n",
    "recall_imbalanced = recall_score(y_test_imbalanced, prediction_imbalanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prikaz dobijenih evaluacionih parametara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  93.70249728555918\n",
      "F1 Score:  0.9169054441260746\n",
      "Precision:  0.9384164222873901\n",
      "Recall:  0.896358543417367\n",
      "Specificity:  0.9627659574468085\n",
      "Sensitivity:  0.896358543417367\n",
      "True positive:  320\n",
      "False positive:  21\n",
      "True negative:  543\n",
      "False negative:  37\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_imbalanced)\n",
    "print(\"F1 Score: \", f1_imbalanced)\n",
    "print(\"Precision: \", precision_imbalanced)\n",
    "print(\"Recall: \", recall_imbalanced)\n",
    "print(\"Specificity: \", specificity_imbalanced)\n",
    "print(\"Sensitivity: \", sensitivity_imbalanced)\n",
    "print(\"True positive: \", TP_imbalanced)\n",
    "print(\"False positive: \", FP_imbalanced)\n",
    "print(\"True negative: \", TN_imbalanced)\n",
    "print(\"False negative: \", FN_imbalanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling bez cross validacije"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model_undersampled = LogisticRegression(max_iter = 3000)\n",
    "logistic_regression_model_undersampled.fit(X_train_undersampled,y_train_undersampled)\n",
    "prediction_undersampled = logistic_regression_model_undersampled.predict(X_test_undersampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Različite metrike za evaluaciju prediktivnog algoritma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_undersampled = confusion_matrix(y_true=y_test_undersampled,y_pred=prediction_undersampled)\n",
    "TP_undersampled = conf_matrix_undersampled[1,1]\n",
    "TN_undersampled = conf_matrix_undersampled[0,0]\n",
    "FP_undersampled = conf_matrix_undersampled[0,1]\n",
    "FN_undersampled = conf_matrix_undersampled[1,0]\n",
    "sensitivity_undersampled = TP_undersampled/(TP_undersampled+FN_undersampled)\n",
    "specificity_undersampled = TN_undersampled/(TN_undersampled+FP_undersampled)\n",
    "accuracy_undersampled = accuracy_score(y_test_undersampled,prediction_undersampled)*100\n",
    "f1_undersampled = f1_score(y_test_undersampled, prediction_undersampled)\n",
    "precision_undersampled = precision_score(y_test_undersampled, prediction_undersampled)\n",
    "recall_undersampled = recall_score(y_test_undersampled, prediction_undersampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prikaz dobijenih evaluacionih parametara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  92.01101928374655\n",
      "F1 Score:  0.9171428571428571\n",
      "Precision:  0.9277456647398844\n",
      "Recall:  0.9067796610169492\n",
      "Specificity:  0.9327956989247311\n",
      "Sensitivity:  0.9067796610169492\n",
      "True positive:  321\n",
      "False positive:  25\n",
      "True negative:  347\n",
      "False negative:  33\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_undersampled)\n",
    "print(\"F1 Score: \", f1_undersampled)\n",
    "print(\"Precision: \", precision_undersampled)\n",
    "print(\"Recall: \", recall_undersampled)\n",
    "print(\"Specificity: \", specificity_undersampled)\n",
    "print(\"Sensitivity: \", sensitivity_undersampled)\n",
    "print(\"True positive: \", TP_undersampled)\n",
    "print(\"False positive: \", FP_undersampled)\n",
    "print(\"True negative: \", TN_undersampled)\n",
    "print(\"False negative: \", FN_undersampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upsampling bez cross validacije"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model_upsampled = LogisticRegression(max_iter = 3000)\n",
    "logistic_regression_model_upsampled.fit(X_train_upsampled,y_train_upsampled)\n",
    "prediction_upsampled = logistic_regression_model_upsampled.predict(X_test_upsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Različite metrike za evaluaciju prediktivnog algoritma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_upsampled = confusion_matrix(y_true=y_test_upsampled,y_pred=prediction_upsampled)\n",
    "TP_upsampled = conf_matrix_upsampled[1,1]\n",
    "TN_upsampled = conf_matrix_upsampled[0,0]\n",
    "FP_upsampled = conf_matrix_upsampled[0,1]\n",
    "FN_upsampled = conf_matrix_upsampled[1,0]\n",
    "sensitivity_upsampled = TP_upsampled/(TP_upsampled+FN_upsampled)\n",
    "specificity_upsampled = TN_upsampled/(TN_upsampled+FP_upsampled)\n",
    "accuracy_upsampled = accuracy_score(y_test_upsampled,prediction_upsampled)*100\n",
    "f1_upsampled = f1_score(y_test_upsampled, prediction_upsampled)\n",
    "precision_upsampled = precision_score(y_test_upsampled, prediction_upsampled)\n",
    "recall_upsampled = recall_score(y_test_upsampled, prediction_upsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prikaz dobijenih evaluacionih parametara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  92.92114695340501\n",
      "F1 Score:  0.9296527159394479\n",
      "Precision:  0.9222614840989399\n",
      "Recall:  0.9371633752244165\n",
      "Specificity:  0.9212880143112702\n",
      "Sensitivity:  0.9371633752244165\n",
      "True positive:  522\n",
      "False positive:  44\n",
      "True negative:  515\n",
      "False negative:  35\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_upsampled)\n",
    "print(\"F1 Score: \", f1_upsampled)\n",
    "print(\"Precision: \", precision_upsampled)\n",
    "print(\"Recall: \", recall_upsampled)\n",
    "print(\"Specificity: \", specificity_upsampled)\n",
    "print(\"Sensitivity: \", sensitivity_upsampled)\n",
    "print(\"True positive: \", TP_upsampled)\n",
    "print(\"False positive: \", FP_upsampled)\n",
    "print(\"True negative: \", TN_upsampled)\n",
    "print(\"False negative: \", FN_upsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizuelizacija nekih mera postignutih rezultata korišćenjem Logistic Regression prediktivnog algoritma bez cross validacije"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(13, 3))\n",
    "plt.subplot(131)\n",
    "plt.ylim(90,95)\n",
    "plt.bar(['acc_under', 'acc_up', 'acc_imb'], [accuracy_undersampled, accuracy_upsampled, accuracy_imbalanced])\n",
    "plt.subplot(132)\n",
    "plt.ylim(0.9,0.95)\n",
    "plt.bar(['f1_under', 'f1_up','f1_imb'], [f1_undersampled, f1_upsampled, f1_imbalanced])\n",
    "plt.subplot(133)\n",
    "plt.ylim(0.9,0.95)\n",
    "plt.bar(['prec_under', 'prec_up', 'prec_imb'], [precision_undersampled, precision_upsampled, [precision_imbalanced]])\n",
    "plt.suptitle('Measurement comparison')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression sa cross validacijom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bez balansiranja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  92.29098805646036\n",
      "F1 Score:  0.9064558629776021\n",
      "Precision:  0.9322493224932249\n",
      "Recall:  0.882051282051282\n",
      "Specificity:  0.9529190207156308\n",
      "Sensitivity:  0.882051282051282\n",
      "True positive:  344\n",
      "False positive:  25\n",
      "True negative:  506\n",
      "False negative:  46\n",
      "-------------\n",
      "Accuracy:  92.82608695652173\n",
      "F1 Score:  0.9080779944289694\n",
      "Precision:  0.9157303370786517\n",
      "Recall:  0.9005524861878453\n",
      "Specificity:  0.946236559139785\n",
      "Sensitivity:  0.9005524861878453\n",
      "True positive:  326\n",
      "False positive:  30\n",
      "True negative:  528\n",
      "False negative:  36\n",
      "-------------\n",
      "Accuracy:  92.3913043478261\n",
      "F1 Score:  0.9002849002849003\n",
      "Precision:  0.9080459770114943\n",
      "Recall:  0.8926553672316384\n",
      "Specificity:  0.9434628975265018\n",
      "Sensitivity:  0.8926553672316384\n",
      "True positive:  316\n",
      "False positive:  32\n",
      "True negative:  534\n",
      "False negative:  38\n",
      "-------------\n",
      "Accuracy:  93.15217391304348\n",
      "F1 Score:  0.9088277858176554\n",
      "Precision:  0.9154518950437318\n",
      "Recall:  0.9022988505747126\n",
      "Specificity:  0.9493006993006993\n",
      "Sensitivity:  0.9022988505747126\n",
      "True positive:  314\n",
      "False positive:  29\n",
      "True negative:  543\n",
      "False negative:  34\n",
      "-------------\n",
      "Accuracy:  93.04347826086956\n",
      "F1 Score:  0.9077809798270894\n",
      "Precision:  0.9402985074626866\n",
      "Recall:  0.8774373259052924\n",
      "Specificity:  0.964349376114082\n",
      "Sensitivity:  0.8774373259052924\n",
      "True positive:  315\n",
      "False positive:  20\n",
      "True negative:  541\n",
      "False negative:  44\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5,random_state = 42, shuffle = True)\n",
    "\n",
    "for train_index,test_index in kf.split(X_imbalanced):\n",
    "    X_train_imbalanced,X_test_imbalanced=X_imbalanced[train_index],X_imbalanced[test_index]\n",
    "    y_train_imbalanced,y_test_imbalanced=y_imbalanced[train_index],y_imbalanced[test_index]\n",
    "    \n",
    "    logistic_regression_model_imbalanced = LogisticRegression(max_iter = 3000)\n",
    "    logistic_regression_model_imbalanced.fit(X_train_imbalanced,y_train_imbalanced)\n",
    "    prediction_imbalanced = logistic_regression_model_imbalanced.predict(X_test_imbalanced)\n",
    "    \n",
    "    conf_matrix_imbalanced = confusion_matrix(y_true=y_test_imbalanced,y_pred=prediction_imbalanced)\n",
    "    TP_imbalanced = conf_matrix_imbalanced[1,1]\n",
    "    TN_imbalanced = conf_matrix_imbalanced[0,0]\n",
    "    FP_imbalanced = conf_matrix_imbalanced[0,1]\n",
    "    FN_imbalanced = conf_matrix_imbalanced[1,0]\n",
    "    sensitivity_imbalanced = TP_imbalanced/(TP_imbalanced+FN_imbalanced)\n",
    "    specificity_imbalanced = TN_imbalanced/(TN_imbalanced+FP_imbalanced)\n",
    "    accuracy_imbalanced = accuracy_score(y_test_imbalanced,prediction_imbalanced)*100\n",
    "    f1_imbalanced = f1_score(y_test_imbalanced, prediction_imbalanced)\n",
    "    precision_imbalanced = precision_score(y_test_imbalanced, prediction_imbalanced)\n",
    "    recall_imbalanced = recall_score(y_test_imbalanced, prediction_imbalanced)\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy_imbalanced)\n",
    "    print(\"F1 Score: \", f1_imbalanced)\n",
    "    print(\"Precision: \", precision_imbalanced)\n",
    "    print(\"Recall: \", recall_imbalanced)\n",
    "    print(\"Specificity: \", specificity_imbalanced)\n",
    "    print(\"Sensitivity: \", sensitivity_imbalanced)\n",
    "    print(\"True positive: \", TP_imbalanced)\n",
    "    print(\"False positive: \", FP_imbalanced)\n",
    "    print(\"True negative: \", TN_imbalanced)\n",
    "    print(\"False negative: \", FN_imbalanced)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  93.1129476584022\n",
      "F1 Score:  0.9273255813953488\n",
      "Precision:  0.9354838709677419\n",
      "Recall:  0.9193083573487032\n",
      "Specificity:  0.941952506596306\n",
      "Sensitivity:  0.9193083573487032\n",
      "True positive:  319\n",
      "False positive:  22\n",
      "True negative:  357\n",
      "False negative:  28\n",
      "-------------\n",
      "Accuracy:  92.13793103448276\n",
      "F1 Score:  0.919605077574048\n",
      "Precision:  0.9209039548022598\n",
      "Recall:  0.9183098591549296\n",
      "Specificity:  0.9243243243243243\n",
      "Sensitivity:  0.9183098591549296\n",
      "True positive:  326\n",
      "False positive:  28\n",
      "True negative:  342\n",
      "False negative:  29\n",
      "-------------\n",
      "Accuracy:  92.6896551724138\n",
      "F1 Score:  0.9284750337381917\n",
      "Precision:  0.9347826086956522\n",
      "Recall:  0.9222520107238605\n",
      "Specificity:  0.9318181818181818\n",
      "Sensitivity:  0.9222520107238605\n",
      "True positive:  344\n",
      "False positive:  24\n",
      "True negative:  328\n",
      "False negative:  29\n",
      "-------------\n",
      "Accuracy:  92.55172413793103\n",
      "F1 Score:  0.9262295081967215\n",
      "Precision:  0.9442896935933147\n",
      "Recall:  0.9088471849865952\n",
      "Specificity:  0.9431818181818182\n",
      "Sensitivity:  0.9088471849865952\n",
      "True positive:  339\n",
      "False positive:  20\n",
      "True negative:  332\n",
      "False negative:  34\n",
      "-------------\n",
      "Accuracy:  92.41379310344827\n",
      "F1 Score:  0.9249658935879944\n",
      "Precision:  0.9211956521739131\n",
      "Recall:  0.9287671232876712\n",
      "Specificity:  0.9194444444444444\n",
      "Sensitivity:  0.9287671232876712\n",
      "True positive:  339\n",
      "False positive:  29\n",
      "True negative:  331\n",
      "False negative:  26\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5,random_state = 42, shuffle = True)\n",
    "\n",
    "for train_index,test_index in kf.split(X_undersampled):\n",
    "    X_train_undersampled,X_test_undersampled=X_undersampled[train_index],X_undersampled[test_index]\n",
    "    y_train_undersampled,y_test_undersampled=y_undersampled[train_index],y_undersampled[test_index]\n",
    "    \n",
    "    logistic_regression_model_undersampled = LogisticRegression(max_iter = 3000)\n",
    "    logistic_regression_model_undersampled.fit(X_train_undersampled,y_train_undersampled)\n",
    "    prediction_undersampled = logistic_regression_model_undersampled.predict(X_test_undersampled)\n",
    "    \n",
    "    conf_matrix_undersampled = confusion_matrix(y_true=y_test_undersampled,y_pred=prediction_undersampled)\n",
    "    TP_undersampled = conf_matrix_undersampled[1,1]\n",
    "    TN_undersampled = conf_matrix_undersampled[0,0]\n",
    "    FP_undersampled = conf_matrix_undersampled[0,1]\n",
    "    FN_undersampled = conf_matrix_undersampled[1,0]\n",
    "    sensitivity_undersampled = TP_undersampled/(TP_undersampled+FN_undersampled)\n",
    "    specificity_undersampled = TN_undersampled/(TN_undersampled+FP_undersampled)\n",
    "    accuracy_undersampled = accuracy_score(y_test_undersampled,prediction_undersampled)*100\n",
    "    f1_undersampled = f1_score(y_test_undersampled, prediction_undersampled)\n",
    "    precision_undersampled = precision_score(y_test_undersampled, prediction_undersampled)\n",
    "    recall_undersampled = recall_score(y_test_undersampled, prediction_undersampled)\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy_undersampled)\n",
    "    print(\"F1 Score: \", f1_undersampled)\n",
    "    print(\"Precision: \", precision_undersampled)\n",
    "    print(\"Recall: \", recall_undersampled)\n",
    "    print(\"Specificity: \", specificity_undersampled)\n",
    "    print(\"Sensitivity: \", sensitivity_undersampled)\n",
    "    print(\"True positive: \", TP_undersampled)\n",
    "    print(\"False positive: \", FP_undersampled)\n",
    "    print(\"True negative: \", TN_undersampled)\n",
    "    print(\"False negative: \", FN_undersampled)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  94.17562724014337\n",
      "F1 Score:  0.9395348837209302\n",
      "Precision:  0.9404096834264432\n",
      "Recall:  0.9386617100371747\n",
      "Specificity:  0.9446366782006921\n",
      "Sensitivity:  0.9386617100371747\n",
      "True positive:  505\n",
      "False positive:  32\n",
      "True negative:  546\n",
      "False negative:  33\n",
      "-------------\n",
      "Accuracy:  93.45291479820628\n",
      "F1 Score:  0.9322191272051996\n",
      "Precision:  0.9400749063670412\n",
      "Recall:  0.9244935543278084\n",
      "Specificity:  0.9440559440559441\n",
      "Sensitivity:  0.9244935543278084\n",
      "True positive:  502\n",
      "False positive:  32\n",
      "True negative:  540\n",
      "False negative:  41\n",
      "-------------\n",
      "Accuracy:  92.19730941704036\n",
      "F1 Score:  0.923076923076923\n",
      "Precision:  0.9288256227758007\n",
      "Recall:  0.9173989455184535\n",
      "Specificity:  0.9267399267399268\n",
      "Sensitivity:  0.9173989455184535\n",
      "True positive:  522\n",
      "False positive:  40\n",
      "True negative:  506\n",
      "False negative:  47\n",
      "-------------\n",
      "Accuracy:  93.00448430493273\n",
      "F1 Score:  0.9292196007259528\n",
      "Precision:  0.9377289377289377\n",
      "Recall:  0.920863309352518\n",
      "Specificity:  0.9391771019677997\n",
      "Sensitivity:  0.920863309352518\n",
      "True positive:  512\n",
      "False positive:  34\n",
      "True negative:  525\n",
      "False negative:  44\n",
      "-------------\n",
      "Accuracy:  92.01793721973094\n",
      "F1 Score:  0.9219982471516214\n",
      "Precision:  0.9409660107334525\n",
      "Recall:  0.9037800687285223\n",
      "Specificity:  0.9380863039399625\n",
      "Sensitivity:  0.9037800687285223\n",
      "True positive:  526\n",
      "False positive:  33\n",
      "True negative:  500\n",
      "False negative:  56\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5,random_state = 42, shuffle = True)\n",
    "\n",
    "for train_index,test_index in kf.split(X_upsampled):\n",
    "    X_train_upsampled,X_test_upsampled=X_upsampled[train_index],X_upsampled[test_index]\n",
    "    y_train_upsampled,y_test_upsampled=y_upsampled[train_index],y_upsampled[test_index]\n",
    "    \n",
    "    logistic_regression_model_upsampled = LogisticRegression(max_iter = 3000)\n",
    "    logistic_regression_model_upsampled.fit(X_train_upsampled,y_train_upsampled)\n",
    "    prediction_upsampled = logistic_regression_model_upsampled.predict(X_test_upsampled)\n",
    "    \n",
    "    conf_matrix_upsampled = confusion_matrix(y_true=y_test_upsampled,y_pred=prediction_upsampled)\n",
    "    TP_upsampled = conf_matrix_upsampled[1,1]\n",
    "    TN_upsampled = conf_matrix_upsampled[0,0]\n",
    "    FP_upsampled = conf_matrix_upsampled[0,1]\n",
    "    FN_upsampled = conf_matrix_upsampled[1,0]\n",
    "    sensitivity_upsampled = TP_upsampled/(TP_upsampled+FN_upsampled)\n",
    "    specificity_upsampled = TN_upsampled/(TN_upsampled+FP_upsampled)\n",
    "    accuracy_upsampled = accuracy_score(y_test_upsampled,prediction_upsampled)*100\n",
    "    f1_upsampled = f1_score(y_test_upsampled, prediction_upsampled)\n",
    "    precision_upsampled = precision_score(y_test_upsampled, prediction_upsampled)\n",
    "    recall_upsampled = recall_score(y_test_upsampled, prediction_upsampled)\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy_upsampled)\n",
    "    print(\"F1 Score: \", f1_upsampled)\n",
    "    print(\"Precision: \", precision_upsampled)\n",
    "    print(\"Recall: \", recall_upsampled)\n",
    "    print(\"Specificity: \", specificity_upsampled)\n",
    "    print(\"Sensitivity: \", sensitivity_upsampled)\n",
    "    print(\"True positive: \", TP_upsampled)\n",
    "    print(\"False positive: \", FP_upsampled)\n",
    "    print(\"True negative: \", TN_upsampled)\n",
    "    print(\"False negative: \", FN_upsampled)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeated kfold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bez balansiranja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  92.84164859002169\n",
      "F1 Score:  0.9090909090909092\n",
      "Precision:  0.9217877094972067\n",
      "Recall:  0.8967391304347826\n",
      "Specificity:  0.9494584837545126\n",
      "Sensitivity:  0.8967391304347826\n",
      "True positive:  165\n",
      "False positive:  14\n",
      "True negative:  263\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.91304347826087\n",
      "F1 Score:  0.9166666666666667\n",
      "Precision:  0.9447852760736196\n",
      "Recall:  0.8901734104046243\n",
      "Specificity:  0.9686411149825784\n",
      "Sensitivity:  0.8901734104046243\n",
      "True positive:  154\n",
      "False positive:  9\n",
      "True negative:  278\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.26086956521739\n",
      "F1 Score:  0.9096209912536443\n",
      "Precision:  0.975\n",
      "Recall:  0.8524590163934426\n",
      "Specificity:  0.9855595667870036\n",
      "Sensitivity:  0.8524590163934426\n",
      "True positive:  156\n",
      "False positive:  4\n",
      "True negative:  273\n",
      "False negative:  27\n",
      "-------------\n",
      "Accuracy:  91.73913043478261\n",
      "F1 Score:  0.9040404040404041\n",
      "Precision:  0.927461139896373\n",
      "Recall:  0.8817733990147784\n",
      "Specificity:  0.9455252918287937\n",
      "Sensitivity:  0.8817733990147784\n",
      "True positive:  179\n",
      "False positive:  14\n",
      "True negative:  243\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  92.3913043478261\n",
      "F1 Score:  0.9030470914127424\n",
      "Precision:  0.9005524861878453\n",
      "Recall:  0.9055555555555556\n",
      "Specificity:  0.9357142857142857\n",
      "Sensitivity:  0.9055555555555556\n",
      "True positive:  163\n",
      "False positive:  18\n",
      "True negative:  262\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  90.8695652173913\n",
      "F1 Score:  0.8882978723404256\n",
      "Precision:  0.912568306010929\n",
      "Recall:  0.8652849740932642\n",
      "Specificity:  0.9400749063670412\n",
      "Sensitivity:  0.8652849740932642\n",
      "True positive:  167\n",
      "False positive:  16\n",
      "True negative:  251\n",
      "False negative:  26\n",
      "-------------\n",
      "Accuracy:  91.95652173913044\n",
      "F1 Score:  0.8882175226586103\n",
      "Precision:  0.8963414634146342\n",
      "Recall:  0.8802395209580839\n",
      "Specificity:  0.9419795221843004\n",
      "Sensitivity:  0.8802395209580839\n",
      "True positive:  147\n",
      "False positive:  17\n",
      "True negative:  276\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  93.69565217391305\n",
      "F1 Score:  0.9169054441260744\n",
      "Precision:  0.9411764705882353\n",
      "Recall:  0.8938547486033519\n",
      "Specificity:  0.9644128113879004\n",
      "Sensitivity:  0.8938547486033519\n",
      "True positive:  160\n",
      "False positive:  10\n",
      "True negative:  271\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  94.1304347826087\n",
      "F1 Score:  0.9194029850746269\n",
      "Precision:  0.9005847953216374\n",
      "Recall:  0.9390243902439024\n",
      "Specificity:  0.9425675675675675\n",
      "Sensitivity:  0.9390243902439024\n",
      "True positive:  154\n",
      "False positive:  17\n",
      "True negative:  279\n",
      "False negative:  10\n",
      "-------------\n",
      "Accuracy:  94.56521739130434\n",
      "F1 Score:  0.932975871313673\n",
      "Precision:  0.9354838709677419\n",
      "Recall:  0.93048128342246\n",
      "Specificity:  0.9560439560439561\n",
      "Sensitivity:  0.93048128342246\n",
      "True positive:  174\n",
      "False positive:  12\n",
      "True negative:  261\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  94.36008676789588\n",
      "F1 Score:  0.9182389937106918\n",
      "Precision:  0.954248366013072\n",
      "Recall:  0.8848484848484849\n",
      "Specificity:  0.9763513513513513\n",
      "Sensitivity:  0.8848484848484849\n",
      "True positive:  146\n",
      "False positive:  7\n",
      "True negative:  289\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  90.8695652173913\n",
      "F1 Score:  0.8870967741935485\n",
      "Precision:  0.9016393442622951\n",
      "Recall:  0.873015873015873\n",
      "Specificity:  0.933579335793358\n",
      "Sensitivity:  0.873015873015873\n",
      "True positive:  165\n",
      "False positive:  18\n",
      "True negative:  253\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  93.04347826086956\n",
      "F1 Score:  0.9157894736842106\n",
      "Precision:  0.9405405405405406\n",
      "Recall:  0.8923076923076924\n",
      "Specificity:  0.9584905660377359\n",
      "Sensitivity:  0.8923076923076924\n",
      "True positive:  174\n",
      "False positive:  11\n",
      "True negative:  254\n",
      "False negative:  21\n",
      "-------------\n",
      "Accuracy:  91.95652173913044\n",
      "F1 Score:  0.8914956011730206\n",
      "Precision:  0.8994082840236687\n",
      "Recall:  0.8837209302325582\n",
      "Specificity:  0.9409722222222222\n",
      "Sensitivity:  0.8837209302325582\n",
      "True positive:  152\n",
      "False positive:  17\n",
      "True negative:  271\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  94.78260869565217\n",
      "F1 Score:  0.9384615384615386\n",
      "Precision:  0.9682539682539683\n",
      "Recall:  0.9104477611940298\n",
      "Specificity:  0.9768339768339769\n",
      "Sensitivity:  0.9104477611940298\n",
      "True positive:  183\n",
      "False positive:  6\n",
      "True negative:  253\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  93.04347826086956\n",
      "F1 Score:  0.8961038961038961\n",
      "Precision:  0.9078947368421053\n",
      "Recall:  0.8846153846153846\n",
      "Specificity:  0.9539473684210527\n",
      "Sensitivity:  0.8846153846153846\n",
      "True positive:  138\n",
      "False positive:  14\n",
      "True negative:  290\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  91.95652173913044\n",
      "F1 Score:  0.8980716253443527\n",
      "Precision:  0.9106145251396648\n",
      "Recall:  0.8858695652173914\n",
      "Specificity:  0.9420289855072463\n",
      "Sensitivity:  0.8858695652173914\n",
      "True positive:  163\n",
      "False positive:  16\n",
      "True negative:  260\n",
      "False negative:  21\n",
      "-------------\n",
      "Accuracy:  92.6086956521739\n",
      "F1 Score:  0.9170731707317074\n",
      "Precision:  0.9447236180904522\n",
      "Recall:  0.8909952606635071\n",
      "Specificity:  0.9558232931726908\n",
      "Sensitivity:  0.8909952606635071\n",
      "True positive:  188\n",
      "False positive:  11\n",
      "True negative:  238\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  93.04347826086956\n",
      "F1 Score:  0.9058823529411765\n",
      "Precision:  0.8953488372093024\n",
      "Recall:  0.9166666666666666\n",
      "Specificity:  0.9383561643835616\n",
      "Sensitivity:  0.9166666666666666\n",
      "True positive:  154\n",
      "False positive:  18\n",
      "True negative:  274\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  94.1304347826087\n",
      "F1 Score:  0.9198813056379823\n",
      "Precision:  0.9393939393939394\n",
      "Recall:  0.9011627906976745\n",
      "Specificity:  0.9652777777777778\n",
      "Sensitivity:  0.9011627906976745\n",
      "True positive:  155\n",
      "False positive:  10\n",
      "True negative:  278\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  91.75704989154013\n",
      "F1 Score:  0.8944444444444446\n",
      "Precision:  0.92\n",
      "Recall:  0.8702702702702703\n",
      "Specificity:  0.9492753623188406\n",
      "Sensitivity:  0.8702702702702703\n",
      "True positive:  161\n",
      "False positive:  14\n",
      "True negative:  262\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  93.91304347826087\n",
      "F1 Score:  0.9166666666666666\n",
      "Precision:  0.9005847953216374\n",
      "Recall:  0.9333333333333333\n",
      "Specificity:  0.9423728813559322\n",
      "Sensitivity:  0.9333333333333333\n",
      "True positive:  154\n",
      "False positive:  17\n",
      "True negative:  278\n",
      "False negative:  11\n",
      "-------------\n",
      "Accuracy:  94.34782608695652\n",
      "F1 Score:  0.9239766081871346\n",
      "Precision:  0.9404761904761905\n",
      "Recall:  0.9080459770114943\n",
      "Specificity:  0.965034965034965\n",
      "Sensitivity:  0.9080459770114943\n",
      "True positive:  158\n",
      "False positive:  10\n",
      "True negative:  276\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  93.26086956521739\n",
      "F1 Score:  0.9116809116809117\n",
      "Precision:  0.9248554913294798\n",
      "Recall:  0.898876404494382\n",
      "Specificity:  0.9539007092198581\n",
      "Sensitivity:  0.898876404494382\n",
      "True positive:  160\n",
      "False positive:  13\n",
      "True negative:  269\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  90.8695652173913\n",
      "F1 Score:  0.8939393939393939\n",
      "Precision:  0.917098445595855\n",
      "Recall:  0.8719211822660099\n",
      "Specificity:  0.9377431906614786\n",
      "Sensitivity:  0.8719211822660099\n",
      "True positive:  177\n",
      "False positive:  16\n",
      "True negative:  241\n",
      "False negative:  26\n",
      "-------------\n",
      "Accuracy:  91.73913043478261\n",
      "F1 Score:  0.8819875776397514\n",
      "Precision:  0.9281045751633987\n",
      "Recall:  0.8402366863905325\n",
      "Specificity:  0.9621993127147767\n",
      "Sensitivity:  0.8402366863905325\n",
      "True positive:  142\n",
      "False positive:  11\n",
      "True negative:  280\n",
      "False negative:  27\n",
      "-------------\n",
      "Accuracy:  93.04347826086956\n",
      "F1 Score:  0.9101123595505618\n",
      "Precision:  0.9418604651162791\n",
      "Recall:  0.8804347826086957\n",
      "Specificity:  0.9637681159420289\n",
      "Sensitivity:  0.8804347826086957\n",
      "True positive:  162\n",
      "False positive:  10\n",
      "True negative:  266\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  91.95652173913044\n",
      "F1 Score:  0.900804289544236\n",
      "Precision:  0.8842105263157894\n",
      "Recall:  0.9180327868852459\n",
      "Specificity:  0.9205776173285198\n",
      "Sensitivity:  0.9180327868852459\n",
      "True positive:  168\n",
      "False positive:  22\n",
      "True negative:  255\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  93.91304347826087\n",
      "F1 Score:  0.9213483146067416\n",
      "Precision:  0.9371428571428572\n",
      "Recall:  0.9060773480662984\n",
      "Specificity:  0.9605734767025089\n",
      "Sensitivity:  0.9060773480662984\n",
      "True positive:  164\n",
      "False positive:  11\n",
      "True negative:  268\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  93.04347826086956\n",
      "F1 Score:  0.9153439153439152\n",
      "Precision:  0.9251336898395722\n",
      "Recall:  0.9057591623036649\n",
      "Specificity:  0.9479553903345725\n",
      "Sensitivity:  0.9057591623036649\n",
      "True positive:  173\n",
      "False positive:  14\n",
      "True negative:  255\n",
      "False negative:  18\n",
      "-------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  94.57700650759219\n",
      "F1 Score:  0.9326145552560647\n",
      "Precision:  0.9301075268817204\n",
      "Recall:  0.9351351351351351\n",
      "Specificity:  0.9528985507246377\n",
      "Sensitivity:  0.9351351351351351\n",
      "True positive:  173\n",
      "False positive:  13\n",
      "True negative:  263\n",
      "False negative:  12\n",
      "-------------\n",
      "Accuracy:  94.1304347826087\n",
      "F1 Score:  0.9203539823008848\n",
      "Precision:  0.9069767441860465\n",
      "Recall:  0.9341317365269461\n",
      "Specificity:  0.9453924914675768\n",
      "Sensitivity:  0.9341317365269461\n",
      "True positive:  156\n",
      "False positive:  16\n",
      "True negative:  277\n",
      "False negative:  11\n",
      "-------------\n",
      "Accuracy:  94.34782608695652\n",
      "F1 Score:  0.9322916666666666\n",
      "Precision:  0.9521276595744681\n",
      "Recall:  0.9132653061224489\n",
      "Specificity:  0.9659090909090909\n",
      "Sensitivity:  0.9132653061224489\n",
      "True positive:  179\n",
      "False positive:  9\n",
      "True negative:  255\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  91.73913043478261\n",
      "F1 Score:  0.8875739644970415\n",
      "Precision:  0.872093023255814\n",
      "Recall:  0.9036144578313253\n",
      "Specificity:  0.9251700680272109\n",
      "Sensitivity:  0.9036144578313253\n",
      "True positive:  150\n",
      "False positive:  22\n",
      "True negative:  272\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  92.6086956521739\n",
      "F1 Score:  0.8999999999999999\n",
      "Precision:  0.8947368421052632\n",
      "Recall:  0.9053254437869822\n",
      "Specificity:  0.9381443298969072\n",
      "Sensitivity:  0.9053254437869822\n",
      "True positive:  153\n",
      "False positive:  18\n",
      "True negative:  273\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  90.8695652173913\n",
      "F1 Score:  0.8839779005524862\n",
      "Precision:  0.9248554913294798\n",
      "Recall:  0.8465608465608465\n",
      "Specificity:  0.9520295202952029\n",
      "Sensitivity:  0.8465608465608465\n",
      "True positive:  160\n",
      "False positive:  13\n",
      "True negative:  258\n",
      "False negative:  29\n",
      "-------------\n",
      "Accuracy:  92.3913043478261\n",
      "F1 Score:  0.899135446685879\n",
      "Precision:  0.9122807017543859\n",
      "Recall:  0.8863636363636364\n",
      "Specificity:  0.9471830985915493\n",
      "Sensitivity:  0.8863636363636364\n",
      "True positive:  156\n",
      "False positive:  15\n",
      "True negative:  269\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  93.91304347826087\n",
      "F1 Score:  0.9186046511627907\n",
      "Precision:  0.9461077844311377\n",
      "Recall:  0.8926553672316384\n",
      "Specificity:  0.9681978798586572\n",
      "Sensitivity:  0.8926553672316384\n",
      "True positive:  158\n",
      "False positive:  9\n",
      "True negative:  274\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  91.08695652173913\n",
      "F1 Score:  0.8951406649616369\n",
      "Precision:  0.9408602150537635\n",
      "Recall:  0.8536585365853658\n",
      "Specificity:  0.9568627450980393\n",
      "Sensitivity:  0.8536585365853658\n",
      "True positive:  175\n",
      "False positive:  11\n",
      "True negative:  244\n",
      "False negative:  30\n",
      "-------------\n",
      "Accuracy:  93.04347826086956\n",
      "F1 Score:  0.907514450867052\n",
      "Precision:  0.9631901840490797\n",
      "Recall:  0.8579234972677595\n",
      "Specificity:  0.9783393501805054\n",
      "Sensitivity:  0.8579234972677595\n",
      "True positive:  157\n",
      "False positive:  6\n",
      "True negative:  271\n",
      "False negative:  26\n",
      "-------------\n",
      "Accuracy:  93.058568329718\n",
      "F1 Score:  0.9090909090909091\n",
      "Precision:  0.8938547486033519\n",
      "Recall:  0.9248554913294798\n",
      "Specificity:  0.9340277777777778\n",
      "Sensitivity:  0.9248554913294798\n",
      "True positive:  160\n",
      "False positive:  19\n",
      "True negative:  269\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  93.04347826086956\n",
      "F1 Score:  0.9041916167664672\n",
      "Precision:  0.9320987654320988\n",
      "Recall:  0.877906976744186\n",
      "Specificity:  0.9618055555555556\n",
      "Sensitivity:  0.877906976744186\n",
      "True positive:  151\n",
      "False positive:  11\n",
      "True negative:  277\n",
      "False negative:  21\n",
      "-------------\n",
      "Accuracy:  94.1304347826087\n",
      "F1 Score:  0.9272237196765499\n",
      "Precision:  0.9555555555555556\n",
      "Recall:  0.900523560209424\n",
      "Specificity:  0.9702602230483272\n",
      "Sensitivity:  0.900523560209424\n",
      "True positive:  172\n",
      "False positive:  8\n",
      "True negative:  261\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.26086956521739\n",
      "F1 Score:  0.9159891598915989\n",
      "Precision:  0.9388888888888889\n",
      "Recall:  0.8941798941798942\n",
      "Specificity:  0.959409594095941\n",
      "Sensitivity:  0.8941798941798942\n",
      "True positive:  169\n",
      "False positive:  11\n",
      "True negative:  260\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  94.56521739130434\n",
      "F1 Score:  0.9326145552560646\n",
      "Precision:  0.9402173913043478\n",
      "Recall:  0.9251336898395722\n",
      "Specificity:  0.9597069597069597\n",
      "Sensitivity:  0.9251336898395722\n",
      "True positive:  173\n",
      "False positive:  11\n",
      "True negative:  262\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  93.69565217391305\n",
      "F1 Score:  0.9178470254957507\n",
      "Precision:  0.9101123595505618\n",
      "Recall:  0.9257142857142857\n",
      "Specificity:  0.9438596491228071\n",
      "Sensitivity:  0.9257142857142857\n",
      "True positive:  162\n",
      "False positive:  16\n",
      "True negative:  269\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  90.21739130434783\n",
      "F1 Score:  0.8767123287671232\n",
      "Precision:  0.9411764705882353\n",
      "Recall:  0.8205128205128205\n",
      "Specificity:  0.9622641509433962\n",
      "Sensitivity:  0.8205128205128205\n",
      "True positive:  160\n",
      "False positive:  10\n",
      "True negative:  255\n",
      "False negative:  35\n",
      "-------------\n",
      "Accuracy:  92.17391304347827\n",
      "F1 Score:  0.8922155688622753\n",
      "Precision:  0.8975903614457831\n",
      "Recall:  0.8869047619047619\n",
      "Specificity:  0.9417808219178082\n",
      "Sensitivity:  0.8869047619047619\n",
      "True positive:  149\n",
      "False positive:  17\n",
      "True negative:  275\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  91.73913043478261\n",
      "F1 Score:  0.8932584269662922\n",
      "Precision:  0.9085714285714286\n",
      "Recall:  0.8784530386740331\n",
      "Specificity:  0.942652329749104\n",
      "Sensitivity:  0.8784530386740331\n",
      "True positive:  159\n",
      "False positive:  16\n",
      "True negative:  263\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  92.6086956521739\n",
      "F1 Score:  0.9034090909090909\n",
      "Precision:  0.9352941176470588\n",
      "Recall:  0.8736263736263736\n",
      "Specificity:  0.960431654676259\n",
      "Sensitivity:  0.8736263736263736\n",
      "True positive:  159\n",
      "False positive:  11\n",
      "True negative:  267\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  92.40780911062907\n",
      "F1 Score:  0.906166219839142\n",
      "Precision:  0.9086021505376344\n",
      "Recall:  0.9037433155080213\n",
      "Specificity:  0.9379562043795621\n",
      "Sensitivity:  0.9037433155080213\n",
      "True positive:  169\n",
      "False positive:  17\n",
      "True negative:  257\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  91.95652173913044\n",
      "F1 Score:  0.8840125391849529\n",
      "Precision:  0.8924050632911392\n",
      "Recall:  0.8757763975155279\n",
      "Specificity:  0.9431438127090301\n",
      "Sensitivity:  0.8757763975155279\n",
      "True positive:  141\n",
      "False positive:  17\n",
      "True negative:  282\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  91.30434782608695\n",
      "F1 Score:  0.8974358974358975\n",
      "Precision:  0.9020618556701031\n",
      "Recall:  0.8928571428571429\n",
      "Specificity:  0.928030303030303\n",
      "Sensitivity:  0.8928571428571429\n",
      "True positive:  175\n",
      "False positive:  19\n",
      "True negative:  245\n",
      "False negative:  21\n",
      "-------------\n",
      "Accuracy:  93.26086956521739\n",
      "F1 Score:  0.9106628242074928\n",
      "Precision:  0.9239766081871345\n",
      "Recall:  0.8977272727272727\n",
      "Specificity:  0.954225352112676\n",
      "Sensitivity:  0.8977272727272727\n",
      "True positive:  158\n",
      "False positive:  13\n",
      "True negative:  271\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  93.69565217391305\n",
      "F1 Score:  0.9144542772861357\n",
      "Precision:  0.9226190476190477\n",
      "Recall:  0.9064327485380117\n",
      "Specificity:  0.9550173010380623\n",
      "Sensitivity:  0.9064327485380117\n",
      "True positive:  155\n",
      "False positive:  13\n",
      "True negative:  276\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  93.04347826086956\n",
      "F1 Score:  0.9162303664921466\n",
      "Precision:  0.9259259259259259\n",
      "Recall:  0.9067357512953368\n",
      "Specificity:  0.947565543071161\n",
      "Sensitivity:  0.9067357512953368\n",
      "True positive:  175\n",
      "False positive:  14\n",
      "True negative:  253\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  95.0\n",
      "F1 Score:  0.9321533923303835\n",
      "Precision:  0.9753086419753086\n",
      "Recall:  0.8926553672316384\n",
      "Specificity:  0.9858657243816255\n",
      "Sensitivity:  0.8926553672316384\n",
      "True positive:  158\n",
      "False positive:  4\n",
      "True negative:  279\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  94.34782608695652\n",
      "F1 Score:  0.9235294117647059\n",
      "Precision:  0.9515151515151515\n",
      "Recall:  0.8971428571428571\n",
      "Specificity:  0.9719298245614035\n",
      "Sensitivity:  0.8971428571428571\n",
      "True positive:  157\n",
      "False positive:  8\n",
      "True negative:  277\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  90.65217391304347\n",
      "F1 Score:  0.8788732394366197\n",
      "Precision:  0.9176470588235294\n",
      "Recall:  0.8432432432432433\n",
      "Specificity:  0.9490909090909091\n",
      "Sensitivity:  0.8432432432432433\n",
      "True positive:  156\n",
      "False positive:  14\n",
      "True negative:  261\n",
      "False negative:  29\n",
      "-------------\n",
      "Accuracy:  93.26086956521739\n",
      "F1 Score:  0.9186351706036746\n",
      "Precision:  0.9259259259259259\n",
      "Recall:  0.9114583333333334\n",
      "Specificity:  0.9477611940298507\n",
      "Sensitivity:  0.9114583333333334\n",
      "True positive:  175\n",
      "False positive:  14\n",
      "True negative:  254\n",
      "False negative:  17\n",
      "-------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  93.058568329718\n",
      "F1 Score:  0.9096045197740114\n",
      "Precision:  0.9415204678362573\n",
      "Recall:  0.8797814207650273\n",
      "Specificity:  0.9640287769784173\n",
      "Sensitivity:  0.8797814207650273\n",
      "True positive:  161\n",
      "False positive:  10\n",
      "True negative:  268\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  90.8695652173913\n",
      "F1 Score:  0.8771929824561403\n",
      "Precision:  0.8670520231213873\n",
      "Recall:  0.8875739644970414\n",
      "Specificity:  0.9209621993127147\n",
      "Sensitivity:  0.8875739644970414\n",
      "True positive:  150\n",
      "False positive:  23\n",
      "True negative:  268\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  91.95652173913044\n",
      "F1 Score:  0.8969359331476323\n",
      "Precision:  0.9252873563218391\n",
      "Recall:  0.8702702702702703\n",
      "Specificity:  0.9527272727272728\n",
      "Sensitivity:  0.8702702702702703\n",
      "True positive:  161\n",
      "False positive:  13\n",
      "True negative:  262\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  93.69565217391305\n",
      "F1 Score:  0.9164265129682997\n",
      "Precision:  0.9298245614035088\n",
      "Recall:  0.9034090909090909\n",
      "Specificity:  0.9577464788732394\n",
      "Sensitivity:  0.9034090909090909\n",
      "True positive:  159\n",
      "False positive:  12\n",
      "True negative:  272\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  92.82608695652173\n",
      "F1 Score:  0.906515580736544\n",
      "Precision:  0.898876404494382\n",
      "Recall:  0.9142857142857143\n",
      "Specificity:  0.9368421052631579\n",
      "Sensitivity:  0.9142857142857143\n",
      "True positive:  160\n",
      "False positive:  18\n",
      "True negative:  267\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  93.26086956521739\n",
      "F1 Score:  0.9069069069069069\n",
      "Precision:  0.9263803680981595\n",
      "Recall:  0.888235294117647\n",
      "Specificity:  0.9586206896551724\n",
      "Sensitivity:  0.888235294117647\n",
      "True positive:  151\n",
      "False positive:  12\n",
      "True negative:  278\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.26086956521739\n",
      "F1 Score:  0.9168900804289545\n",
      "Precision:  0.95\n",
      "Recall:  0.8860103626943006\n",
      "Specificity:  0.9662921348314607\n",
      "Sensitivity:  0.8860103626943006\n",
      "True positive:  171\n",
      "False positive:  9\n",
      "True negative:  258\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  92.82608695652173\n",
      "F1 Score:  0.9054441260744985\n",
      "Precision:  0.9404761904761905\n",
      "Recall:  0.8729281767955801\n",
      "Specificity:  0.96415770609319\n",
      "Sensitivity:  0.8729281767955801\n",
      "True positive:  158\n",
      "False positive:  10\n",
      "True negative:  269\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  93.69565217391305\n",
      "F1 Score:  0.9265822784810125\n",
      "Precision:  0.9336734693877551\n",
      "Recall:  0.9195979899497487\n",
      "Specificity:  0.9501915708812261\n",
      "Sensitivity:  0.9195979899497487\n",
      "True positive:  183\n",
      "False positive:  13\n",
      "True negative:  248\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  93.91304347826087\n",
      "F1 Score:  0.9234972677595629\n",
      "Precision:  0.9184782608695652\n",
      "Recall:  0.9285714285714286\n",
      "Specificity:  0.9460431654676259\n",
      "Sensitivity:  0.9285714285714286\n",
      "True positive:  169\n",
      "False positive:  15\n",
      "True negative:  263\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  94.14316702819957\n",
      "F1 Score:  0.9279999999999999\n",
      "Precision:  0.9508196721311475\n",
      "Recall:  0.90625\n",
      "Specificity:  0.966542750929368\n",
      "Sensitivity:  0.90625\n",
      "True positive:  174\n",
      "False positive:  9\n",
      "True negative:  260\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  92.6086956521739\n",
      "F1 Score:  0.9\n",
      "Precision:  0.9386503067484663\n",
      "Recall:  0.864406779661017\n",
      "Specificity:  0.9646643109540636\n",
      "Sensitivity:  0.864406779661017\n",
      "True positive:  153\n",
      "False positive:  10\n",
      "True negative:  273\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  94.34782608695652\n",
      "F1 Score:  0.9281767955801105\n",
      "Precision:  0.9385474860335196\n",
      "Recall:  0.9180327868852459\n",
      "Specificity:  0.9602888086642599\n",
      "Sensitivity:  0.9180327868852459\n",
      "True positive:  168\n",
      "False positive:  11\n",
      "True negative:  266\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  92.6086956521739\n",
      "F1 Score:  0.9119170984455959\n",
      "Precision:  0.8934010152284264\n",
      "Recall:  0.9312169312169312\n",
      "Specificity:  0.922509225092251\n",
      "Sensitivity:  0.9312169312169312\n",
      "True positive:  176\n",
      "False positive:  21\n",
      "True negative:  250\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  93.69565217391305\n",
      "F1 Score:  0.913946587537092\n",
      "Precision:  0.927710843373494\n",
      "Recall:  0.9005847953216374\n",
      "Specificity:  0.9584775086505191\n",
      "Sensitivity:  0.9005847953216374\n",
      "True positive:  154\n",
      "False positive:  12\n",
      "True negative:  277\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  91.52173913043478\n",
      "F1 Score:  0.8814589665653494\n",
      "Precision:  0.9119496855345912\n",
      "Recall:  0.8529411764705882\n",
      "Specificity:  0.9517241379310345\n",
      "Sensitivity:  0.8529411764705882\n",
      "True positive:  145\n",
      "False positive:  14\n",
      "True negative:  276\n",
      "False negative:  25\n",
      "-------------\n",
      "Accuracy:  91.73913043478261\n",
      "F1 Score:  0.8967391304347826\n",
      "Precision:  0.9217877094972067\n",
      "Recall:  0.873015873015873\n",
      "Specificity:  0.948339483394834\n",
      "Sensitivity:  0.873015873015873\n",
      "True positive:  165\n",
      "False positive:  14\n",
      "True negative:  257\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  92.82608695652173\n",
      "F1 Score:  0.9129287598944591\n",
      "Precision:  0.9251336898395722\n",
      "Recall:  0.9010416666666666\n",
      "Specificity:  0.9477611940298507\n",
      "Sensitivity:  0.9010416666666666\n",
      "True positive:  173\n",
      "False positive:  14\n",
      "True negative:  254\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.04347826086956\n",
      "F1 Score:  0.9120879120879122\n",
      "Precision:  0.9273743016759777\n",
      "Recall:  0.8972972972972973\n",
      "Specificity:  0.9527272727272728\n",
      "Sensitivity:  0.8972972972972973\n",
      "True positive:  166\n",
      "False positive:  13\n",
      "True negative:  262\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.91304347826087\n",
      "F1 Score:  0.9135802469135803\n",
      "Precision:  0.9308176100628931\n",
      "Recall:  0.896969696969697\n",
      "Specificity:  0.9627118644067797\n",
      "Sensitivity:  0.896969696969697\n",
      "True positive:  148\n",
      "False positive:  11\n",
      "True negative:  284\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  92.19088937093277\n",
      "F1 Score:  0.901639344262295\n",
      "Precision:  0.9217877094972067\n",
      "Recall:  0.8823529411764706\n",
      "Specificity:  0.948905109489051\n",
      "Sensitivity:  0.8823529411764706\n",
      "True positive:  165\n",
      "False positive:  14\n",
      "True negative:  260\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  91.52173913043478\n",
      "F1 Score:  0.8842729970326408\n",
      "Precision:  0.8713450292397661\n",
      "Recall:  0.8975903614457831\n",
      "Specificity:  0.9251700680272109\n",
      "Sensitivity:  0.8975903614457831\n",
      "True positive:  149\n",
      "False positive:  22\n",
      "True negative:  272\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  91.95652173913044\n",
      "F1 Score:  0.8945868945868946\n",
      "Precision:  0.8971428571428571\n",
      "Recall:  0.8920454545454546\n",
      "Specificity:  0.9366197183098591\n",
      "Sensitivity:  0.8920454545454546\n",
      "True positive:  157\n",
      "False positive:  18\n",
      "True negative:  266\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  94.34782608695652\n",
      "F1 Score:  0.9293478260869565\n",
      "Precision:  0.9293478260869565\n",
      "Recall:  0.9293478260869565\n",
      "Specificity:  0.9528985507246377\n",
      "Sensitivity:  0.9293478260869565\n",
      "True positive:  171\n",
      "False positive:  13\n",
      "True negative:  263\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  94.34782608695652\n",
      "F1 Score:  0.9277777777777778\n",
      "Precision:  0.9542857142857143\n",
      "Recall:  0.9027027027027027\n",
      "Specificity:  0.9709090909090909\n",
      "Sensitivity:  0.9027027027027027\n",
      "True positive:  167\n",
      "False positive:  8\n",
      "True negative:  267\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  93.04347826086956\n",
      "F1 Score:  0.9106145251396649\n",
      "Precision:  0.9261363636363636\n",
      "Recall:  0.8956043956043956\n",
      "Specificity:  0.9532374100719424\n",
      "Sensitivity:  0.8956043956043956\n",
      "True positive:  163\n",
      "False positive:  13\n",
      "True negative:  265\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.26086956521739\n",
      "F1 Score:  0.9121813031161473\n",
      "Precision:  0.930635838150289\n",
      "Recall:  0.8944444444444445\n",
      "Specificity:  0.9571428571428572\n",
      "Sensitivity:  0.8944444444444445\n",
      "True positive:  161\n",
      "False positive:  12\n",
      "True negative:  268\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.04347826086956\n",
      "F1 Score:  0.9153439153439153\n",
      "Precision:  0.9351351351351351\n",
      "Recall:  0.8963730569948186\n",
      "Specificity:  0.9550561797752809\n",
      "Sensitivity:  0.8963730569948186\n",
      "True positive:  173\n",
      "False positive:  12\n",
      "True negative:  255\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  93.26086956521739\n",
      "F1 Score:  0.909090909090909\n",
      "Precision:  0.9281437125748503\n",
      "Recall:  0.8908045977011494\n",
      "Specificity:  0.958041958041958\n",
      "Sensitivity:  0.8908045977011494\n",
      "True positive:  155\n",
      "False positive:  12\n",
      "True negative:  274\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  92.3913043478261\n",
      "F1 Score:  0.9025069637883008\n",
      "Precision:  0.9364161849710982\n",
      "Recall:  0.8709677419354839\n",
      "Specificity:  0.9598540145985401\n",
      "Sensitivity:  0.8709677419354839\n",
      "True positive:  162\n",
      "False positive:  11\n",
      "True negative:  263\n",
      "False negative:  24\n",
      "-------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  91.54013015184381\n",
      "F1 Score:  0.8828828828828829\n",
      "Precision:  0.901840490797546\n",
      "Recall:  0.8647058823529412\n",
      "Specificity:  0.9450171821305842\n",
      "Sensitivity:  0.8647058823529412\n",
      "True positive:  147\n",
      "False positive:  16\n",
      "True negative:  275\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  91.08695652173913\n",
      "F1 Score:  0.895674300254453\n",
      "Precision:  0.9166666666666666\n",
      "Recall:  0.8756218905472637\n",
      "Specificity:  0.9382239382239382\n",
      "Sensitivity:  0.8756218905472637\n",
      "True positive:  176\n",
      "False positive:  16\n",
      "True negative:  243\n",
      "False negative:  25\n",
      "-------------\n",
      "Accuracy:  93.26086956521739\n",
      "F1 Score:  0.9159891598915989\n",
      "Precision:  0.9285714285714286\n",
      "Recall:  0.9037433155080213\n",
      "Specificity:  0.9523809523809523\n",
      "Sensitivity:  0.9037433155080213\n",
      "True positive:  169\n",
      "False positive:  13\n",
      "True negative:  260\n",
      "False negative:  18\n",
      "-------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-0316757a6844>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mlogistic_regression_model_imbalanced\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mlogistic_regression_model_imbalanced\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_imbalanced\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_imbalanced\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mprediction_imbalanced\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression_model_imbalanced\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_imbalanced\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1415\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1416\u001b[0m                       sample_weight=sample_weight)\n\u001b[1;32m-> 1417\u001b[1;33m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[0;32m   1418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[0;32m    758\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"L-BFGS-B\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 760\u001b[1;33m                 \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"iprint\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0miprint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"gtol\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"maxiter\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    761\u001b[0m             )\n\u001b[0;32m    762\u001b[0m             n_iter_i = _check_optimize_result(\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'l-bfgs-b'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m--> 618\u001b[1;33m                                 callback=callback, **options)\n\u001b[0m\u001b[0;32m    619\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[1;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnfev\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;34m\"\"\" returns the the function value \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_logistic_loss_and_grad\u001b[1;34m(w, X, y, alpha, sample_weight)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m     \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_intercept_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_intercept_dot\u001b[1;34m(w, X, y)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m     \u001b[0myz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "\n",
    "for train_index,test_index in kf.split(X_imbalanced):\n",
    "    X_train_imbalanced,X_test_imbalanced=X_imbalanced[train_index],X_imbalanced[test_index]\n",
    "    y_train_imbalanced,y_test_imbalanced=y_imbalanced[train_index],y_imbalanced[test_index]\n",
    "    \n",
    "    logistic_regression_model_imbalanced = LogisticRegression(max_iter = 3000)\n",
    "    logistic_regression_model_imbalanced.fit(X_train_imbalanced,y_train_imbalanced)\n",
    "    prediction_imbalanced = logistic_regression_model_imbalanced.predict(X_test_imbalanced)\n",
    "    \n",
    "    conf_matrix_imbalanced = confusion_matrix(y_true=y_test_imbalanced,y_pred=prediction_imbalanced)\n",
    "    TP_imbalanced = conf_matrix_imbalanced[1,1]\n",
    "    TN_imbalanced = conf_matrix_imbalanced[0,0]\n",
    "    FP_imbalanced = conf_matrix_imbalanced[0,1]\n",
    "    FN_imbalanced = conf_matrix_imbalanced[1,0]\n",
    "    sensitivity_imbalanced = TP_imbalanced/(TP_imbalanced+FN_imbalanced)\n",
    "    specificity_imbalanced = TN_imbalanced/(TN_imbalanced+FP_imbalanced)\n",
    "    accuracy_imbalanced = accuracy_score(y_test_imbalanced,prediction_imbalanced)*100\n",
    "    f1_imbalanced = f1_score(y_test_imbalanced, prediction_imbalanced)\n",
    "    precision_imbalanced = precision_score(y_test_imbalanced, prediction_imbalanced)\n",
    "    recall_imbalanced = recall_score(y_test_imbalanced, prediction_imbalanced)\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy_imbalanced)\n",
    "    print(\"F1 Score: \", f1_imbalanced)\n",
    "    print(\"Precision: \", precision_imbalanced)\n",
    "    print(\"Recall: \", recall_imbalanced)\n",
    "    print(\"Specificity: \", specificity_imbalanced)\n",
    "    print(\"Sensitivity: \", sensitivity_imbalanced)\n",
    "    print(\"True positive: \", TP_imbalanced)\n",
    "    print(\"False positive: \", FP_imbalanced)\n",
    "    print(\"True negative: \", TN_imbalanced)\n",
    "    print(\"False negative: \", FN_imbalanced)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  92.8374655647383\n",
      "F1 Score:  0.9252873563218391\n",
      "Precision:  0.930635838150289\n",
      "Recall:  0.92\n",
      "Specificity:  0.9361702127659575\n",
      "Sensitivity:  0.92\n",
      "True positive:  161\n",
      "False positive:  12\n",
      "True negative:  176\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  92.8374655647383\n",
      "F1 Score:  0.9269662921348314\n",
      "Precision:  0.9322033898305084\n",
      "Recall:  0.9217877094972067\n",
      "Specificity:  0.9347826086956522\n",
      "Sensitivity:  0.9217877094972067\n",
      "True positive:  165\n",
      "False positive:  12\n",
      "True negative:  172\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  89.25619834710744\n",
      "F1 Score:  0.8876080691642652\n",
      "Precision:  0.9005847953216374\n",
      "Recall:  0.875\n",
      "Specificity:  0.9090909090909091\n",
      "Sensitivity:  0.875\n",
      "True positive:  154\n",
      "False positive:  17\n",
      "True negative:  170\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  94.21487603305785\n",
      "F1 Score:  0.9405099150141643\n",
      "Precision:  0.9431818181818182\n",
      "Recall:  0.9378531073446328\n",
      "Specificity:  0.946236559139785\n",
      "Sensitivity:  0.9378531073446328\n",
      "True positive:  166\n",
      "False positive:  10\n",
      "True negative:  176\n",
      "False negative:  11\n",
      "-------------\n",
      "Accuracy:  93.66391184573003\n",
      "F1 Score:  0.9355742296918768\n",
      "Precision:  0.9653179190751445\n",
      "Recall:  0.907608695652174\n",
      "Specificity:  0.9664804469273743\n",
      "Sensitivity:  0.907608695652174\n",
      "True positive:  167\n",
      "False positive:  6\n",
      "True negative:  173\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  91.18457300275482\n",
      "F1 Score:  0.9106145251396649\n",
      "Precision:  0.9261363636363636\n",
      "Recall:  0.8956043956043956\n",
      "Specificity:  0.9281767955801105\n",
      "Sensitivity:  0.8956043956043956\n",
      "True positive:  163\n",
      "False positive:  13\n",
      "True negative:  168\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.646408839779\n",
      "F1 Score:  0.9399477806788512\n",
      "Precision:  0.9424083769633508\n",
      "Recall:  0.9375\n",
      "Specificity:  0.9352941176470588\n",
      "Sensitivity:  0.9375\n",
      "True positive:  180\n",
      "False positive:  11\n",
      "True negative:  159\n",
      "False negative:  12\n",
      "-------------\n",
      "Accuracy:  93.37016574585635\n",
      "F1 Score:  0.9393939393939393\n",
      "Precision:  0.9441624365482234\n",
      "Recall:  0.9346733668341709\n",
      "Specificity:  0.9325153374233128\n",
      "Sensitivity:  0.9346733668341709\n",
      "True positive:  186\n",
      "False positive:  11\n",
      "True negative:  152\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  93.37016574585635\n",
      "F1 Score:  0.9298245614035088\n",
      "Precision:  0.9085714285714286\n",
      "Recall:  0.9520958083832335\n",
      "Specificity:  0.9179487179487179\n",
      "Sensitivity:  0.9520958083832335\n",
      "True positive:  159\n",
      "False positive:  16\n",
      "True negative:  179\n",
      "False negative:  8\n",
      "-------------\n",
      "Accuracy:  92.5414364640884\n",
      "F1 Score:  0.9247910863509751\n",
      "Precision:  0.9378531073446328\n",
      "Recall:  0.9120879120879121\n",
      "Specificity:  0.9388888888888889\n",
      "Sensitivity:  0.9120879120879121\n",
      "True positive:  166\n",
      "False positive:  11\n",
      "True negative:  169\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  93.38842975206612\n",
      "F1 Score:  0.9351351351351351\n",
      "Precision:  0.9664804469273743\n",
      "Recall:  0.9057591623036649\n",
      "Specificity:  0.9651162790697675\n",
      "Sensitivity:  0.9057591623036649\n",
      "True positive:  173\n",
      "False positive:  6\n",
      "True negative:  166\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  91.46005509641874\n",
      "F1 Score:  0.9173333333333333\n",
      "Precision:  0.9197860962566845\n",
      "Recall:  0.9148936170212766\n",
      "Specificity:  0.9142857142857143\n",
      "Sensitivity:  0.9148936170212766\n",
      "True positive:  172\n",
      "False positive:  15\n",
      "True negative:  160\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  93.66391184573003\n",
      "F1 Score:  0.9317507418397627\n",
      "Precision:  0.9345238095238095\n",
      "Recall:  0.9289940828402367\n",
      "Specificity:  0.9432989690721649\n",
      "Sensitivity:  0.9289940828402367\n",
      "True positive:  157\n",
      "False positive:  11\n",
      "True negative:  183\n",
      "False negative:  12\n",
      "-------------\n",
      "Accuracy:  93.1129476584022\n",
      "F1 Score:  0.9347258485639687\n",
      "Precision:  0.9572192513368984\n",
      "Recall:  0.9132653061224489\n",
      "Specificity:  0.9520958083832335\n",
      "Sensitivity:  0.9132653061224489\n",
      "True positive:  179\n",
      "False positive:  8\n",
      "True negative:  159\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  91.18457300275482\n",
      "F1 Score:  0.909090909090909\n",
      "Precision:  0.8695652173913043\n",
      "Recall:  0.9523809523809523\n",
      "Specificity:  0.8769230769230769\n",
      "Sensitivity:  0.9523809523809523\n",
      "True positive:  160\n",
      "False positive:  24\n",
      "True negative:  171\n",
      "False negative:  8\n",
      "-------------\n",
      "Accuracy:  92.56198347107438\n",
      "F1 Score:  0.9272237196765498\n",
      "Precision:  0.9197860962566845\n",
      "Recall:  0.9347826086956522\n",
      "Specificity:  0.9162011173184358\n",
      "Sensitivity:  0.9347826086956522\n",
      "True positive:  172\n",
      "False positive:  15\n",
      "True negative:  164\n",
      "False negative:  12\n",
      "-------------\n",
      "Accuracy:  93.37016574585635\n",
      "F1 Score:  0.9325842696629213\n",
      "Precision:  0.9651162790697675\n",
      "Recall:  0.9021739130434783\n",
      "Specificity:  0.9662921348314607\n",
      "Sensitivity:  0.9021739130434783\n",
      "True positive:  166\n",
      "False positive:  6\n",
      "True negative:  172\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  91.71270718232044\n",
      "F1 Score:  0.9157303370786517\n",
      "Precision:  0.9157303370786517\n",
      "Recall:  0.9157303370786517\n",
      "Specificity:  0.9184782608695652\n",
      "Sensitivity:  0.9157303370786517\n",
      "True positive:  163\n",
      "False positive:  15\n",
      "True negative:  169\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  93.646408839779\n",
      "F1 Score:  0.9337175792507205\n",
      "Precision:  0.9418604651162791\n",
      "Recall:  0.9257142857142857\n",
      "Specificity:  0.946524064171123\n",
      "Sensitivity:  0.9257142857142857\n",
      "True positive:  162\n",
      "False positive:  10\n",
      "True negative:  177\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  93.37016574585635\n",
      "F1 Score:  0.9318181818181819\n",
      "Precision:  0.9534883720930233\n",
      "Recall:  0.9111111111111111\n",
      "Specificity:  0.9560439560439561\n",
      "Sensitivity:  0.9111111111111111\n",
      "True positive:  164\n",
      "False positive:  8\n",
      "True negative:  174\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  90.633608815427\n",
      "F1 Score:  0.9022988505747127\n",
      "Precision:  0.9289940828402367\n",
      "Recall:  0.8770949720670391\n",
      "Specificity:  0.9347826086956522\n",
      "Sensitivity:  0.8770949720670391\n",
      "True positive:  157\n",
      "False positive:  12\n",
      "True negative:  172\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  91.18457300275482\n",
      "F1 Score:  0.9069767441860466\n",
      "Precision:  0.9122807017543859\n",
      "Recall:  0.9017341040462428\n",
      "Specificity:  0.9210526315789473\n",
      "Sensitivity:  0.9017341040462428\n",
      "True positive:  156\n",
      "False positive:  15\n",
      "True negative:  175\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  94.21487603305785\n",
      "F1 Score:  0.9462915601023019\n",
      "Precision:  0.9390862944162437\n",
      "Recall:  0.9536082474226805\n",
      "Specificity:  0.9289940828402367\n",
      "Sensitivity:  0.9536082474226805\n",
      "True positive:  185\n",
      "False positive:  12\n",
      "True negative:  157\n",
      "False negative:  9\n",
      "-------------\n",
      "Accuracy:  92.01101928374655\n",
      "F1 Score:  0.9205479452054794\n",
      "Precision:  0.9281767955801105\n",
      "Recall:  0.9130434782608695\n",
      "Specificity:  0.9273743016759777\n",
      "Sensitivity:  0.9130434782608695\n",
      "True positive:  168\n",
      "False positive:  13\n",
      "True negative:  166\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  90.35812672176309\n",
      "F1 Score:  0.8985507246376812\n",
      "Precision:  0.8908045977011494\n",
      "Recall:  0.9064327485380117\n",
      "Specificity:  0.9010416666666666\n",
      "Sensitivity:  0.9064327485380117\n",
      "True positive:  155\n",
      "False positive:  19\n",
      "True negative:  173\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  93.38842975206612\n",
      "F1 Score:  0.9344262295081968\n",
      "Precision:  0.9193548387096774\n",
      "Recall:  0.95\n",
      "Specificity:  0.9180327868852459\n",
      "Sensitivity:  0.95\n",
      "True positive:  171\n",
      "False positive:  15\n",
      "True negative:  168\n",
      "False negative:  9\n",
      "-------------\n",
      "Accuracy:  91.9889502762431\n",
      "F1 Score:  0.9226666666666669\n",
      "Precision:  0.9202127659574468\n",
      "Recall:  0.9251336898395722\n",
      "Specificity:  0.9142857142857143\n",
      "Sensitivity:  0.9251336898395722\n",
      "True positive:  173\n",
      "False positive:  15\n",
      "True negative:  160\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  93.646408839779\n",
      "F1 Score:  0.9373297002724795\n",
      "Precision:  0.945054945054945\n",
      "Recall:  0.9297297297297298\n",
      "Specificity:  0.943502824858757\n",
      "Sensitivity:  0.9297297297297298\n",
      "True positive:  172\n",
      "False positive:  10\n",
      "True negative:  167\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  93.646408839779\n",
      "F1 Score:  0.9352112676056339\n",
      "Precision:  0.9651162790697675\n",
      "Recall:  0.907103825136612\n",
      "Specificity:  0.9664804469273743\n",
      "Sensitivity:  0.907103825136612\n",
      "True positive:  166\n",
      "False positive:  6\n",
      "True negative:  173\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  92.5414364640884\n",
      "F1 Score:  0.9217391304347826\n",
      "Precision:  0.9464285714285714\n",
      "Recall:  0.8983050847457628\n",
      "Specificity:  0.9513513513513514\n",
      "Sensitivity:  0.8983050847457628\n",
      "True positive:  159\n",
      "False positive:  9\n",
      "True negative:  176\n",
      "False negative:  18\n",
      "-------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  92.01101928374655\n",
      "F1 Score:  0.9201101928374656\n",
      "Precision:  0.9175824175824175\n",
      "Recall:  0.9226519337016574\n",
      "Specificity:  0.9175824175824175\n",
      "Sensitivity:  0.9226519337016574\n",
      "True positive:  167\n",
      "False positive:  15\n",
      "True negative:  167\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  92.56198347107438\n",
      "F1 Score:  0.9221902017291066\n",
      "Precision:  0.9195402298850575\n",
      "Recall:  0.9248554913294798\n",
      "Specificity:  0.9263157894736842\n",
      "Sensitivity:  0.9248554913294798\n",
      "True positive:  160\n",
      "False positive:  14\n",
      "True negative:  176\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  94.21487603305785\n",
      "F1 Score:  0.9433962264150944\n",
      "Precision:  0.9408602150537635\n",
      "Recall:  0.9459459459459459\n",
      "Specificity:  0.9382022471910112\n",
      "Sensitivity:  0.9459459459459459\n",
      "True positive:  175\n",
      "False positive:  11\n",
      "True negative:  167\n",
      "False negative:  10\n",
      "-------------\n",
      "Accuracy:  92.8374655647383\n",
      "F1 Score:  0.9226190476190477\n",
      "Precision:  0.9281437125748503\n",
      "Recall:  0.9171597633136095\n",
      "Specificity:  0.9381443298969072\n",
      "Sensitivity:  0.9171597633136095\n",
      "True positive:  155\n",
      "False positive:  12\n",
      "True negative:  182\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  92.28650137741047\n",
      "F1 Score:  0.9267015706806283\n",
      "Precision:  0.9516129032258065\n",
      "Recall:  0.9030612244897959\n",
      "Specificity:  0.9461077844311377\n",
      "Sensitivity:  0.9030612244897959\n",
      "True positive:  177\n",
      "False positive:  9\n",
      "True negative:  158\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.66391184573003\n",
      "F1 Score:  0.9386666666666668\n",
      "Precision:  0.9513513513513514\n",
      "Recall:  0.9263157894736842\n",
      "Specificity:  0.9479768786127167\n",
      "Sensitivity:  0.9263157894736842\n",
      "True positive:  176\n",
      "False positive:  9\n",
      "True negative:  164\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  91.16022099447514\n",
      "F1 Score:  0.9120879120879122\n",
      "Precision:  0.9273743016759777\n",
      "Recall:  0.8972972972972973\n",
      "Specificity:  0.9265536723163842\n",
      "Sensitivity:  0.8972972972972973\n",
      "True positive:  166\n",
      "False positive:  13\n",
      "True negative:  164\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.646408839779\n",
      "F1 Score:  0.9309309309309309\n",
      "Precision:  0.9451219512195121\n",
      "Recall:  0.9171597633136095\n",
      "Specificity:  0.9533678756476683\n",
      "Sensitivity:  0.9171597633136095\n",
      "True positive:  155\n",
      "False positive:  9\n",
      "True negative:  184\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  92.26519337016575\n",
      "F1 Score:  0.9243243243243244\n",
      "Precision:  0.9447513812154696\n",
      "Recall:  0.9047619047619048\n",
      "Specificity:  0.9421965317919075\n",
      "Sensitivity:  0.9047619047619048\n",
      "True positive:  171\n",
      "False positive:  10\n",
      "True negative:  163\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  91.71270718232044\n",
      "F1 Score:  0.9157303370786517\n",
      "Precision:  0.9055555555555556\n",
      "Recall:  0.9261363636363636\n",
      "Specificity:  0.9086021505376344\n",
      "Sensitivity:  0.9261363636363636\n",
      "True positive:  163\n",
      "False positive:  17\n",
      "True negative:  169\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  90.9090909090909\n",
      "F1 Score:  0.9095890410958904\n",
      "Precision:  0.9222222222222223\n",
      "Recall:  0.8972972972972973\n",
      "Specificity:  0.9213483146067416\n",
      "Sensitivity:  0.8972972972972973\n",
      "True positive:  166\n",
      "False positive:  14\n",
      "True negative:  164\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.93939393939394\n",
      "F1 Score:  0.9414893617021277\n",
      "Precision:  0.9516129032258065\n",
      "Recall:  0.9315789473684211\n",
      "Specificity:  0.9479768786127167\n",
      "Sensitivity:  0.9315789473684211\n",
      "True positive:  177\n",
      "False positive:  9\n",
      "True negative:  164\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  94.21487603305785\n",
      "F1 Score:  0.9418282548476454\n",
      "Precision:  0.9497206703910615\n",
      "Recall:  0.9340659340659341\n",
      "Specificity:  0.9502762430939227\n",
      "Sensitivity:  0.9340659340659341\n",
      "True positive:  170\n",
      "False positive:  9\n",
      "True negative:  172\n",
      "False negative:  12\n",
      "-------------\n",
      "Accuracy:  93.66391184573003\n",
      "F1 Score:  0.934844192634561\n",
      "Precision:  0.9593023255813954\n",
      "Recall:  0.9116022099447514\n",
      "Specificity:  0.9615384615384616\n",
      "Sensitivity:  0.9116022099447514\n",
      "True positive:  165\n",
      "False positive:  7\n",
      "True negative:  175\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  92.8374655647383\n",
      "F1 Score:  0.9297297297297297\n",
      "Precision:  0.9347826086956522\n",
      "Recall:  0.9247311827956989\n",
      "Specificity:  0.9322033898305084\n",
      "Sensitivity:  0.9247311827956989\n",
      "True positive:  172\n",
      "False positive:  12\n",
      "True negative:  165\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  93.93939393939394\n",
      "F1 Score:  0.9395604395604396\n",
      "Precision:  0.95\n",
      "Recall:  0.9293478260869565\n",
      "Specificity:  0.9497206703910615\n",
      "Sensitivity:  0.9293478260869565\n",
      "True positive:  171\n",
      "False positive:  9\n",
      "True negative:  170\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  92.5414364640884\n",
      "F1 Score:  0.9164086687306502\n",
      "Precision:  0.9079754601226994\n",
      "Recall:  0.925\n",
      "Specificity:  0.9257425742574258\n",
      "Sensitivity:  0.925\n",
      "True positive:  148\n",
      "False positive:  15\n",
      "True negative:  187\n",
      "False negative:  12\n",
      "-------------\n",
      "Accuracy:  88.67403314917127\n",
      "F1 Score:  0.8864265927977839\n",
      "Precision:  0.9142857142857143\n",
      "Recall:  0.8602150537634409\n",
      "Specificity:  0.9147727272727273\n",
      "Sensitivity:  0.8602150537634409\n",
      "True positive:  160\n",
      "False positive:  15\n",
      "True negative:  161\n",
      "False negative:  26\n",
      "-------------\n",
      "Accuracy:  94.1988950276243\n",
      "F1 Score:  0.9454545454545454\n",
      "Precision:  0.9430051813471503\n",
      "Recall:  0.9479166666666666\n",
      "Specificity:  0.9352941176470588\n",
      "Sensitivity:  0.9479166666666666\n",
      "True positive:  182\n",
      "False positive:  11\n",
      "True negative:  159\n",
      "False negative:  10\n",
      "-------------\n",
      "Accuracy:  93.0939226519337\n",
      "F1 Score:  0.927536231884058\n",
      "Precision:  0.898876404494382\n",
      "Recall:  0.9580838323353293\n",
      "Specificity:  0.9076923076923077\n",
      "Sensitivity:  0.9580838323353293\n",
      "True positive:  160\n",
      "False positive:  18\n",
      "True negative:  177\n",
      "False negative:  7\n",
      "-------------\n",
      "Accuracy:  93.1129476584022\n",
      "F1 Score:  0.9299719887955182\n",
      "Precision:  0.9378531073446328\n",
      "Recall:  0.9222222222222223\n",
      "Specificity:  0.9398907103825137\n",
      "Sensitivity:  0.9222222222222223\n",
      "True positive:  166\n",
      "False positive:  11\n",
      "True negative:  172\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  93.38842975206612\n",
      "F1 Score:  0.9306358381502892\n",
      "Precision:  0.936046511627907\n",
      "Recall:  0.9252873563218391\n",
      "Specificity:  0.9417989417989417\n",
      "Sensitivity:  0.9252873563218391\n",
      "True positive:  161\n",
      "False positive:  11\n",
      "True negative:  178\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  91.73553719008265\n",
      "F1 Score:  0.9152542372881356\n",
      "Precision:  0.9310344827586207\n",
      "Recall:  0.9\n",
      "Specificity:  0.9344262295081968\n",
      "Sensitivity:  0.9\n",
      "True positive:  162\n",
      "False positive:  12\n",
      "True negative:  171\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  90.35812672176309\n",
      "F1 Score:  0.8991354466858789\n",
      "Precision:  0.8813559322033898\n",
      "Recall:  0.9176470588235294\n",
      "Specificity:  0.8911917098445595\n",
      "Sensitivity:  0.9176470588235294\n",
      "True positive:  156\n",
      "False positive:  21\n",
      "True negative:  172\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  95.31680440771349\n",
      "F1 Score:  0.9526462395543176\n",
      "Precision:  0.9606741573033708\n",
      "Recall:  0.9447513812154696\n",
      "Specificity:  0.9615384615384616\n",
      "Sensitivity:  0.9447513812154696\n",
      "True positive:  171\n",
      "False positive:  7\n",
      "True negative:  175\n",
      "False negative:  10\n",
      "-------------\n",
      "Accuracy:  92.01101928374655\n",
      "F1 Score:  0.911854103343465\n",
      "Precision:  0.9316770186335404\n",
      "Recall:  0.8928571428571429\n",
      "Specificity:  0.9435897435897436\n",
      "Sensitivity:  0.8928571428571429\n",
      "True positive:  150\n",
      "False positive:  11\n",
      "True negative:  184\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  91.4364640883978\n",
      "F1 Score:  0.9168900804289545\n",
      "Precision:  0.9193548387096774\n",
      "Recall:  0.9144385026737968\n",
      "Specificity:  0.9142857142857143\n",
      "Sensitivity:  0.9144385026737968\n",
      "True positive:  171\n",
      "False positive:  15\n",
      "True negative:  160\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  90.33149171270718\n",
      "F1 Score:  0.904109589041096\n",
      "Precision:  0.9116022099447514\n",
      "Recall:  0.8967391304347826\n",
      "Specificity:  0.9101123595505618\n",
      "Sensitivity:  0.8967391304347826\n",
      "True positive:  165\n",
      "False positive:  16\n",
      "True negative:  162\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.37016574585635\n",
      "F1 Score:  0.9408866995073892\n",
      "Precision:  0.9597989949748744\n",
      "Recall:  0.9227053140096618\n",
      "Specificity:  0.9483870967741935\n",
      "Sensitivity:  0.9227053140096618\n",
      "True positive:  191\n",
      "False positive:  8\n",
      "True negative:  147\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  93.92265193370166\n",
      "F1 Score:  0.9395604395604396\n",
      "Precision:  0.9395604395604396\n",
      "Recall:  0.9395604395604396\n",
      "Specificity:  0.9388888888888889\n",
      "Sensitivity:  0.9395604395604396\n",
      "True positive:  171\n",
      "False positive:  11\n",
      "True negative:  169\n",
      "False negative:  11\n",
      "-------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  94.49035812672176\n",
      "F1 Score:  0.9470899470899472\n",
      "Precision:  0.93717277486911\n",
      "Recall:  0.9572192513368984\n",
      "Specificity:  0.9318181818181818\n",
      "Sensitivity:  0.9572192513368984\n",
      "True positive:  179\n",
      "False positive:  12\n",
      "True negative:  164\n",
      "False negative:  8\n",
      "-------------\n",
      "Accuracy:  94.76584022038568\n",
      "F1 Score:  0.9449275362318841\n",
      "Precision:  0.9532163742690059\n",
      "Recall:  0.9367816091954023\n",
      "Specificity:  0.9576719576719577\n",
      "Sensitivity:  0.9367816091954023\n",
      "True positive:  163\n",
      "False positive:  8\n",
      "True negative:  181\n",
      "False negative:  11\n",
      "-------------\n",
      "Accuracy:  94.21487603305785\n",
      "F1 Score:  0.9424657534246575\n",
      "Precision:  0.9662921348314607\n",
      "Recall:  0.9197860962566845\n",
      "Specificity:  0.9659090909090909\n",
      "Sensitivity:  0.9197860962566845\n",
      "True positive:  172\n",
      "False positive:  6\n",
      "True negative:  170\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  87.87878787878788\n",
      "F1 Score:  0.8757062146892656\n",
      "Precision:  0.8959537572254336\n",
      "Recall:  0.856353591160221\n",
      "Specificity:  0.9010989010989011\n",
      "Sensitivity:  0.856353591160221\n",
      "True positive:  155\n",
      "False positive:  18\n",
      "True negative:  164\n",
      "False negative:  26\n",
      "-------------\n",
      "Accuracy:  93.1129476584022\n",
      "F1 Score:  0.9240121580547112\n",
      "Precision:  0.9101796407185628\n",
      "Recall:  0.9382716049382716\n",
      "Specificity:  0.9253731343283582\n",
      "Sensitivity:  0.9382716049382716\n",
      "True positive:  152\n",
      "False positive:  15\n",
      "True negative:  186\n",
      "False negative:  10\n",
      "-------------\n",
      "Accuracy:  91.46005509641874\n",
      "F1 Score:  0.9146005509641872\n",
      "Precision:  0.9120879120879121\n",
      "Recall:  0.9171270718232044\n",
      "Specificity:  0.9120879120879121\n",
      "Sensitivity:  0.9171270718232044\n",
      "True positive:  166\n",
      "False positive:  16\n",
      "True negative:  166\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  94.47513812154696\n",
      "F1 Score:  0.9411764705882353\n",
      "Precision:  0.935672514619883\n",
      "Recall:  0.9467455621301775\n",
      "Specificity:  0.9430051813471503\n",
      "Sensitivity:  0.9467455621301775\n",
      "True positive:  160\n",
      "False positive:  11\n",
      "True negative:  182\n",
      "False negative:  9\n",
      "-------------\n",
      "Accuracy:  92.26519337016575\n",
      "F1 Score:  0.9209039548022598\n",
      "Precision:  0.9367816091954023\n",
      "Recall:  0.9055555555555556\n",
      "Specificity:  0.9395604395604396\n",
      "Sensitivity:  0.9055555555555556\n",
      "True positive:  163\n",
      "False positive:  11\n",
      "True negative:  171\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  90.33149171270718\n",
      "F1 Score:  0.906166219839142\n",
      "Precision:  0.9086021505376344\n",
      "Recall:  0.9037433155080213\n",
      "Specificity:  0.9028571428571428\n",
      "Sensitivity:  0.9037433155080213\n",
      "True positive:  169\n",
      "False positive:  17\n",
      "True negative:  158\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  91.9889502762431\n",
      "F1 Score:  0.9273182957393484\n",
      "Precision:  0.9536082474226805\n",
      "Recall:  0.9024390243902439\n",
      "Specificity:  0.9426751592356688\n",
      "Sensitivity:  0.9024390243902439\n",
      "True positive:  185\n",
      "False positive:  9\n",
      "True negative:  148\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  93.38842975206612\n",
      "F1 Score:  0.934065934065934\n",
      "Precision:  0.9444444444444444\n",
      "Recall:  0.9239130434782609\n",
      "Specificity:  0.9441340782122905\n",
      "Sensitivity:  0.9239130434782609\n",
      "True positive:  170\n",
      "False positive:  10\n",
      "True negative:  169\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  94.76584022038568\n",
      "F1 Score:  0.9482288828337876\n",
      "Precision:  0.9405405405405406\n",
      "Recall:  0.9560439560439561\n",
      "Specificity:  0.9392265193370166\n",
      "Sensitivity:  0.9560439560439561\n",
      "True positive:  174\n",
      "False positive:  11\n",
      "True negative:  170\n",
      "False negative:  8\n",
      "-------------\n",
      "Accuracy:  92.28650137741047\n",
      "F1 Score:  0.9204545454545455\n",
      "Precision:  0.9642857142857143\n",
      "Recall:  0.8804347826086957\n",
      "Specificity:  0.9664804469273743\n",
      "Sensitivity:  0.8804347826086957\n",
      "True positive:  162\n",
      "False positive:  6\n",
      "True negative:  173\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  88.42975206611571\n",
      "F1 Score:  0.8793103448275862\n",
      "Precision:  0.864406779661017\n",
      "Recall:  0.8947368421052632\n",
      "Specificity:  0.875\n",
      "Sensitivity:  0.8947368421052632\n",
      "True positive:  153\n",
      "False positive:  24\n",
      "True negative:  168\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  93.93939393939394\n",
      "F1 Score:  0.9364161849710982\n",
      "Precision:  0.9473684210526315\n",
      "Recall:  0.9257142857142857\n",
      "Specificity:  0.9521276595744681\n",
      "Sensitivity:  0.9257142857142857\n",
      "True positive:  162\n",
      "False positive:  9\n",
      "True negative:  179\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  92.56198347107438\n",
      "F1 Score:  0.9272237196765499\n",
      "Precision:  0.9297297297297298\n",
      "Recall:  0.9247311827956989\n",
      "Specificity:  0.9265536723163842\n",
      "Sensitivity:  0.9247311827956989\n",
      "True positive:  172\n",
      "False positive:  13\n",
      "True negative:  164\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  92.5414364640884\n",
      "F1 Score:  0.925207756232687\n",
      "Precision:  0.9329608938547486\n",
      "Recall:  0.9175824175824175\n",
      "Specificity:  0.9333333333333333\n",
      "Sensitivity:  0.9175824175824175\n",
      "True positive:  167\n",
      "False positive:  12\n",
      "True negative:  168\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  93.646408839779\n",
      "F1 Score:  0.9393139841688654\n",
      "Precision:  0.9222797927461139\n",
      "Recall:  0.956989247311828\n",
      "Specificity:  0.9147727272727273\n",
      "Sensitivity:  0.956989247311828\n",
      "True positive:  178\n",
      "False positive:  15\n",
      "True negative:  161\n",
      "False negative:  8\n",
      "-------------\n",
      "Accuracy:  91.16022099447514\n",
      "F1 Score:  0.9111111111111111\n",
      "Precision:  0.9265536723163842\n",
      "Recall:  0.8961748633879781\n",
      "Specificity:  0.9273743016759777\n",
      "Sensitivity:  0.8961748633879781\n",
      "True positive:  164\n",
      "False positive:  13\n",
      "True negative:  166\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  92.5414364640884\n",
      "F1 Score:  0.9230769230769231\n",
      "Precision:  0.9473684210526315\n",
      "Recall:  0.9\n",
      "Specificity:  0.9505494505494505\n",
      "Sensitivity:  0.9\n",
      "True positive:  162\n",
      "False positive:  9\n",
      "True negative:  173\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  92.56198347107438\n",
      "F1 Score:  0.9312977099236641\n",
      "Precision:  0.9384615384615385\n",
      "Recall:  0.9242424242424242\n",
      "Specificity:  0.9272727272727272\n",
      "Sensitivity:  0.9242424242424242\n",
      "True positive:  183\n",
      "False positive:  12\n",
      "True negative:  153\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  92.28650137741047\n",
      "F1 Score:  0.92\n",
      "Precision:  0.930635838150289\n",
      "Recall:  0.9096045197740112\n",
      "Specificity:  0.9354838709677419\n",
      "Sensitivity:  0.9096045197740112\n",
      "True positive:  161\n",
      "False positive:  12\n",
      "True negative:  174\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  92.8374655647383\n",
      "F1 Score:  0.9248554913294796\n",
      "Precision:  0.9523809523809523\n",
      "Recall:  0.898876404494382\n",
      "Specificity:  0.9567567567567568\n",
      "Sensitivity:  0.898876404494382\n",
      "True positive:  160\n",
      "False positive:  8\n",
      "True negative:  177\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  91.46005509641874\n",
      "F1 Score:  0.9211195928753181\n",
      "Precision:  0.923469387755102\n",
      "Recall:  0.9187817258883249\n",
      "Specificity:  0.9096385542168675\n",
      "Sensitivity:  0.9187817258883249\n",
      "True positive:  181\n",
      "False positive:  15\n",
      "True negative:  151\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  92.28650137741047\n",
      "F1 Score:  0.9263157894736843\n",
      "Precision:  0.9119170984455959\n",
      "Recall:  0.9411764705882353\n",
      "Specificity:  0.9034090909090909\n",
      "Sensitivity:  0.9411764705882353\n",
      "True positive:  176\n",
      "False positive:  17\n",
      "True negative:  159\n",
      "False negative:  11\n",
      "-------------\n",
      "Accuracy:  94.49035812672176\n",
      "F1 Score:  0.9468085106382979\n",
      "Precision:  0.9518716577540107\n",
      "Recall:  0.9417989417989417\n",
      "Specificity:  0.9482758620689655\n",
      "Sensitivity:  0.9417989417989417\n",
      "True positive:  178\n",
      "False positive:  9\n",
      "True negative:  165\n",
      "False negative:  11\n",
      "-------------\n",
      "Accuracy:  93.0939226519337\n",
      "F1 Score:  0.9266862170087976\n",
      "Precision:  0.9186046511627907\n",
      "Recall:  0.9349112426035503\n",
      "Specificity:  0.927461139896373\n",
      "Sensitivity:  0.9349112426035503\n",
      "True positive:  158\n",
      "False positive:  14\n",
      "True negative:  179\n",
      "False negative:  11\n",
      "-------------\n",
      "Accuracy:  94.47513812154696\n",
      "F1 Score:  0.9382716049382717\n",
      "Precision:  0.9440993788819876\n",
      "Recall:  0.9325153374233128\n",
      "Specificity:  0.9547738693467337\n",
      "Sensitivity:  0.9325153374233128\n",
      "True positive:  152\n",
      "False positive:  9\n",
      "True negative:  190\n",
      "False negative:  11\n",
      "-------------\n",
      "Accuracy:  93.0939226519337\n",
      "F1 Score:  0.931129476584022\n",
      "Precision:  0.949438202247191\n",
      "Recall:  0.9135135135135135\n",
      "Specificity:  0.9491525423728814\n",
      "Sensitivity:  0.9135135135135135\n",
      "True positive:  169\n",
      "False positive:  9\n",
      "True negative:  168\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  90.33149171270718\n",
      "F1 Score:  0.8961424332344213\n",
      "Precision:  0.9041916167664671\n",
      "Recall:  0.888235294117647\n",
      "Specificity:  0.9166666666666666\n",
      "Sensitivity:  0.888235294117647\n",
      "True positive:  151\n",
      "False positive:  16\n",
      "True negative:  176\n",
      "False negative:  19\n",
      "-------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  91.46005509641874\n",
      "F1 Score:  0.9155313351498637\n",
      "Precision:  0.9130434782608695\n",
      "Recall:  0.9180327868852459\n",
      "Specificity:  0.9111111111111111\n",
      "Sensitivity:  0.9180327868852459\n",
      "True positive:  168\n",
      "False positive:  16\n",
      "True negative:  164\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  93.1129476584022\n",
      "F1 Score:  0.9299719887955182\n",
      "Precision:  0.9171270718232044\n",
      "Recall:  0.9431818181818182\n",
      "Specificity:  0.9197860962566845\n",
      "Sensitivity:  0.9431818181818182\n",
      "True positive:  166\n",
      "False positive:  15\n",
      "True negative:  172\n",
      "False negative:  10\n",
      "-------------\n",
      "Accuracy:  91.73553719008265\n",
      "F1 Score:  0.9127906976744186\n",
      "Precision:  0.9127906976744186\n",
      "Recall:  0.9127906976744186\n",
      "Specificity:  0.9214659685863874\n",
      "Sensitivity:  0.9127906976744186\n",
      "True positive:  157\n",
      "False positive:  15\n",
      "True negative:  176\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  93.93939393939394\n",
      "F1 Score:  0.9438775510204082\n",
      "Precision:  0.9736842105263158\n",
      "Recall:  0.9158415841584159\n",
      "Specificity:  0.968944099378882\n",
      "Sensitivity:  0.9158415841584159\n",
      "True positive:  185\n",
      "False positive:  5\n",
      "True negative:  156\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  93.93939393939394\n",
      "F1 Score:  0.9345238095238095\n",
      "Precision:  0.9289940828402367\n",
      "Recall:  0.9401197604790419\n",
      "Specificity:  0.9387755102040817\n",
      "Sensitivity:  0.9401197604790419\n",
      "True positive:  157\n",
      "False positive:  12\n",
      "True negative:  184\n",
      "False negative:  10\n",
      "-------------\n",
      "Accuracy:  93.38842975206612\n",
      "F1 Score:  0.9314285714285714\n",
      "Precision:  0.9532163742690059\n",
      "Recall:  0.9106145251396648\n",
      "Specificity:  0.9565217391304348\n",
      "Sensitivity:  0.9106145251396648\n",
      "True positive:  163\n",
      "False positive:  8\n",
      "True negative:  176\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  92.5414364640884\n",
      "F1 Score:  0.9194029850746268\n",
      "Precision:  0.9333333333333333\n",
      "Recall:  0.9058823529411765\n",
      "Specificity:  0.9427083333333334\n",
      "Sensitivity:  0.9058823529411765\n",
      "True positive:  154\n",
      "False positive:  11\n",
      "True negative:  181\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  92.26519337016575\n",
      "F1 Score:  0.9234972677595629\n",
      "Precision:  0.9234972677595629\n",
      "Recall:  0.9234972677595629\n",
      "Specificity:  0.9217877094972067\n",
      "Sensitivity:  0.9234972677595629\n",
      "True positive:  169\n",
      "False positive:  14\n",
      "True negative:  165\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  92.26519337016575\n",
      "F1 Score:  0.9217877094972068\n",
      "Precision:  0.9482758620689655\n",
      "Recall:  0.8967391304347826\n",
      "Specificity:  0.949438202247191\n",
      "Sensitivity:  0.8967391304347826\n",
      "True positive:  165\n",
      "False positive:  9\n",
      "True negative:  169\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.646408839779\n",
      "F1 Score:  0.9417721518987343\n",
      "Precision:  0.9393939393939394\n",
      "Recall:  0.9441624365482234\n",
      "Specificity:  0.9272727272727272\n",
      "Sensitivity:  0.9441624365482234\n",
      "True positive:  186\n",
      "False positive:  12\n",
      "True negative:  153\n",
      "False negative:  11\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "kf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "\n",
    "for train_index,test_index in kf.split(X_undersampled):\n",
    "    X_train_undersampled,X_test_undersampled=X_undersampled[train_index],X_undersampled[test_index]\n",
    "    y_train_undersampled,y_test_undersampled=y_undersampled[train_index],y_undersampled[test_index]\n",
    "    \n",
    "    logistic_regression_model_undersampled = LogisticRegression(max_iter = 3000)\n",
    "    logistic_regression_model_undersampled.fit(X_train_undersampled,y_train_undersampled)\n",
    "    prediction_undersampled = logistic_regression_model_undersampled.predict(X_test_undersampled)\n",
    "    \n",
    "    conf_matrix_undersampled = confusion_matrix(y_true=y_test_undersampled,y_pred=prediction_undersampled)\n",
    "    TP_undersampled = conf_matrix_undersampled[1,1]\n",
    "    TN_undersampled = conf_matrix_undersampled[0,0]\n",
    "    FP_undersampled = conf_matrix_undersampled[0,1]\n",
    "    FN_undersampled = conf_matrix_undersampled[1,0]\n",
    "    sensitivity_undersampled = TP_undersampled/(TP_undersampled+FN_undersampled)\n",
    "    specificity_undersampled = TN_undersampled/(TN_undersampled+FP_undersampled)\n",
    "    accuracy_undersampled = accuracy_score(y_test_undersampled,prediction_undersampled)*100\n",
    "    f1_undersampled = f1_score(y_test_undersampled, prediction_undersampled)\n",
    "    precision_undersampled = precision_score(y_test_undersampled, prediction_undersampled)\n",
    "    recall_undersampled = recall_score(y_test_undersampled, prediction_undersampled)\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy_undersampled)\n",
    "    print(\"F1 Score: \", f1_undersampled)\n",
    "    print(\"Precision: \", precision_undersampled)\n",
    "    print(\"Recall: \", recall_undersampled)\n",
    "    print(\"Specificity: \", specificity_undersampled)\n",
    "    print(\"Sensitivity: \", sensitivity_undersampled)\n",
    "    print(\"True positive: \", TP_undersampled)\n",
    "    print(\"False positive: \", FP_undersampled)\n",
    "    print(\"True negative: \", TN_undersampled)\n",
    "    print(\"False negative: \", FN_undersampled)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  91.39784946236558\n",
      "F1 Score:  0.9124087591240877\n",
      "Precision:  0.8992805755395683\n",
      "Recall:  0.9259259259259259\n",
      "Specificity:  0.9027777777777778\n",
      "Sensitivity:  0.9259259259259259\n",
      "True positive:  250\n",
      "False positive:  28\n",
      "True negative:  260\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  94.6236559139785\n",
      "F1 Score:  0.9477351916376306\n",
      "Precision:  0.9477351916376306\n",
      "Recall:  0.9477351916376306\n",
      "Specificity:  0.9446494464944649\n",
      "Sensitivity:  0.9477351916376306\n",
      "True positive:  272\n",
      "False positive:  15\n",
      "True negative:  256\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  91.93548387096774\n",
      "F1 Score:  0.918918918918919\n",
      "Precision:  0.9239130434782609\n",
      "Recall:  0.9139784946236559\n",
      "Specificity:  0.9247311827956989\n",
      "Sensitivity:  0.9139784946236559\n",
      "True positive:  255\n",
      "False positive:  21\n",
      "True negative:  258\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  93.01075268817203\n",
      "F1 Score:  0.9233791748526522\n",
      "Precision:  0.94\n",
      "Recall:  0.9073359073359073\n",
      "Specificity:  0.9498327759197325\n",
      "Sensitivity:  0.9073359073359073\n",
      "True positive:  235\n",
      "False positive:  15\n",
      "True negative:  284\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  93.9068100358423\n",
      "F1 Score:  0.9417808219178082\n",
      "Precision:  0.935374149659864\n",
      "Recall:  0.9482758620689655\n",
      "Specificity:  0.9291044776119403\n",
      "Sensitivity:  0.9482758620689655\n",
      "True positive:  275\n",
      "False positive:  19\n",
      "True negative:  249\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  91.21863799283155\n",
      "F1 Score:  0.9090909090909091\n",
      "Precision:  0.9315589353612167\n",
      "Recall:  0.8876811594202898\n",
      "Specificity:  0.9361702127659575\n",
      "Sensitivity:  0.8876811594202898\n",
      "True positive:  245\n",
      "False positive:  18\n",
      "True negative:  264\n",
      "False negative:  31\n",
      "-------------\n",
      "Accuracy:  92.99820466786356\n",
      "F1 Score:  0.9307282415630551\n",
      "Precision:  0.9290780141843972\n",
      "Recall:  0.9323843416370107\n",
      "Specificity:  0.927536231884058\n",
      "Sensitivity:  0.9323843416370107\n",
      "True positive:  262\n",
      "False positive:  20\n",
      "True negative:  256\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  94.7935368043088\n",
      "F1 Score:  0.9473684210526316\n",
      "Precision:  0.9525547445255474\n",
      "Recall:  0.9422382671480144\n",
      "Specificity:  0.9535714285714286\n",
      "Sensitivity:  0.9422382671480144\n",
      "True positive:  261\n",
      "False positive:  13\n",
      "True negative:  267\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  91.92100538599641\n",
      "F1 Score:  0.9217391304347825\n",
      "Precision:  0.9330985915492958\n",
      "Recall:  0.9106529209621993\n",
      "Specificity:  0.9285714285714286\n",
      "Sensitivity:  0.9106529209621993\n",
      "True positive:  265\n",
      "False positive:  19\n",
      "True negative:  247\n",
      "False negative:  26\n",
      "-------------\n",
      "Accuracy:  93.17773788150808\n",
      "F1 Score:  0.9304029304029303\n",
      "Precision:  0.9477611940298507\n",
      "Recall:  0.9136690647482014\n",
      "Specificity:  0.9498207885304659\n",
      "Sensitivity:  0.9136690647482014\n",
      "True positive:  254\n",
      "False positive:  14\n",
      "True negative:  265\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  93.9068100358423\n",
      "F1 Score:  0.9356060606060606\n",
      "Precision:  0.9427480916030534\n",
      "Recall:  0.9285714285714286\n",
      "Specificity:  0.9486301369863014\n",
      "Sensitivity:  0.9285714285714286\n",
      "True positive:  247\n",
      "False positive:  15\n",
      "True negative:  277\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.9068100358423\n",
      "F1 Score:  0.9401408450704226\n",
      "Precision:  0.956989247311828\n",
      "Recall:  0.9238754325259516\n",
      "Specificity:  0.9553903345724907\n",
      "Sensitivity:  0.9238754325259516\n",
      "True positive:  267\n",
      "False positive:  12\n",
      "True negative:  257\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  93.36917562724014\n",
      "F1 Score:  0.9352014010507881\n",
      "Precision:  0.9303135888501742\n",
      "Recall:  0.9401408450704225\n",
      "Specificity:  0.927007299270073\n",
      "Sensitivity:  0.9401408450704225\n",
      "True positive:  267\n",
      "False positive:  20\n",
      "True negative:  254\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  90.86021505376344\n",
      "F1 Score:  0.9057301293900184\n",
      "Precision:  0.8909090909090909\n",
      "Recall:  0.9210526315789473\n",
      "Specificity:  0.8972602739726028\n",
      "Sensitivity:  0.9210526315789473\n",
      "True positive:  245\n",
      "False positive:  30\n",
      "True negative:  262\n",
      "False negative:  21\n",
      "-------------\n",
      "Accuracy:  93.9068100358423\n",
      "F1 Score:  0.9341085271317829\n",
      "Precision:  0.926923076923077\n",
      "Recall:  0.94140625\n",
      "Specificity:  0.9370860927152318\n",
      "Sensitivity:  0.94140625\n",
      "True positive:  241\n",
      "False positive:  19\n",
      "True negative:  283\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  92.47311827956989\n",
      "F1 Score:  0.9273356401384083\n",
      "Precision:  0.9436619718309859\n",
      "Recall:  0.9115646258503401\n",
      "Specificity:  0.9393939393939394\n",
      "Sensitivity:  0.9115646258503401\n",
      "True positive:  268\n",
      "False positive:  16\n",
      "True negative:  248\n",
      "False negative:  26\n",
      "-------------\n",
      "Accuracy:  91.02333931777379\n",
      "F1 Score:  0.9056603773584906\n",
      "Precision:  0.9230769230769231\n",
      "Recall:  0.8888888888888888\n",
      "Specificity:  0.9303135888501742\n",
      "Sensitivity:  0.8888888888888888\n",
      "True positive:  240\n",
      "False positive:  20\n",
      "True negative:  267\n",
      "False negative:  30\n",
      "-------------\n",
      "Accuracy:  92.99820466786356\n",
      "F1 Score:  0.934453781512605\n",
      "Precision:  0.9391891891891891\n",
      "Recall:  0.9297658862876255\n",
      "Specificity:  0.9302325581395349\n",
      "Sensitivity:  0.9297658862876255\n",
      "True positive:  278\n",
      "False positive:  18\n",
      "True negative:  240\n",
      "False negative:  21\n",
      "-------------\n",
      "Accuracy:  93.53680430879713\n",
      "F1 Score:  0.9368421052631579\n",
      "Precision:  0.956989247311828\n",
      "Recall:  0.9175257731958762\n",
      "Specificity:  0.9548872180451128\n",
      "Sensitivity:  0.9175257731958762\n",
      "True positive:  267\n",
      "False positive:  12\n",
      "True negative:  254\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  92.45960502692998\n",
      "F1 Score:  0.9230769230769231\n",
      "Precision:  0.9230769230769231\n",
      "Recall:  0.9230769230769231\n",
      "Specificity:  0.926056338028169\n",
      "Sensitivity:  0.9230769230769231\n",
      "True positive:  252\n",
      "False positive:  21\n",
      "True negative:  263\n",
      "False negative:  21\n",
      "-------------\n",
      "Accuracy:  92.29390681003584\n",
      "F1 Score:  0.9254766031195841\n",
      "Precision:  0.9303135888501742\n",
      "Recall:  0.9206896551724137\n",
      "Specificity:  0.9253731343283582\n",
      "Sensitivity:  0.9206896551724137\n",
      "True positive:  267\n",
      "False positive:  20\n",
      "True negative:  248\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  91.21863799283155\n",
      "F1 Score:  0.9077212806026365\n",
      "Precision:  0.926923076923077\n",
      "Recall:  0.8892988929889298\n",
      "Specificity:  0.9337979094076655\n",
      "Sensitivity:  0.8892988929889298\n",
      "True positive:  241\n",
      "False positive:  19\n",
      "True negative:  268\n",
      "False negative:  30\n",
      "-------------\n",
      "Accuracy:  94.44444444444444\n",
      "F1 Score:  0.9437386569872959\n",
      "Precision:  0.9386281588447654\n",
      "Recall:  0.948905109489051\n",
      "Specificity:  0.9401408450704225\n",
      "Sensitivity:  0.948905109489051\n",
      "True positive:  260\n",
      "False positive:  17\n",
      "True negative:  267\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  91.57706093189965\n",
      "F1 Score:  0.9179755671902269\n",
      "Precision:  0.926056338028169\n",
      "Recall:  0.9100346020761245\n",
      "Specificity:  0.9219330855018587\n",
      "Sensitivity:  0.9100346020761245\n",
      "True positive:  263\n",
      "False positive:  21\n",
      "True negative:  248\n",
      "False negative:  26\n",
      "-------------\n",
      "Accuracy:  94.08602150537635\n",
      "F1 Score:  0.9385474860335195\n",
      "Precision:  0.9618320610687023\n",
      "Recall:  0.9163636363636364\n",
      "Specificity:  0.9646643109540636\n",
      "Sensitivity:  0.9163636363636364\n",
      "True positive:  252\n",
      "False positive:  10\n",
      "True negative:  273\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  92.29390681003584\n",
      "F1 Score:  0.9236234458259325\n",
      "Precision:  0.948905109489051\n",
      "Recall:  0.8996539792387543\n",
      "Specificity:  0.9479553903345725\n",
      "Sensitivity:  0.8996539792387543\n",
      "True positive:  260\n",
      "False positive:  14\n",
      "True negative:  255\n",
      "False negative:  29\n",
      "-------------\n",
      "Accuracy:  91.74147217235189\n",
      "F1 Score:  0.9144981412639405\n",
      "Precision:  0.9077490774907749\n",
      "Recall:  0.9213483146067416\n",
      "Specificity:  0.9137931034482759\n",
      "Sensitivity:  0.9213483146067416\n",
      "True positive:  246\n",
      "False positive:  25\n",
      "True negative:  265\n",
      "False negative:  21\n",
      "-------------\n",
      "Accuracy:  93.53680430879713\n",
      "F1 Score:  0.9357142857142857\n",
      "Precision:  0.9290780141843972\n",
      "Recall:  0.9424460431654677\n",
      "Specificity:  0.9283154121863799\n",
      "Sensitivity:  0.9424460431654677\n",
      "True positive:  262\n",
      "False positive:  20\n",
      "True negative:  259\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  92.99820466786356\n",
      "F1 Score:  0.9271028037383178\n",
      "Precision:  0.9358490566037736\n",
      "Recall:  0.9185185185185185\n",
      "Specificity:  0.9407665505226481\n",
      "Sensitivity:  0.9185185185185185\n",
      "True positive:  248\n",
      "False positive:  17\n",
      "True negative:  270\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  93.89587073608617\n",
      "F1 Score:  0.9407665505226481\n",
      "Precision:  0.9342560553633218\n",
      "Recall:  0.9473684210526315\n",
      "Specificity:  0.9301470588235294\n",
      "Sensitivity:  0.9473684210526315\n",
      "True positive:  270\n",
      "False positive:  19\n",
      "True negative:  253\n",
      "False negative:  15\n",
      "-------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  92.1146953405018\n",
      "F1 Score:  0.9197080291970803\n",
      "Precision:  0.9197080291970803\n",
      "Recall:  0.9197080291970803\n",
      "Specificity:  0.9225352112676056\n",
      "Sensitivity:  0.9197080291970803\n",
      "True positive:  252\n",
      "False positive:  22\n",
      "True negative:  262\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  93.36917562724014\n",
      "F1 Score:  0.9369676320272573\n",
      "Precision:  0.9548611111111112\n",
      "Recall:  0.919732441471572\n",
      "Specificity:  0.9498069498069498\n",
      "Sensitivity:  0.919732441471572\n",
      "True positive:  275\n",
      "False positive:  13\n",
      "True negative:  246\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  93.72759856630825\n",
      "F1 Score:  0.9328214971209214\n",
      "Precision:  0.9274809160305344\n",
      "Recall:  0.9382239382239382\n",
      "Specificity:  0.9364548494983278\n",
      "Sensitivity:  0.9382239382239382\n",
      "True positive:  243\n",
      "False positive:  19\n",
      "True negative:  280\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  92.47311827956989\n",
      "F1 Score:  0.9290540540540541\n",
      "Precision:  0.9581881533101045\n",
      "Recall:  0.9016393442622951\n",
      "Specificity:  0.9525691699604744\n",
      "Sensitivity:  0.9016393442622951\n",
      "True positive:  275\n",
      "False positive:  12\n",
      "True negative:  241\n",
      "False negative:  30\n",
      "-------------\n",
      "Accuracy:  91.75627240143369\n",
      "F1 Score:  0.9154411764705883\n",
      "Precision:  0.9360902255639098\n",
      "Recall:  0.89568345323741\n",
      "Specificity:  0.9392857142857143\n",
      "Sensitivity:  0.89568345323741\n",
      "True positive:  249\n",
      "False positive:  17\n",
      "True negative:  263\n",
      "False negative:  29\n",
      "-------------\n",
      "Accuracy:  92.831541218638\n",
      "F1 Score:  0.9295774647887325\n",
      "Precision:  0.9103448275862069\n",
      "Recall:  0.9496402877697842\n",
      "Specificity:  0.9071428571428571\n",
      "Sensitivity:  0.9496402877697842\n",
      "True positive:  264\n",
      "False positive:  26\n",
      "True negative:  254\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  91.56193895870736\n",
      "F1 Score:  0.91500904159132\n",
      "Precision:  0.9133574007220217\n",
      "Recall:  0.9166666666666666\n",
      "Specificity:  0.9145907473309609\n",
      "Sensitivity:  0.9166666666666666\n",
      "True positive:  253\n",
      "False positive:  24\n",
      "True negative:  257\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  94.43447037701975\n",
      "F1 Score:  0.9457092819614711\n",
      "Precision:  0.9540636042402827\n",
      "Recall:  0.9375\n",
      "Specificity:  0.9516728624535316\n",
      "Sensitivity:  0.9375\n",
      "True positive:  270\n",
      "False positive:  13\n",
      "True negative:  256\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  92.10053859964094\n",
      "F1 Score:  0.9147286821705426\n",
      "Precision:  0.9147286821705426\n",
      "Recall:  0.9147286821705426\n",
      "Specificity:  0.9264214046822743\n",
      "Sensitivity:  0.9147286821705426\n",
      "True positive:  236\n",
      "False positive:  22\n",
      "True negative:  277\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  93.89587073608617\n",
      "F1 Score:  0.9375\n",
      "Precision:  0.940959409594096\n",
      "Recall:  0.9340659340659341\n",
      "Specificity:  0.9436619718309859\n",
      "Sensitivity:  0.9340659340659341\n",
      "True positive:  255\n",
      "False positive:  16\n",
      "True negative:  268\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  92.29390681003584\n",
      "F1 Score:  0.9264957264957264\n",
      "Precision:  0.928082191780822\n",
      "Recall:  0.9249146757679181\n",
      "Specificity:  0.9207547169811321\n",
      "Sensitivity:  0.9249146757679181\n",
      "True positive:  271\n",
      "False positive:  21\n",
      "True negative:  244\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  92.831541218638\n",
      "F1 Score:  0.9270072992700731\n",
      "Precision:  0.9338235294117647\n",
      "Recall:  0.9202898550724637\n",
      "Specificity:  0.9361702127659575\n",
      "Sensitivity:  0.9202898550724637\n",
      "True positive:  254\n",
      "False positive:  18\n",
      "True negative:  264\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  91.75627240143369\n",
      "F1 Score:  0.9195804195804196\n",
      "Precision:  0.9163763066202091\n",
      "Recall:  0.9228070175438596\n",
      "Specificity:  0.9120879120879121\n",
      "Sensitivity:  0.9228070175438596\n",
      "True positive:  263\n",
      "False positive:  24\n",
      "True negative:  249\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  92.831541218638\n",
      "F1 Score:  0.9224806201550387\n",
      "Precision:  0.937007874015748\n",
      "Recall:  0.9083969465648855\n",
      "Specificity:  0.9459459459459459\n",
      "Sensitivity:  0.9083969465648855\n",
      "True positive:  238\n",
      "False positive:  16\n",
      "True negative:  280\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  93.01075268817203\n",
      "F1 Score:  0.9328743545611016\n",
      "Precision:  0.954225352112676\n",
      "Recall:  0.9124579124579124\n",
      "Specificity:  0.9501915708812261\n",
      "Sensitivity:  0.9124579124579124\n",
      "True positive:  271\n",
      "False positive:  13\n",
      "True negative:  248\n",
      "False negative:  26\n",
      "-------------\n",
      "Accuracy:  93.01075268817203\n",
      "F1 Score:  0.9259962049335864\n",
      "Precision:  0.9277566539923955\n",
      "Recall:  0.9242424242424242\n",
      "Specificity:  0.935374149659864\n",
      "Sensitivity:  0.9242424242424242\n",
      "True positive:  244\n",
      "False positive:  19\n",
      "True negative:  275\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  93.35727109515261\n",
      "F1 Score:  0.9278752436647173\n",
      "Precision:  0.9407114624505929\n",
      "Recall:  0.9153846153846154\n",
      "Specificity:  0.9494949494949495\n",
      "Sensitivity:  0.9153846153846154\n",
      "True positive:  238\n",
      "False positive:  15\n",
      "True negative:  282\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  92.28007181328546\n",
      "F1 Score:  0.9262435677530018\n",
      "Precision:  0.9278350515463918\n",
      "Recall:  0.9246575342465754\n",
      "Specificity:  0.9207547169811321\n",
      "Sensitivity:  0.9246575342465754\n",
      "True positive:  270\n",
      "False positive:  21\n",
      "True negative:  244\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  94.97307001795332\n",
      "F1 Score:  0.950530035335689\n",
      "Precision:  0.9607142857142857\n",
      "Recall:  0.9405594405594405\n",
      "Specificity:  0.959409594095941\n",
      "Sensitivity:  0.9405594405594405\n",
      "True positive:  269\n",
      "False positive:  11\n",
      "True negative:  260\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  93.53680430879713\n",
      "F1 Score:  0.9345454545454545\n",
      "Precision:  0.927797833935018\n",
      "Recall:  0.9413919413919414\n",
      "Specificity:  0.9295774647887324\n",
      "Sensitivity:  0.9413919413919414\n",
      "True positive:  257\n",
      "False positive:  20\n",
      "True negative:  264\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  93.36917562724014\n",
      "F1 Score:  0.9330922242314648\n",
      "Precision:  0.9555555555555556\n",
      "Recall:  0.911660777385159\n",
      "Specificity:  0.9563636363636364\n",
      "Sensitivity:  0.911660777385159\n",
      "True positive:  258\n",
      "False positive:  12\n",
      "True negative:  263\n",
      "False negative:  25\n",
      "-------------\n",
      "Accuracy:  94.26523297491039\n",
      "F1 Score:  0.943661971830986\n",
      "Precision:  0.9469964664310954\n",
      "Recall:  0.9403508771929825\n",
      "Specificity:  0.945054945054945\n",
      "Sensitivity:  0.9403508771929825\n",
      "True positive:  268\n",
      "False positive:  15\n",
      "True negative:  258\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  91.75627240143369\n",
      "F1 Score:  0.9169675090252708\n",
      "Precision:  0.9039145907473309\n",
      "Recall:  0.9304029304029304\n",
      "Specificity:  0.9052631578947369\n",
      "Sensitivity:  0.9304029304029304\n",
      "True positive:  254\n",
      "False positive:  27\n",
      "True negative:  258\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  94.08602150537635\n",
      "F1 Score:  0.9433962264150942\n",
      "Precision:  0.9450171821305842\n",
      "Recall:  0.9417808219178082\n",
      "Specificity:  0.9398496240601504\n",
      "Sensitivity:  0.9417808219178082\n",
      "True positive:  275\n",
      "False positive:  16\n",
      "True negative:  250\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  94.26523297491039\n",
      "F1 Score:  0.9379844961240311\n",
      "Precision:  0.937984496124031\n",
      "Recall:  0.937984496124031\n",
      "Specificity:  0.9466666666666667\n",
      "Sensitivity:  0.937984496124031\n",
      "True positive:  242\n",
      "False positive:  16\n",
      "True negative:  284\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  94.6236559139785\n",
      "F1 Score:  0.9468085106382979\n",
      "Precision:  0.9535714285714286\n",
      "Recall:  0.9401408450704225\n",
      "Specificity:  0.9525547445255474\n",
      "Sensitivity:  0.9401408450704225\n",
      "True positive:  267\n",
      "False positive:  13\n",
      "True negative:  261\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  91.74147217235189\n",
      "F1 Score:  0.9175627240143369\n",
      "Precision:  0.9045936395759717\n",
      "Recall:  0.9309090909090909\n",
      "Specificity:  0.9042553191489362\n",
      "Sensitivity:  0.9309090909090909\n",
      "True positive:  256\n",
      "False positive:  27\n",
      "True negative:  255\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.35727109515261\n",
      "F1 Score:  0.9360967184801382\n",
      "Precision:  0.9442508710801394\n",
      "Recall:  0.928082191780822\n",
      "Specificity:  0.939622641509434\n",
      "Sensitivity:  0.928082191780822\n",
      "True positive:  271\n",
      "False positive:  16\n",
      "True negative:  249\n",
      "False negative:  21\n",
      "-------------\n",
      "Accuracy:  92.10053859964094\n",
      "F1 Score:  0.9176029962546817\n",
      "Precision:  0.928030303030303\n",
      "Recall:  0.9074074074074074\n",
      "Specificity:  0.9337979094076655\n",
      "Sensitivity:  0.9074074074074074\n",
      "True positive:  245\n",
      "False positive:  19\n",
      "True negative:  268\n",
      "False negative:  25\n",
      "-------------\n",
      "Accuracy:  88.68940754039497\n",
      "F1 Score:  0.88268156424581\n",
      "Precision:  0.9080459770114943\n",
      "Recall:  0.8586956521739131\n",
      "Specificity:  0.9145907473309609\n",
      "Sensitivity:  0.8586956521739131\n",
      "True positive:  237\n",
      "False positive:  24\n",
      "True negative:  257\n",
      "False negative:  39\n",
      "-------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  92.1146953405018\n",
      "F1 Score:  0.9194139194139194\n",
      "Precision:  0.9261992619926199\n",
      "Recall:  0.9127272727272727\n",
      "Specificity:  0.9293286219081273\n",
      "Sensitivity:  0.9127272727272727\n",
      "True positive:  251\n",
      "False positive:  20\n",
      "True negative:  263\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  92.1146953405018\n",
      "F1 Score:  0.92\n",
      "Precision:  0.9440298507462687\n",
      "Recall:  0.8971631205673759\n",
      "Specificity:  0.9456521739130435\n",
      "Sensitivity:  0.8971631205673759\n",
      "True positive:  253\n",
      "False positive:  15\n",
      "True negative:  261\n",
      "False negative:  29\n",
      "-------------\n",
      "Accuracy:  92.831541218638\n",
      "F1 Score:  0.9224806201550387\n",
      "Precision:  0.937007874015748\n",
      "Recall:  0.9083969465648855\n",
      "Specificity:  0.9459459459459459\n",
      "Sensitivity:  0.9083969465648855\n",
      "True positive:  238\n",
      "False positive:  16\n",
      "True negative:  280\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  92.47311827956989\n",
      "F1 Score:  0.9268292682926829\n",
      "Precision:  0.9333333333333333\n",
      "Recall:  0.9204152249134948\n",
      "Specificity:  0.929368029739777\n",
      "Sensitivity:  0.9204152249134948\n",
      "True positive:  266\n",
      "False positive:  19\n",
      "True negative:  250\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  94.44444444444444\n",
      "F1 Score:  0.9451327433628319\n",
      "Precision:  0.9434628975265018\n",
      "Recall:  0.9468085106382979\n",
      "Specificity:  0.9420289855072463\n",
      "Sensitivity:  0.9468085106382979\n",
      "True positive:  267\n",
      "False positive:  16\n",
      "True negative:  260\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  94.80286738351255\n",
      "F1 Score:  0.9488536155202822\n",
      "Precision:  0.9676258992805755\n",
      "Recall:  0.9307958477508651\n",
      "Specificity:  0.966542750929368\n",
      "Sensitivity:  0.9307958477508651\n",
      "True positive:  269\n",
      "False positive:  9\n",
      "True negative:  260\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  94.25493716337523\n",
      "F1 Score:  0.937984496124031\n",
      "Precision:  0.9343629343629344\n",
      "Recall:  0.9416342412451362\n",
      "Specificity:  0.9433333333333334\n",
      "Sensitivity:  0.9416342412451362\n",
      "True positive:  242\n",
      "False positive:  17\n",
      "True negative:  283\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  91.74147217235189\n",
      "F1 Score:  0.917562724014337\n",
      "Precision:  0.920863309352518\n",
      "Recall:  0.9142857142857143\n",
      "Specificity:  0.9205776173285198\n",
      "Sensitivity:  0.9142857142857143\n",
      "True positive:  256\n",
      "False positive:  22\n",
      "True negative:  255\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  92.28007181328546\n",
      "F1 Score:  0.9228007181328547\n",
      "Precision:  0.9244604316546763\n",
      "Recall:  0.921146953405018\n",
      "Specificity:  0.9244604316546763\n",
      "Sensitivity:  0.921146953405018\n",
      "True positive:  257\n",
      "False positive:  21\n",
      "True negative:  257\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  91.92100538599641\n",
      "F1 Score:  0.9235993208828525\n",
      "Precision:  0.918918918918919\n",
      "Recall:  0.9283276450511946\n",
      "Specificity:  0.9090909090909091\n",
      "Sensitivity:  0.9283276450511946\n",
      "True positive:  272\n",
      "False positive:  24\n",
      "True negative:  240\n",
      "False negative:  21\n",
      "-------------\n",
      "Accuracy:  92.29390681003584\n",
      "F1 Score:  0.9211009174311927\n",
      "Precision:  0.9330855018587361\n",
      "Recall:  0.9094202898550725\n",
      "Specificity:  0.9361702127659575\n",
      "Sensitivity:  0.9094202898550725\n",
      "True positive:  251\n",
      "False positive:  18\n",
      "True negative:  264\n",
      "False negative:  25\n",
      "-------------\n",
      "Accuracy:  91.93548387096774\n",
      "F1 Score:  0.9192100538599641\n",
      "Precision:  0.9110320284697508\n",
      "Recall:  0.927536231884058\n",
      "Specificity:  0.9113475177304965\n",
      "Sensitivity:  0.927536231884058\n",
      "True positive:  256\n",
      "False positive:  25\n",
      "True negative:  257\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  92.65232974910394\n",
      "F1 Score:  0.9239332096474954\n",
      "Precision:  0.9188191881918819\n",
      "Recall:  0.9291044776119403\n",
      "Specificity:  0.9241379310344827\n",
      "Sensitivity:  0.9291044776119403\n",
      "True positive:  249\n",
      "False positive:  22\n",
      "True negative:  268\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.72759856630825\n",
      "F1 Score:  0.935064935064935\n",
      "Precision:  0.9230769230769231\n",
      "Recall:  0.9473684210526315\n",
      "Specificity:  0.928082191780822\n",
      "Sensitivity:  0.9473684210526315\n",
      "True positive:  252\n",
      "False positive:  21\n",
      "True negative:  271\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  94.08602150537635\n",
      "F1 Score:  0.9413854351687388\n",
      "Precision:  0.9498207885304659\n",
      "Recall:  0.9330985915492958\n",
      "Specificity:  0.948905109489051\n",
      "Sensitivity:  0.9330985915492958\n",
      "True positive:  265\n",
      "False positive:  14\n",
      "True negative:  260\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  92.29390681003584\n",
      "F1 Score:  0.9262435677530018\n",
      "Precision:  0.9574468085106383\n",
      "Recall:  0.8970099667774086\n",
      "Specificity:  0.953307392996109\n",
      "Sensitivity:  0.8970099667774086\n",
      "True positive:  270\n",
      "False positive:  12\n",
      "True negative:  245\n",
      "False negative:  31\n",
      "-------------\n",
      "Accuracy:  92.28007181328546\n",
      "F1 Score:  0.9211009174311927\n",
      "Precision:  0.916058394160584\n",
      "Recall:  0.9261992619926199\n",
      "Specificity:  0.9195804195804196\n",
      "Sensitivity:  0.9261992619926199\n",
      "True positive:  251\n",
      "False positive:  23\n",
      "True negative:  263\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  93.35727109515261\n",
      "F1 Score:  0.9330922242314647\n",
      "Precision:  0.945054945054945\n",
      "Recall:  0.9214285714285714\n",
      "Specificity:  0.9458483754512635\n",
      "Sensitivity:  0.9214285714285714\n",
      "True positive:  258\n",
      "False positive:  15\n",
      "True negative:  262\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  92.99820466786356\n",
      "F1 Score:  0.9307282415630549\n",
      "Precision:  0.9390681003584229\n",
      "Recall:  0.9225352112676056\n",
      "Specificity:  0.9377289377289377\n",
      "Sensitivity:  0.9225352112676056\n",
      "True positive:  262\n",
      "False positive:  17\n",
      "True negative:  256\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  93.71633752244165\n",
      "F1 Score:  0.9369369369369369\n",
      "Precision:  0.9523809523809523\n",
      "Recall:  0.9219858156028369\n",
      "Specificity:  0.9527272727272728\n",
      "Sensitivity:  0.9219858156028369\n",
      "True positive:  260\n",
      "False positive:  13\n",
      "True negative:  262\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  93.54838709677419\n",
      "F1 Score:  0.9333333333333335\n",
      "Precision:  0.9402985074626866\n",
      "Recall:  0.9264705882352942\n",
      "Specificity:  0.9440559440559441\n",
      "Sensitivity:  0.9264705882352942\n",
      "True positive:  252\n",
      "False positive:  16\n",
      "True negative:  270\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  93.36917562724014\n",
      "F1 Score:  0.928709055876686\n",
      "Precision:  0.9450980392156862\n",
      "Recall:  0.9128787878787878\n",
      "Specificity:  0.9523809523809523\n",
      "Sensitivity:  0.9128787878787878\n",
      "True positive:  241\n",
      "False positive:  14\n",
      "True negative:  280\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  92.1146953405018\n",
      "F1 Score:  0.9228070175438596\n",
      "Precision:  0.9359430604982206\n",
      "Recall:  0.9100346020761245\n",
      "Specificity:  0.9330855018587361\n",
      "Sensitivity:  0.9100346020761245\n",
      "True positive:  263\n",
      "False positive:  18\n",
      "True negative:  251\n",
      "False negative:  26\n",
      "-------------\n",
      "Accuracy:  93.72759856630825\n",
      "F1 Score:  0.9369369369369368\n",
      "Precision:  0.9285714285714286\n",
      "Recall:  0.9454545454545454\n",
      "Specificity:  0.9293286219081273\n",
      "Sensitivity:  0.9454545454545454\n",
      "True positive:  260\n",
      "False positive:  20\n",
      "True negative:  263\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  91.93548387096774\n",
      "F1 Score:  0.9203539823008849\n",
      "Precision:  0.9252669039145908\n",
      "Recall:  0.9154929577464789\n",
      "Specificity:  0.9233576642335767\n",
      "Sensitivity:  0.9154929577464789\n",
      "True positive:  260\n",
      "False positive:  21\n",
      "True negative:  253\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  92.831541218638\n",
      "F1 Score:  0.9315068493150686\n",
      "Precision:  0.9411764705882353\n",
      "Recall:  0.9220338983050848\n",
      "Specificity:  0.935361216730038\n",
      "Sensitivity:  0.9220338983050848\n",
      "True positive:  272\n",
      "False positive:  17\n",
      "True negative:  246\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  90.30520646319569\n",
      "F1 Score:  0.9032258064516129\n",
      "Precision:  0.8904593639575972\n",
      "Recall:  0.9163636363636364\n",
      "Specificity:  0.8900709219858156\n",
      "Sensitivity:  0.9163636363636364\n",
      "True positive:  252\n",
      "False positive:  31\n",
      "True negative:  251\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  95.15260323159784\n",
      "F1 Score:  0.9497206703910613\n",
      "Precision:  0.9550561797752809\n",
      "Recall:  0.9444444444444444\n",
      "Specificity:  0.9581881533101045\n",
      "Sensitivity:  0.9444444444444444\n",
      "True positive:  255\n",
      "False positive:  12\n",
      "True negative:  275\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  92.99820466786356\n",
      "F1 Score:  0.9357495881383856\n",
      "Precision:  0.9403973509933775\n",
      "Recall:  0.9311475409836065\n",
      "Specificity:  0.9285714285714286\n",
      "Sensitivity:  0.9311475409836065\n",
      "True positive:  284\n",
      "False positive:  18\n",
      "True negative:  234\n",
      "False negative:  21\n",
      "-------------\n",
      "Accuracy:  92.99820466786356\n",
      "F1 Score:  0.9236790606653621\n",
      "Precision:  0.9365079365079365\n",
      "Recall:  0.9111969111969112\n",
      "Specificity:  0.9463087248322147\n",
      "Sensitivity:  0.9111969111969112\n",
      "True positive:  236\n",
      "False positive:  16\n",
      "True negative:  282\n",
      "False negative:  23\n",
      "-------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  91.75627240143369\n",
      "F1 Score:  0.915129151291513\n",
      "Precision:  0.9117647058823529\n",
      "Recall:  0.9185185185185185\n",
      "Specificity:  0.9166666666666666\n",
      "Sensitivity:  0.9185185185185185\n",
      "True positive:  248\n",
      "False positive:  24\n",
      "True negative:  264\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  90.68100358422939\n",
      "F1 Score:  0.9040590405904059\n",
      "Precision:  0.8844765342960289\n",
      "Recall:  0.9245283018867925\n",
      "Specificity:  0.8907849829351536\n",
      "Sensitivity:  0.9245283018867925\n",
      "True positive:  245\n",
      "False positive:  32\n",
      "True negative:  261\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  93.36917562724014\n",
      "F1 Score:  0.937394247038917\n",
      "Precision:  0.9358108108108109\n",
      "Recall:  0.9389830508474576\n",
      "Specificity:  0.9277566539923955\n",
      "Sensitivity:  0.9389830508474576\n",
      "True positive:  277\n",
      "False positive:  19\n",
      "True negative:  244\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  94.26523297491039\n",
      "F1 Score:  0.9372549019607843\n",
      "Precision:  0.9637096774193549\n",
      "Recall:  0.9122137404580153\n",
      "Specificity:  0.9695945945945946\n",
      "Sensitivity:  0.9122137404580153\n",
      "True positive:  239\n",
      "False positive:  9\n",
      "True negative:  287\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  92.831541218638\n",
      "F1 Score:  0.9267399267399268\n",
      "Precision:  0.9547169811320755\n",
      "Recall:  0.900355871886121\n",
      "Specificity:  0.9566787003610109\n",
      "Sensitivity:  0.900355871886121\n",
      "True positive:  253\n",
      "False positive:  12\n",
      "True negative:  265\n",
      "False negative:  28\n",
      "-------------\n",
      "Accuracy:  93.01075268817203\n",
      "F1 Score:  0.9297297297297297\n",
      "Precision:  0.9247311827956989\n",
      "Recall:  0.9347826086956522\n",
      "Specificity:  0.925531914893617\n",
      "Sensitivity:  0.9347826086956522\n",
      "True positive:  258\n",
      "False positive:  21\n",
      "True negative:  261\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  92.99820466786356\n",
      "F1 Score:  0.9302325581395349\n",
      "Precision:  0.931899641577061\n",
      "Recall:  0.9285714285714286\n",
      "Specificity:  0.9314079422382672\n",
      "Sensitivity:  0.9285714285714286\n",
      "True positive:  260\n",
      "False positive:  19\n",
      "True negative:  258\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  93.71633752244165\n",
      "F1 Score:  0.9380530973451328\n",
      "Precision:  0.9464285714285714\n",
      "Recall:  0.9298245614035088\n",
      "Specificity:  0.9448529411764706\n",
      "Sensitivity:  0.9298245614035088\n",
      "True positive:  265\n",
      "False positive:  15\n",
      "True negative:  257\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  93.53680430879713\n",
      "F1 Score:  0.9352517985611511\n",
      "Precision:  0.9558823529411765\n",
      "Recall:  0.9154929577464789\n",
      "Specificity:  0.9560439560439561\n",
      "Sensitivity:  0.9154929577464789\n",
      "True positive:  260\n",
      "False positive:  12\n",
      "True negative:  261\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  92.99820466786356\n",
      "F1 Score:  0.9326424870466321\n",
      "Precision:  0.9342560553633218\n",
      "Recall:  0.9310344827586207\n",
      "Specificity:  0.9288389513108615\n",
      "Sensitivity:  0.9310344827586207\n",
      "True positive:  270\n",
      "False positive:  19\n",
      "True negative:  248\n",
      "False negative:  20\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "kf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "\n",
    "for train_index,test_index in kf.split(X_upsampled):\n",
    "    X_train_upsampled,X_test_upsampled=X_upsampled[train_index],X_upsampled[test_index]\n",
    "    y_train_upsampled,y_test_upsampled=y_upsampled[train_index],y_upsampled[test_index]\n",
    "    \n",
    "    logistic_regression_model_upsampled = LogisticRegression(max_iter = 3000)\n",
    "    logistic_regression_model_upsampled.fit(X_train_upsampled,y_train_upsampled)\n",
    "    prediction_upsampled = logistic_regression_model_upsampled.predict(X_test_upsampled)\n",
    "    \n",
    "    conf_matrix_upsampled = confusion_matrix(y_true=y_test_upsampled,y_pred=prediction_upsampled)\n",
    "    TP_upsampled = conf_matrix_upsampled[1,1]\n",
    "    TN_upsampled = conf_matrix_upsampled[0,0]\n",
    "    FP_upsampled = conf_matrix_upsampled[0,1]\n",
    "    FN_upsampled = conf_matrix_upsampled[1,0]\n",
    "    sensitivity_upsampled = TP_upsampled/(TP_upsampled+FN_upsampled)\n",
    "    specificity_upsampled = TN_upsampled/(TN_upsampled+FP_upsampled)\n",
    "    accuracy_upsampled = accuracy_score(y_test_upsampled,prediction_upsampled)*100\n",
    "    f1_upsampled = f1_score(y_test_upsampled, prediction_upsampled)\n",
    "    precision_upsampled = precision_score(y_test_upsampled, prediction_upsampled)\n",
    "    recall_upsampled = recall_score(y_test_upsampled, prediction_upsampled)\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy_upsampled)\n",
    "    print(\"F1 Score: \", f1_upsampled)\n",
    "    print(\"Precision: \", precision_upsampled)\n",
    "    print(\"Recall: \", recall_upsampled)\n",
    "    print(\"Specificity: \", specificity_upsampled)\n",
    "    print(\"Sensitivity: \", sensitivity_upsampled)\n",
    "    print(\"True positive: \", TP_upsampled)\n",
    "    print(\"False positive: \", FP_upsampled)\n",
    "    print(\"True negative: \", TN_upsampled)\n",
    "    print(\"False negative: \", FN_upsampled)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADIENT BOOST "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bez balansiranja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imbalanced = dataset.drop(\"label_spam\",axis = 1).values\n",
    "y_imbalanced = dataset[\"label_spam\"].values\n",
    "\n",
    "X_train_imbalanced,X_test_imbalanced, y_train_imbalanced, y_test_imbalanced = train_test_split(X_imbalanced,y_imbalanced,test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_booster = GradientBoostingClassifier(learning_rate=0.1)\n",
    "params = gradient_booster.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_booster.fit(X_train_imbalanced,y_train_imbalanced)\n",
    "prediction_imbalanced=gradient_booster.predict(X_test_imbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_imbalanced = confusion_matrix(y_true=y_test_imbalanced,y_pred=prediction_imbalanced)\n",
    "TP_imbalanced = conf_matrix_imbalanced[1,1]\n",
    "TN_imbalanced = conf_matrix_imbalanced[0,0]\n",
    "FP_imbalanced = conf_matrix_imbalanced[0,1]\n",
    "FN_imbalanced = conf_matrix_imbalanced[1,0]\n",
    "sensitivity_imbalanced = TP_imbalanced/(TP_imbalanced+FN_imbalanced)\n",
    "specificity_imbalanced = TN_imbalanced/(TN_imbalanced+FP_imbalanced)\n",
    "accuracy_imbalanced = accuracy_score(y_test_imbalanced,prediction_imbalanced)*100\n",
    "f1_imbalanced = f1_score(y_test_imbalanced, prediction_imbalanced)\n",
    "precision_imbalanced = precision_score(y_test_imbalanced, prediction_imbalanced)\n",
    "recall_imbalanced = recall_score(y_test_imbalanced, prediction_imbalanced)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  95.00542888165037\n",
      "F1 Score:  0.9340974212034385\n",
      "Precision:  0.9560117302052786\n",
      "Recall:  0.9131652661064426\n",
      "Specificity:  0.973404255319149\n",
      "Sensitivity:  0.9131652661064426\n",
      "True positive:  326\n",
      "False positive:  15\n",
      "True negative:  549\n",
      "False negative:  31\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_imbalanced)\n",
    "print(\"F1 Score: \", f1_imbalanced)\n",
    "print(\"Precision: \", precision_imbalanced)\n",
    "print(\"Recall: \", recall_imbalanced)\n",
    "print(\"Specificity: \", specificity_imbalanced)\n",
    "print(\"Sensitivity: \", sensitivity_imbalanced)\n",
    "print(\"True positive: \", TP_imbalanced)\n",
    "print(\"False positive: \", FP_imbalanced)\n",
    "print(\"True negative: \", TN_imbalanced)\n",
    "print(\"False negative: \", FN_imbalanced)\n",
    "print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_undersampled = df_undersampled.drop(\"label_spam\",axis = 1).values\n",
    "y_undersampled = df_undersampled[\"label_spam\"].values\n",
    "X_train_undersampled,X_test_undersampled, y_train_undersampled, y_test_undersampled = train_test_split(X_undersampled,y_undersampled,test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_booster = GradientBoostingClassifier(learning_rate=0.1)\n",
    "params = gradient_booster.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_booster.fit(X_train_undersampled,y_train_undersampled)\n",
    "prediction_undersampled=gradient_booster.predict(X_test_undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_undersampled = confusion_matrix(y_true=y_test_undersampled,y_pred=prediction_undersampled)\n",
    "TP_undersampled = conf_matrix_undersampled[1,1]\n",
    "TN_undersampled = conf_matrix_undersampled[0,0]\n",
    "FP_undersampled = conf_matrix_undersampled[0,1]\n",
    "FN_undersampled = conf_matrix_undersampled[1,0]\n",
    "sensitivity_undersampled = TP_undersampled/(TP_undersampled+FN_undersampled)\n",
    "specificity_undersampled = TN_undersampled/(TN_undersampled+FP_undersampled)\n",
    "accuracy_undersampled = accuracy_score(y_test_undersampled,prediction_undersampled)*100\n",
    "f1_undersampled = f1_score(y_test_undersampled, prediction_undersampled)\n",
    "precision_undersampled = precision_score(y_test_undersampled, prediction_undersampled)\n",
    "recall_undersampled = recall_score(y_test_undersampled, prediction_undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  94.49035812672176\n",
      "F1 Score:  0.9426934097421203\n",
      "Precision:  0.9563953488372093\n",
      "Recall:  0.9293785310734464\n",
      "Specificity:  0.9596774193548387\n",
      "Sensitivity:  0.9293785310734464\n",
      "True positive:  329\n",
      "False positive:  15\n",
      "True negative:  357\n",
      "False negative:  25\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_undersampled)\n",
    "print(\"F1 Score: \", f1_undersampled)\n",
    "print(\"Precision: \", precision_undersampled)\n",
    "print(\"Recall: \", recall_undersampled)\n",
    "print(\"Specificity: \", specificity_undersampled)\n",
    "print(\"Sensitivity: \", sensitivity_undersampled)\n",
    "print(\"True positive: \", TP_undersampled)\n",
    "print(\"False positive: \", FP_undersampled)\n",
    "print(\"True negative: \", TN_undersampled)\n",
    "print(\"False negative: \", FN_undersampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_upsampled = df_upsampled.drop(\"label_spam\",axis = 1).values\n",
    "y_upsampled = df_upsampled[\"label_spam\"].values\n",
    "X_train_upsampled,X_test_upsampled, y_train_upsampled, y_test_upsampled = train_test_split(X_upsampled,y_upsampled,test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_booster = GradientBoostingClassifier(learning_rate=0.1)\n",
    "params = gradient_booster.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_booster.fit(X_train_upsampled,y_train_upsampled)\n",
    "prediction_upsampled=gradient_booster.predict(X_test_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_upsampled = confusion_matrix(y_true=y_test_upsampled,y_pred=prediction_upsampled)\n",
    "TP_upsampled = conf_matrix_upsampled[1,1]\n",
    "TN_upsampled = conf_matrix_upsampled[0,0]\n",
    "FP_upsampled = conf_matrix_upsampled[0,1]\n",
    "FN_upsampled = conf_matrix_upsampled[1,0]\n",
    "sensitivity_upsampled = TP_upsampled/(TP_upsampled+FN_upsampled)\n",
    "specificity_upsampled = TN_upsampled/(TN_upsampled+FP_upsampled)\n",
    "accuracy_upsampled = accuracy_score(y_test_upsampled,prediction_upsampled)*100\n",
    "f1_upsampled = f1_score(y_test_upsampled, prediction_upsampled)\n",
    "precision_upsampled = precision_score(y_test_upsampled, prediction_upsampled)\n",
    "recall_upsampled = recall_score(y_test_upsampled, prediction_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  95.43010752688173\n",
      "F1 Score:  0.9546666666666667\n",
      "Precision:  0.9454225352112676\n",
      "Recall:  0.9640933572710951\n",
      "Specificity:  0.9445438282647585\n",
      "Sensitivity:  0.9640933572710951\n",
      "True positive:  537\n",
      "False positive:  31\n",
      "True negative:  528\n",
      "False negative:  20\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_upsampled)\n",
    "print(\"F1 Score: \", f1_upsampled)\n",
    "print(\"Precision: \", precision_upsampled)\n",
    "print(\"Recall: \", recall_upsampled)\n",
    "print(\"Specificity: \", specificity_upsampled)\n",
    "print(\"Sensitivity: \", sensitivity_upsampled)\n",
    "print(\"True positive: \", TP_upsampled)\n",
    "print(\"False positive: \", FP_upsampled)\n",
    "print(\"True negative: \", TN_upsampled)\n",
    "print(\"False negative: \", FN_upsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Komparacija mera za Grdient Boost bez cross validacije"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(13, 3))\n",
    "plt.subplot(131)\n",
    "plt.ylim(90,95)\n",
    "plt.bar(['acc_under', 'acc_up', 'acc_imb'], [accuracy_undersampled, accuracy_upsampled, accuracy_imbalanced])\n",
    "plt.subplot(132)\n",
    "plt.ylim(0.9,0.95)\n",
    "plt.bar(['f1_under', 'f1_up','f1_imb'], [f1_undersampled, f1_upsampled, f1_imbalanced])\n",
    "plt.subplot(133)\n",
    "plt.ylim(0.9,0.95)\n",
    "plt.bar(['prec_under', 'prec_up', 'prec_imb'], [precision_undersampled, precision_upsampled, [precision_imbalanced]])\n",
    "plt.suptitle('Measurement comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boost sa cross validacijom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bez balansiranja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  95.00542888165037\n",
      "F1 Score:  0.9340974212034385\n",
      "Precision:  0.9560117302052786\n",
      "Recall:  0.9131652661064426\n",
      "Specificity:  0.973404255319149\n",
      "Sensitivity:  0.9131652661064426\n",
      "True positive:  326\n",
      "False positive:  15\n",
      "True negative:  549\n",
      "False negative:  31\n",
      "-------------\n",
      "Accuracy:  95.00542888165037\n",
      "F1 Score:  0.9340974212034385\n",
      "Precision:  0.9560117302052786\n",
      "Recall:  0.9131652661064426\n",
      "Specificity:  0.973404255319149\n",
      "Sensitivity:  0.9131652661064426\n",
      "True positive:  326\n",
      "False positive:  15\n",
      "True negative:  549\n",
      "False negative:  31\n",
      "-------------\n",
      "Accuracy:  95.11400651465797\n",
      "F1 Score:  0.9356223175965666\n",
      "Precision:  0.956140350877193\n",
      "Recall:  0.9159663865546218\n",
      "Specificity:  0.973404255319149\n",
      "Sensitivity:  0.9159663865546218\n",
      "True positive:  327\n",
      "False positive:  15\n",
      "True negative:  549\n",
      "False negative:  30\n",
      "-------------\n",
      "Accuracy:  95.00542888165037\n",
      "F1 Score:  0.9340974212034385\n",
      "Precision:  0.9560117302052786\n",
      "Recall:  0.9131652661064426\n",
      "Specificity:  0.973404255319149\n",
      "Sensitivity:  0.9131652661064426\n",
      "True positive:  326\n",
      "False positive:  15\n",
      "True negative:  549\n",
      "False negative:  31\n",
      "-------------\n",
      "Accuracy:  95.00542888165037\n",
      "F1 Score:  0.9340974212034385\n",
      "Precision:  0.9560117302052786\n",
      "Recall:  0.9131652661064426\n",
      "Specificity:  0.973404255319149\n",
      "Sensitivity:  0.9131652661064426\n",
      "True positive:  326\n",
      "False positive:  15\n",
      "True negative:  549\n",
      "False negative:  31\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "X_imbalanced = dataset.drop(\"label_spam\",axis = 1).values\n",
    "y_imbalanced = dataset[\"label_spam\"].values\n",
    "X_train_imbalanced,X_test_imbalanced, y_train_imbalanced, y_test_imbalanced = train_test_split(X_imbalanced,y_imbalanced,test_size = 0.2, random_state = 1)\n",
    "\n",
    "kf = KFold(n_splits=5,random_state = 42, shuffle = True)\n",
    "\n",
    "for train_index,test_index in kf.split(X_imbalanced):    \n",
    "    gradient_booster = GradientBoostingClassifier(learning_rate=0.1)\n",
    "    params = gradient_booster.get_params()\n",
    "    \n",
    "    gradient_booster.fit(X_train_imbalanced,y_train_imbalanced)\n",
    "    prediction_imbalanced=gradient_booster.predict(X_test_imbalanced)\n",
    "    \n",
    "    conf_matrix_imbalanced = confusion_matrix(y_true=y_test_imbalanced,y_pred=prediction_imbalanced)\n",
    "    TP_imbalanced = conf_matrix_imbalanced[1,1]\n",
    "    TN_imbalanced = conf_matrix_imbalanced[0,0]\n",
    "    FP_imbalanced = conf_matrix_imbalanced[0,1]\n",
    "    FN_imbalanced = conf_matrix_imbalanced[1,0]\n",
    "    sensitivity_imbalanced = TP_imbalanced/(TP_imbalanced+FN_imbalanced)\n",
    "    specificity_imbalanced = TN_imbalanced/(TN_imbalanced+FP_imbalanced)\n",
    "    accuracy_imbalanced = accuracy_score(y_test_imbalanced,prediction_imbalanced)*100\n",
    "    f1_imbalanced = f1_score(y_test_imbalanced, prediction_imbalanced)\n",
    "    precision_imbalanced = precision_score(y_test_imbalanced, prediction_imbalanced)\n",
    "    recall_imbalanced = recall_score(y_test_imbalanced, prediction_imbalanced)\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy_imbalanced)\n",
    "    print(\"F1 Score: \", f1_imbalanced)\n",
    "    print(\"Precision: \", precision_imbalanced)\n",
    "    print(\"Recall: \", recall_imbalanced)\n",
    "    print(\"Specificity: \", specificity_imbalanced)\n",
    "    print(\"Sensitivity: \", sensitivity_imbalanced)\n",
    "    print(\"True positive: \", TP_imbalanced)\n",
    "    print(\"False positive: \", FP_imbalanced)\n",
    "    print(\"True negative: \", TN_imbalanced)\n",
    "    print(\"False negative: \", FN_imbalanced)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  94.0771349862259\n",
      "F1 Score:  0.9375907111756169\n",
      "Precision:  0.9444444444444444\n",
      "Recall:  0.930835734870317\n",
      "Specificity:  0.9498680738786279\n",
      "Sensitivity:  0.930835734870317\n",
      "True positive:  323\n",
      "False positive:  19\n",
      "True negative:  360\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  93.79310344827586\n",
      "F1 Score:  0.9363507779349363\n",
      "Precision:  0.9403409090909091\n",
      "Recall:  0.9323943661971831\n",
      "Specificity:  0.9432432432432433\n",
      "Sensitivity:  0.9323943661971831\n",
      "True positive:  331\n",
      "False positive:  21\n",
      "True negative:  349\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  93.65517241379311\n",
      "F1 Score:  0.9378378378378378\n",
      "Precision:  0.9455040871934605\n",
      "Recall:  0.9302949061662198\n",
      "Specificity:  0.9431818181818182\n",
      "Sensitivity:  0.9302949061662198\n",
      "True positive:  347\n",
      "False positive:  20\n",
      "True negative:  332\n",
      "False negative:  26\n",
      "-------------\n",
      "Accuracy:  94.62068965517241\n",
      "F1 Score:  0.946938775510204\n",
      "Precision:  0.9613259668508287\n",
      "Recall:  0.9329758713136729\n",
      "Specificity:  0.9602272727272727\n",
      "Sensitivity:  0.9329758713136729\n",
      "True positive:  348\n",
      "False positive:  14\n",
      "True negative:  338\n",
      "False negative:  25\n",
      "-------------\n",
      "Accuracy:  96.13793103448276\n",
      "F1 Score:  0.9612188365650969\n",
      "Precision:  0.9719887955182073\n",
      "Recall:  0.9506849315068493\n",
      "Specificity:  0.9722222222222222\n",
      "Sensitivity:  0.9506849315068493\n",
      "True positive:  347\n",
      "False positive:  10\n",
      "True negative:  350\n",
      "False negative:  18\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "#gradient boost\n",
    "X_undersampled = df_undersampled.drop(\"label_spam\",axis = 1).values\n",
    "y_undersampled = df_undersampled[\"label_spam\"].values\n",
    "X_train_undersampled,X_test_undersampled, y_train_undersampled, y_test_undersampled = train_test_split(X_undersampled,y_undersampled,test_size = 0.2, random_state = 1)\n",
    "\n",
    "kf = KFold(n_splits=5,random_state = 42, shuffle = True)\n",
    "\n",
    "for train_index,test_index in kf.split(X_undersampled):\n",
    "    X_train_undersampled,X_test_undersampled=X_undersampled[train_index],X_undersampled[test_index]\n",
    "    y_train_undersampled,y_test_undersampled=y_undersampled[train_index],y_undersampled[test_index]\n",
    "    \n",
    "    gradient_booster = GradientBoostingClassifier(learning_rate=0.1)\n",
    "    params = gradient_booster.get_params()\n",
    "    \n",
    "    gradient_booster.fit(X_train_undersampled,y_train_undersampled)\n",
    "    prediction_undersampled=gradient_booster.predict(X_test_undersampled)\n",
    "    \n",
    "    conf_matrix_undersampled = confusion_matrix(y_true=y_test_undersampled,y_pred=prediction_undersampled)\n",
    "    TP_undersampled = conf_matrix_undersampled[1,1]\n",
    "    TN_undersampled = conf_matrix_undersampled[0,0]\n",
    "    FP_undersampled = conf_matrix_undersampled[0,1]\n",
    "    FN_undersampled = conf_matrix_undersampled[1,0]\n",
    "    sensitivity_undersampled = TP_undersampled/(TP_undersampled+FN_undersampled)\n",
    "    specificity_undersampled = TN_undersampled/(TN_undersampled+FP_undersampled)\n",
    "    accuracy_undersampled = accuracy_score(y_test_undersampled,prediction_undersampled)*100\n",
    "    f1_undersampled = f1_score(y_test_undersampled, prediction_undersampled)\n",
    "    precision_undersampled = precision_score(y_test_undersampled, prediction_undersampled)\n",
    "    recall_undersampled = recall_score(y_test_undersampled, prediction_undersampled)\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy_undersampled)\n",
    "    print(\"F1 Score: \", f1_undersampled)\n",
    "    print(\"Precision: \", precision_undersampled)\n",
    "    print(\"Recall: \", recall_undersampled)\n",
    "    print(\"Specificity: \", specificity_undersampled)\n",
    "    print(\"Sensitivity: \", sensitivity_undersampled)\n",
    "    print(\"True positive: \", TP_undersampled)\n",
    "    print(\"False positive: \", FP_undersampled)\n",
    "    print(\"True negative: \", TN_undersampled)\n",
    "    print(\"False negative: \", FN_undersampled)\n",
    "    print(\"-------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  95.96774193548387\n",
      "F1 Score:  0.9581395348837208\n",
      "Precision:  0.9590316573556797\n",
      "Recall:  0.9572490706319703\n",
      "Specificity:  0.9619377162629758\n",
      "Sensitivity:  0.9572490706319703\n",
      "True positive:  515\n",
      "False positive:  22\n",
      "True negative:  556\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  96.23318385650225\n",
      "F1 Score:  0.9615384615384616\n",
      "Precision:  0.9562841530054644\n",
      "Recall:  0.9668508287292817\n",
      "Specificity:  0.958041958041958\n",
      "Sensitivity:  0.9668508287292817\n",
      "True positive:  525\n",
      "False positive:  24\n",
      "True negative:  548\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  95.15695067264573\n",
      "F1 Score:  0.9523809523809526\n",
      "Precision:  0.9557522123893806\n",
      "Recall:  0.9490333919156415\n",
      "Specificity:  0.9542124542124543\n",
      "Sensitivity:  0.9490333919156415\n",
      "True positive:  540\n",
      "False positive:  25\n",
      "True negative:  521\n",
      "False negative:  29\n",
      "-------------\n",
      "Accuracy:  95.33632286995515\n",
      "F1 Score:  0.9530685920577617\n",
      "Precision:  0.9565217391304348\n",
      "Recall:  0.9496402877697842\n",
      "Specificity:  0.9570661896243292\n",
      "Sensitivity:  0.9496402877697842\n",
      "True positive:  528\n",
      "False positive:  24\n",
      "True negative:  535\n",
      "False negative:  28\n",
      "-------------\n",
      "Accuracy:  95.06726457399103\n",
      "F1 Score:  0.9523809523809524\n",
      "Precision:  0.9598603839441536\n",
      "Recall:  0.9450171821305842\n",
      "Specificity:  0.9568480300187617\n",
      "Sensitivity:  0.9450171821305842\n",
      "True positive:  550\n",
      "False positive:  23\n",
      "True negative:  510\n",
      "False negative:  32\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "X_upsampled = df_upsampled.drop(\"label_spam\",axis = 1).values\n",
    "y_upsampled = df_upsampled[\"label_spam\"].values\n",
    "X_train_upsampled,X_test_upsampled, y_train_upsampled, y_test_upsampled = train_test_split(X_upsampled,y_upsampled,test_size = 0.2, random_state = 1)\n",
    "\n",
    "kf = KFold(n_splits=5,random_state = 42, shuffle = True)\n",
    "\n",
    "for train_index,test_index in kf.split(X_upsampled):\n",
    "    X_train_upsampled,X_test_upsampled=X_upsampled[train_index],X_upsampled[test_index]\n",
    "    y_train_upsampled,y_test_upsampled=y_upsampled[train_index],y_upsampled[test_index]\n",
    "    \n",
    "    gradient_booster = GradientBoostingClassifier(learning_rate=0.1)\n",
    "    params = gradient_booster.get_params()\n",
    "    \n",
    "    gradient_booster.fit(X_train_upsampled,y_train_upsampled)\n",
    "    prediction_upsampled=gradient_booster.predict(X_test_upsampled)\n",
    "    \n",
    "    conf_matrix_upsampled = confusion_matrix(y_true=y_test_upsampled,y_pred=prediction_upsampled)\n",
    "    TP_upsampled = conf_matrix_upsampled[1,1]\n",
    "    TN_upsampled = conf_matrix_upsampled[0,0]\n",
    "    FP_upsampled = conf_matrix_upsampled[0,1]\n",
    "    FN_upsampled = conf_matrix_upsampled[1,0]\n",
    "    sensitivity_upsampled = TP_upsampled/(TP_upsampled+FN_upsampled)\n",
    "    specificity_upsampled = TN_upsampled/(TN_upsampled+FP_upsampled)\n",
    "    accuracy_upsampled = accuracy_score(y_test_upsampled,prediction_upsampled)*100\n",
    "    f1_upsampled = f1_score(y_test_upsampled, prediction_upsampled)\n",
    "    precision_upsampled = precision_score(y_test_upsampled, prediction_upsampled)\n",
    "    recall_upsampled = recall_score(y_test_upsampled, prediction_upsampled)\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy_upsampled)\n",
    "    print(\"F1 Score: \", f1_upsampled)\n",
    "    print(\"Precision: \", precision_upsampled)\n",
    "    print(\"Recall: \", recall_upsampled)\n",
    "    print(\"Specificity: \", specificity_upsampled)\n",
    "    print(\"Sensitivity: \", sensitivity_upsampled)\n",
    "    print(\"True positive: \", TP_upsampled)\n",
    "    print(\"False positive: \", FP_upsampled)\n",
    "    print(\"True negative: \", TN_upsampled)\n",
    "    print(\"False negative: \", FN_upsampled)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeated KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bez balansiranja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  92.84164859002169\n",
      "F1 Score:  0.9090909090909092\n",
      "Precision:  0.9217877094972067\n",
      "Recall:  0.8967391304347826\n",
      "Specificity:  0.9494584837545126\n",
      "Sensitivity:  0.8967391304347826\n",
      "True positive:  165\n",
      "False positive:  14\n",
      "True negative:  263\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.91304347826087\n",
      "F1 Score:  0.9166666666666667\n",
      "Precision:  0.9447852760736196\n",
      "Recall:  0.8901734104046243\n",
      "Specificity:  0.9686411149825784\n",
      "Sensitivity:  0.8901734104046243\n",
      "True positive:  154\n",
      "False positive:  9\n",
      "True negative:  278\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.26086956521739\n",
      "F1 Score:  0.9096209912536443\n",
      "Precision:  0.975\n",
      "Recall:  0.8524590163934426\n",
      "Specificity:  0.9855595667870036\n",
      "Sensitivity:  0.8524590163934426\n",
      "True positive:  156\n",
      "False positive:  4\n",
      "True negative:  273\n",
      "False negative:  27\n",
      "-------------\n",
      "Accuracy:  91.73913043478261\n",
      "F1 Score:  0.9040404040404041\n",
      "Precision:  0.927461139896373\n",
      "Recall:  0.8817733990147784\n",
      "Specificity:  0.9455252918287937\n",
      "Sensitivity:  0.8817733990147784\n",
      "True positive:  179\n",
      "False positive:  14\n",
      "True negative:  243\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  92.3913043478261\n",
      "F1 Score:  0.9030470914127424\n",
      "Precision:  0.9005524861878453\n",
      "Recall:  0.9055555555555556\n",
      "Specificity:  0.9357142857142857\n",
      "Sensitivity:  0.9055555555555556\n",
      "True positive:  163\n",
      "False positive:  18\n",
      "True negative:  262\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  90.8695652173913\n",
      "F1 Score:  0.8882978723404256\n",
      "Precision:  0.912568306010929\n",
      "Recall:  0.8652849740932642\n",
      "Specificity:  0.9400749063670412\n",
      "Sensitivity:  0.8652849740932642\n",
      "True positive:  167\n",
      "False positive:  16\n",
      "True negative:  251\n",
      "False negative:  26\n",
      "-------------\n",
      "Accuracy:  91.95652173913044\n",
      "F1 Score:  0.8882175226586103\n",
      "Precision:  0.8963414634146342\n",
      "Recall:  0.8802395209580839\n",
      "Specificity:  0.9419795221843004\n",
      "Sensitivity:  0.8802395209580839\n",
      "True positive:  147\n",
      "False positive:  17\n",
      "True negative:  276\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  93.69565217391305\n",
      "F1 Score:  0.9169054441260744\n",
      "Precision:  0.9411764705882353\n",
      "Recall:  0.8938547486033519\n",
      "Specificity:  0.9644128113879004\n",
      "Sensitivity:  0.8938547486033519\n",
      "True positive:  160\n",
      "False positive:  10\n",
      "True negative:  271\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  94.1304347826087\n",
      "F1 Score:  0.9194029850746269\n",
      "Precision:  0.9005847953216374\n",
      "Recall:  0.9390243902439024\n",
      "Specificity:  0.9425675675675675\n",
      "Sensitivity:  0.9390243902439024\n",
      "True positive:  154\n",
      "False positive:  17\n",
      "True negative:  279\n",
      "False negative:  10\n",
      "-------------\n",
      "Accuracy:  94.56521739130434\n",
      "F1 Score:  0.932975871313673\n",
      "Precision:  0.9354838709677419\n",
      "Recall:  0.93048128342246\n",
      "Specificity:  0.9560439560439561\n",
      "Sensitivity:  0.93048128342246\n",
      "True positive:  174\n",
      "False positive:  12\n",
      "True negative:  261\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  94.36008676789588\n",
      "F1 Score:  0.9182389937106918\n",
      "Precision:  0.954248366013072\n",
      "Recall:  0.8848484848484849\n",
      "Specificity:  0.9763513513513513\n",
      "Sensitivity:  0.8848484848484849\n",
      "True positive:  146\n",
      "False positive:  7\n",
      "True negative:  289\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  90.8695652173913\n",
      "F1 Score:  0.8870967741935485\n",
      "Precision:  0.9016393442622951\n",
      "Recall:  0.873015873015873\n",
      "Specificity:  0.933579335793358\n",
      "Sensitivity:  0.873015873015873\n",
      "True positive:  165\n",
      "False positive:  18\n",
      "True negative:  253\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  93.04347826086956\n",
      "F1 Score:  0.9157894736842106\n",
      "Precision:  0.9405405405405406\n",
      "Recall:  0.8923076923076924\n",
      "Specificity:  0.9584905660377359\n",
      "Sensitivity:  0.8923076923076924\n",
      "True positive:  174\n",
      "False positive:  11\n",
      "True negative:  254\n",
      "False negative:  21\n",
      "-------------\n",
      "Accuracy:  91.95652173913044\n",
      "F1 Score:  0.8914956011730206\n",
      "Precision:  0.8994082840236687\n",
      "Recall:  0.8837209302325582\n",
      "Specificity:  0.9409722222222222\n",
      "Sensitivity:  0.8837209302325582\n",
      "True positive:  152\n",
      "False positive:  17\n",
      "True negative:  271\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  94.78260869565217\n",
      "F1 Score:  0.9384615384615386\n",
      "Precision:  0.9682539682539683\n",
      "Recall:  0.9104477611940298\n",
      "Specificity:  0.9768339768339769\n",
      "Sensitivity:  0.9104477611940298\n",
      "True positive:  183\n",
      "False positive:  6\n",
      "True negative:  253\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  93.04347826086956\n",
      "F1 Score:  0.8961038961038961\n",
      "Precision:  0.9078947368421053\n",
      "Recall:  0.8846153846153846\n",
      "Specificity:  0.9539473684210527\n",
      "Sensitivity:  0.8846153846153846\n",
      "True positive:  138\n",
      "False positive:  14\n",
      "True negative:  290\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  91.95652173913044\n",
      "F1 Score:  0.8980716253443527\n",
      "Precision:  0.9106145251396648\n",
      "Recall:  0.8858695652173914\n",
      "Specificity:  0.9420289855072463\n",
      "Sensitivity:  0.8858695652173914\n",
      "True positive:  163\n",
      "False positive:  16\n",
      "True negative:  260\n",
      "False negative:  21\n",
      "-------------\n",
      "Accuracy:  92.6086956521739\n",
      "F1 Score:  0.9170731707317074\n",
      "Precision:  0.9447236180904522\n",
      "Recall:  0.8909952606635071\n",
      "Specificity:  0.9558232931726908\n",
      "Sensitivity:  0.8909952606635071\n",
      "True positive:  188\n",
      "False positive:  11\n",
      "True negative:  238\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  93.04347826086956\n",
      "F1 Score:  0.9058823529411765\n",
      "Precision:  0.8953488372093024\n",
      "Recall:  0.9166666666666666\n",
      "Specificity:  0.9383561643835616\n",
      "Sensitivity:  0.9166666666666666\n",
      "True positive:  154\n",
      "False positive:  18\n",
      "True negative:  274\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  94.1304347826087\n",
      "F1 Score:  0.9198813056379823\n",
      "Precision:  0.9393939393939394\n",
      "Recall:  0.9011627906976745\n",
      "Specificity:  0.9652777777777778\n",
      "Sensitivity:  0.9011627906976745\n",
      "True positive:  155\n",
      "False positive:  10\n",
      "True negative:  278\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  91.75704989154013\n",
      "F1 Score:  0.8944444444444446\n",
      "Precision:  0.92\n",
      "Recall:  0.8702702702702703\n",
      "Specificity:  0.9492753623188406\n",
      "Sensitivity:  0.8702702702702703\n",
      "True positive:  161\n",
      "False positive:  14\n",
      "True negative:  262\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  93.91304347826087\n",
      "F1 Score:  0.9166666666666666\n",
      "Precision:  0.9005847953216374\n",
      "Recall:  0.9333333333333333\n",
      "Specificity:  0.9423728813559322\n",
      "Sensitivity:  0.9333333333333333\n",
      "True positive:  154\n",
      "False positive:  17\n",
      "True negative:  278\n",
      "False negative:  11\n",
      "-------------\n",
      "Accuracy:  94.34782608695652\n",
      "F1 Score:  0.9239766081871346\n",
      "Precision:  0.9404761904761905\n",
      "Recall:  0.9080459770114943\n",
      "Specificity:  0.965034965034965\n",
      "Sensitivity:  0.9080459770114943\n",
      "True positive:  158\n",
      "False positive:  10\n",
      "True negative:  276\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  93.26086956521739\n",
      "F1 Score:  0.9116809116809117\n",
      "Precision:  0.9248554913294798\n",
      "Recall:  0.898876404494382\n",
      "Specificity:  0.9539007092198581\n",
      "Sensitivity:  0.898876404494382\n",
      "True positive:  160\n",
      "False positive:  13\n",
      "True negative:  269\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  90.8695652173913\n",
      "F1 Score:  0.8939393939393939\n",
      "Precision:  0.917098445595855\n",
      "Recall:  0.8719211822660099\n",
      "Specificity:  0.9377431906614786\n",
      "Sensitivity:  0.8719211822660099\n",
      "True positive:  177\n",
      "False positive:  16\n",
      "True negative:  241\n",
      "False negative:  26\n",
      "-------------\n",
      "Accuracy:  91.73913043478261\n",
      "F1 Score:  0.8819875776397514\n",
      "Precision:  0.9281045751633987\n",
      "Recall:  0.8402366863905325\n",
      "Specificity:  0.9621993127147767\n",
      "Sensitivity:  0.8402366863905325\n",
      "True positive:  142\n",
      "False positive:  11\n",
      "True negative:  280\n",
      "False negative:  27\n",
      "-------------\n",
      "Accuracy:  93.04347826086956\n",
      "F1 Score:  0.9101123595505618\n",
      "Precision:  0.9418604651162791\n",
      "Recall:  0.8804347826086957\n",
      "Specificity:  0.9637681159420289\n",
      "Sensitivity:  0.8804347826086957\n",
      "True positive:  162\n",
      "False positive:  10\n",
      "True negative:  266\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  91.95652173913044\n",
      "F1 Score:  0.900804289544236\n",
      "Precision:  0.8842105263157894\n",
      "Recall:  0.9180327868852459\n",
      "Specificity:  0.9205776173285198\n",
      "Sensitivity:  0.9180327868852459\n",
      "True positive:  168\n",
      "False positive:  22\n",
      "True negative:  255\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  93.91304347826087\n",
      "F1 Score:  0.9213483146067416\n",
      "Precision:  0.9371428571428572\n",
      "Recall:  0.9060773480662984\n",
      "Specificity:  0.9605734767025089\n",
      "Sensitivity:  0.9060773480662984\n",
      "True positive:  164\n",
      "False positive:  11\n",
      "True negative:  268\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  93.04347826086956\n",
      "F1 Score:  0.9153439153439152\n",
      "Precision:  0.9251336898395722\n",
      "Recall:  0.9057591623036649\n",
      "Specificity:  0.9479553903345725\n",
      "Sensitivity:  0.9057591623036649\n",
      "True positive:  173\n",
      "False positive:  14\n",
      "True negative:  255\n",
      "False negative:  18\n",
      "-------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  94.57700650759219\n",
      "F1 Score:  0.9326145552560647\n",
      "Precision:  0.9301075268817204\n",
      "Recall:  0.9351351351351351\n",
      "Specificity:  0.9528985507246377\n",
      "Sensitivity:  0.9351351351351351\n",
      "True positive:  173\n",
      "False positive:  13\n",
      "True negative:  263\n",
      "False negative:  12\n",
      "-------------\n",
      "Accuracy:  94.1304347826087\n",
      "F1 Score:  0.9203539823008848\n",
      "Precision:  0.9069767441860465\n",
      "Recall:  0.9341317365269461\n",
      "Specificity:  0.9453924914675768\n",
      "Sensitivity:  0.9341317365269461\n",
      "True positive:  156\n",
      "False positive:  16\n",
      "True negative:  277\n",
      "False negative:  11\n",
      "-------------\n",
      "Accuracy:  94.34782608695652\n",
      "F1 Score:  0.9322916666666666\n",
      "Precision:  0.9521276595744681\n",
      "Recall:  0.9132653061224489\n",
      "Specificity:  0.9659090909090909\n",
      "Sensitivity:  0.9132653061224489\n",
      "True positive:  179\n",
      "False positive:  9\n",
      "True negative:  255\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  91.73913043478261\n",
      "F1 Score:  0.8875739644970415\n",
      "Precision:  0.872093023255814\n",
      "Recall:  0.9036144578313253\n",
      "Specificity:  0.9251700680272109\n",
      "Sensitivity:  0.9036144578313253\n",
      "True positive:  150\n",
      "False positive:  22\n",
      "True negative:  272\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  92.6086956521739\n",
      "F1 Score:  0.8999999999999999\n",
      "Precision:  0.8947368421052632\n",
      "Recall:  0.9053254437869822\n",
      "Specificity:  0.9381443298969072\n",
      "Sensitivity:  0.9053254437869822\n",
      "True positive:  153\n",
      "False positive:  18\n",
      "True negative:  273\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  90.8695652173913\n",
      "F1 Score:  0.8839779005524862\n",
      "Precision:  0.9248554913294798\n",
      "Recall:  0.8465608465608465\n",
      "Specificity:  0.9520295202952029\n",
      "Sensitivity:  0.8465608465608465\n",
      "True positive:  160\n",
      "False positive:  13\n",
      "True negative:  258\n",
      "False negative:  29\n",
      "-------------\n",
      "Accuracy:  92.3913043478261\n",
      "F1 Score:  0.899135446685879\n",
      "Precision:  0.9122807017543859\n",
      "Recall:  0.8863636363636364\n",
      "Specificity:  0.9471830985915493\n",
      "Sensitivity:  0.8863636363636364\n",
      "True positive:  156\n",
      "False positive:  15\n",
      "True negative:  269\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  93.91304347826087\n",
      "F1 Score:  0.9186046511627907\n",
      "Precision:  0.9461077844311377\n",
      "Recall:  0.8926553672316384\n",
      "Specificity:  0.9681978798586572\n",
      "Sensitivity:  0.8926553672316384\n",
      "True positive:  158\n",
      "False positive:  9\n",
      "True negative:  274\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  91.08695652173913\n",
      "F1 Score:  0.8951406649616369\n",
      "Precision:  0.9408602150537635\n",
      "Recall:  0.8536585365853658\n",
      "Specificity:  0.9568627450980393\n",
      "Sensitivity:  0.8536585365853658\n",
      "True positive:  175\n",
      "False positive:  11\n",
      "True negative:  244\n",
      "False negative:  30\n",
      "-------------\n",
      "Accuracy:  93.04347826086956\n",
      "F1 Score:  0.907514450867052\n",
      "Precision:  0.9631901840490797\n",
      "Recall:  0.8579234972677595\n",
      "Specificity:  0.9783393501805054\n",
      "Sensitivity:  0.8579234972677595\n",
      "True positive:  157\n",
      "False positive:  6\n",
      "True negative:  271\n",
      "False negative:  26\n",
      "-------------\n",
      "Accuracy:  93.058568329718\n",
      "F1 Score:  0.9090909090909091\n",
      "Precision:  0.8938547486033519\n",
      "Recall:  0.9248554913294798\n",
      "Specificity:  0.9340277777777778\n",
      "Sensitivity:  0.9248554913294798\n",
      "True positive:  160\n",
      "False positive:  19\n",
      "True negative:  269\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  93.04347826086956\n",
      "F1 Score:  0.9041916167664672\n",
      "Precision:  0.9320987654320988\n",
      "Recall:  0.877906976744186\n",
      "Specificity:  0.9618055555555556\n",
      "Sensitivity:  0.877906976744186\n",
      "True positive:  151\n",
      "False positive:  11\n",
      "True negative:  277\n",
      "False negative:  21\n",
      "-------------\n",
      "Accuracy:  94.1304347826087\n",
      "F1 Score:  0.9272237196765499\n",
      "Precision:  0.9555555555555556\n",
      "Recall:  0.900523560209424\n",
      "Specificity:  0.9702602230483272\n",
      "Sensitivity:  0.900523560209424\n",
      "True positive:  172\n",
      "False positive:  8\n",
      "True negative:  261\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.26086956521739\n",
      "F1 Score:  0.9159891598915989\n",
      "Precision:  0.9388888888888889\n",
      "Recall:  0.8941798941798942\n",
      "Specificity:  0.959409594095941\n",
      "Sensitivity:  0.8941798941798942\n",
      "True positive:  169\n",
      "False positive:  11\n",
      "True negative:  260\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  94.56521739130434\n",
      "F1 Score:  0.9326145552560646\n",
      "Precision:  0.9402173913043478\n",
      "Recall:  0.9251336898395722\n",
      "Specificity:  0.9597069597069597\n",
      "Sensitivity:  0.9251336898395722\n",
      "True positive:  173\n",
      "False positive:  11\n",
      "True negative:  262\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  93.69565217391305\n",
      "F1 Score:  0.9178470254957507\n",
      "Precision:  0.9101123595505618\n",
      "Recall:  0.9257142857142857\n",
      "Specificity:  0.9438596491228071\n",
      "Sensitivity:  0.9257142857142857\n",
      "True positive:  162\n",
      "False positive:  16\n",
      "True negative:  269\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  90.21739130434783\n",
      "F1 Score:  0.8767123287671232\n",
      "Precision:  0.9411764705882353\n",
      "Recall:  0.8205128205128205\n",
      "Specificity:  0.9622641509433962\n",
      "Sensitivity:  0.8205128205128205\n",
      "True positive:  160\n",
      "False positive:  10\n",
      "True negative:  255\n",
      "False negative:  35\n",
      "-------------\n",
      "Accuracy:  92.17391304347827\n",
      "F1 Score:  0.8922155688622753\n",
      "Precision:  0.8975903614457831\n",
      "Recall:  0.8869047619047619\n",
      "Specificity:  0.9417808219178082\n",
      "Sensitivity:  0.8869047619047619\n",
      "True positive:  149\n",
      "False positive:  17\n",
      "True negative:  275\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  91.73913043478261\n",
      "F1 Score:  0.8932584269662922\n",
      "Precision:  0.9085714285714286\n",
      "Recall:  0.8784530386740331\n",
      "Specificity:  0.942652329749104\n",
      "Sensitivity:  0.8784530386740331\n",
      "True positive:  159\n",
      "False positive:  16\n",
      "True negative:  263\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  92.6086956521739\n",
      "F1 Score:  0.9034090909090909\n",
      "Precision:  0.9352941176470588\n",
      "Recall:  0.8736263736263736\n",
      "Specificity:  0.960431654676259\n",
      "Sensitivity:  0.8736263736263736\n",
      "True positive:  159\n",
      "False positive:  11\n",
      "True negative:  267\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  92.40780911062907\n",
      "F1 Score:  0.906166219839142\n",
      "Precision:  0.9086021505376344\n",
      "Recall:  0.9037433155080213\n",
      "Specificity:  0.9379562043795621\n",
      "Sensitivity:  0.9037433155080213\n",
      "True positive:  169\n",
      "False positive:  17\n",
      "True negative:  257\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  91.95652173913044\n",
      "F1 Score:  0.8840125391849529\n",
      "Precision:  0.8924050632911392\n",
      "Recall:  0.8757763975155279\n",
      "Specificity:  0.9431438127090301\n",
      "Sensitivity:  0.8757763975155279\n",
      "True positive:  141\n",
      "False positive:  17\n",
      "True negative:  282\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  91.30434782608695\n",
      "F1 Score:  0.8974358974358975\n",
      "Precision:  0.9020618556701031\n",
      "Recall:  0.8928571428571429\n",
      "Specificity:  0.928030303030303\n",
      "Sensitivity:  0.8928571428571429\n",
      "True positive:  175\n",
      "False positive:  19\n",
      "True negative:  245\n",
      "False negative:  21\n",
      "-------------\n",
      "Accuracy:  93.26086956521739\n",
      "F1 Score:  0.9106628242074928\n",
      "Precision:  0.9239766081871345\n",
      "Recall:  0.8977272727272727\n",
      "Specificity:  0.954225352112676\n",
      "Sensitivity:  0.8977272727272727\n",
      "True positive:  158\n",
      "False positive:  13\n",
      "True negative:  271\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  93.69565217391305\n",
      "F1 Score:  0.9144542772861357\n",
      "Precision:  0.9226190476190477\n",
      "Recall:  0.9064327485380117\n",
      "Specificity:  0.9550173010380623\n",
      "Sensitivity:  0.9064327485380117\n",
      "True positive:  155\n",
      "False positive:  13\n",
      "True negative:  276\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  93.04347826086956\n",
      "F1 Score:  0.9162303664921466\n",
      "Precision:  0.9259259259259259\n",
      "Recall:  0.9067357512953368\n",
      "Specificity:  0.947565543071161\n",
      "Sensitivity:  0.9067357512953368\n",
      "True positive:  175\n",
      "False positive:  14\n",
      "True negative:  253\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  95.0\n",
      "F1 Score:  0.9321533923303835\n",
      "Precision:  0.9753086419753086\n",
      "Recall:  0.8926553672316384\n",
      "Specificity:  0.9858657243816255\n",
      "Sensitivity:  0.8926553672316384\n",
      "True positive:  158\n",
      "False positive:  4\n",
      "True negative:  279\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  94.34782608695652\n",
      "F1 Score:  0.9235294117647059\n",
      "Precision:  0.9515151515151515\n",
      "Recall:  0.8971428571428571\n",
      "Specificity:  0.9719298245614035\n",
      "Sensitivity:  0.8971428571428571\n",
      "True positive:  157\n",
      "False positive:  8\n",
      "True negative:  277\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  90.65217391304347\n",
      "F1 Score:  0.8788732394366197\n",
      "Precision:  0.9176470588235294\n",
      "Recall:  0.8432432432432433\n",
      "Specificity:  0.9490909090909091\n",
      "Sensitivity:  0.8432432432432433\n",
      "True positive:  156\n",
      "False positive:  14\n",
      "True negative:  261\n",
      "False negative:  29\n",
      "-------------\n",
      "Accuracy:  93.26086956521739\n",
      "F1 Score:  0.9186351706036746\n",
      "Precision:  0.9259259259259259\n",
      "Recall:  0.9114583333333334\n",
      "Specificity:  0.9477611940298507\n",
      "Sensitivity:  0.9114583333333334\n",
      "True positive:  175\n",
      "False positive:  14\n",
      "True negative:  254\n",
      "False negative:  17\n",
      "-------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  93.058568329718\n",
      "F1 Score:  0.9096045197740114\n",
      "Precision:  0.9415204678362573\n",
      "Recall:  0.8797814207650273\n",
      "Specificity:  0.9640287769784173\n",
      "Sensitivity:  0.8797814207650273\n",
      "True positive:  161\n",
      "False positive:  10\n",
      "True negative:  268\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  90.8695652173913\n",
      "F1 Score:  0.8771929824561403\n",
      "Precision:  0.8670520231213873\n",
      "Recall:  0.8875739644970414\n",
      "Specificity:  0.9209621993127147\n",
      "Sensitivity:  0.8875739644970414\n",
      "True positive:  150\n",
      "False positive:  23\n",
      "True negative:  268\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  91.95652173913044\n",
      "F1 Score:  0.8969359331476323\n",
      "Precision:  0.9252873563218391\n",
      "Recall:  0.8702702702702703\n",
      "Specificity:  0.9527272727272728\n",
      "Sensitivity:  0.8702702702702703\n",
      "True positive:  161\n",
      "False positive:  13\n",
      "True negative:  262\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  93.69565217391305\n",
      "F1 Score:  0.9164265129682997\n",
      "Precision:  0.9298245614035088\n",
      "Recall:  0.9034090909090909\n",
      "Specificity:  0.9577464788732394\n",
      "Sensitivity:  0.9034090909090909\n",
      "True positive:  159\n",
      "False positive:  12\n",
      "True negative:  272\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  92.82608695652173\n",
      "F1 Score:  0.906515580736544\n",
      "Precision:  0.898876404494382\n",
      "Recall:  0.9142857142857143\n",
      "Specificity:  0.9368421052631579\n",
      "Sensitivity:  0.9142857142857143\n",
      "True positive:  160\n",
      "False positive:  18\n",
      "True negative:  267\n",
      "False negative:  15\n",
      "-------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-134-0d160032ae32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mlogistic_regression_model_imbalanced\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mlogistic_regression_model_imbalanced\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_imbalanced\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_imbalanced\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mprediction_imbalanced\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression_model_imbalanced\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_imbalanced\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1415\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1416\u001b[0m                       sample_weight=sample_weight)\n\u001b[1;32m-> 1417\u001b[1;33m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[0;32m   1418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[0;32m    758\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"L-BFGS-B\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 760\u001b[1;33m                 \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"iprint\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0miprint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"gtol\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"maxiter\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    761\u001b[0m             )\n\u001b[0;32m    762\u001b[0m             n_iter_i = _check_optimize_result(\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'l-bfgs-b'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m--> 618\u001b[1;33m                                 callback=callback, **options)\n\u001b[0m\u001b[0;32m    619\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[1;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnfev\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;34m\"\"\" returns the the function value \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_logistic_loss_and_grad\u001b[1;34m(w, X, y, alpha, sample_weight)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m     \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_intercept_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_intercept_dot\u001b[1;34m(w, X, y)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m     \u001b[0myz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_imbalanced = dataset.drop(\"label_spam\",axis = 1).values\n",
    "y_imbalanced = dataset[\"label_spam\"].values\n",
    "X_train_imbalanced,X_test_imbalanced, y_train_imbalanced, y_test_imbalanced = train_test_split(X_imbalanced,y_imbalanced,test_size = 0.2, random_state = 1)\n",
    "\n",
    "kf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "\n",
    "for train_index,test_index in kf.split(X_imbalanced):\n",
    "    X_train_imbalanced,X_test_imbalanced=X_imbalanced[train_index],X_imbalanced[test_index]\n",
    "    y_train_imbalanced,y_test_imbalanced=y_imbalanced[train_index],y_imbalanced[test_index]\n",
    "    \n",
    "    logistic_regression_model_imbalanced = LogisticRegression(max_iter = 3000)\n",
    "    logistic_regression_model_imbalanced.fit(X_train_imbalanced,y_train_imbalanced)\n",
    "    prediction_imbalanced = logistic_regression_model_imbalanced.predict(X_test_imbalanced)\n",
    "    \n",
    "    conf_matrix_imbalanced = confusion_matrix(y_true=y_test_imbalanced,y_pred=prediction_imbalanced)\n",
    "    TP_imbalanced = conf_matrix_imbalanced[1,1]\n",
    "    TN_imbalanced = conf_matrix_imbalanced[0,0]\n",
    "    FP_imbalanced = conf_matrix_imbalanced[0,1]\n",
    "    FN_imbalanced = conf_matrix_imbalanced[1,0]\n",
    "    sensitivity_imbalanced = TP_imbalanced/(TP_imbalanced+FN_imbalanced)\n",
    "    specificity_imbalanced = TN_imbalanced/(TN_imbalanced+FP_imbalanced)\n",
    "    accuracy_imbalanced = accuracy_score(y_test_imbalanced,prediction_imbalanced)*100\n",
    "    f1_imbalanced = f1_score(y_test_imbalanced, prediction_imbalanced)\n",
    "    precision_imbalanced = precision_score(y_test_imbalanced, prediction_imbalanced)\n",
    "    recall_imbalanced = recall_score(y_test_imbalanced, prediction_imbalanced)\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy_imbalanced)\n",
    "    print(\"F1 Score: \", f1_imbalanced)\n",
    "    print(\"Precision: \", precision_imbalanced)\n",
    "    print(\"Recall: \", recall_imbalanced)\n",
    "    print(\"Specificity: \", specificity_imbalanced)\n",
    "    print(\"Sensitivity: \", sensitivity_imbalanced)\n",
    "    print(\"True positive: \", TP_imbalanced)\n",
    "    print(\"False positive: \", FP_imbalanced)\n",
    "    print(\"True negative: \", TN_imbalanced)\n",
    "    print(\"False negative: \", FN_imbalanced)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  92.8374655647383\n",
      "F1 Score:  0.9252873563218391\n",
      "Precision:  0.930635838150289\n",
      "Recall:  0.92\n",
      "Specificity:  0.9361702127659575\n",
      "Sensitivity:  0.92\n",
      "True positive:  161\n",
      "False positive:  12\n",
      "True negative:  176\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  92.8374655647383\n",
      "F1 Score:  0.9269662921348314\n",
      "Precision:  0.9322033898305084\n",
      "Recall:  0.9217877094972067\n",
      "Specificity:  0.9347826086956522\n",
      "Sensitivity:  0.9217877094972067\n",
      "True positive:  165\n",
      "False positive:  12\n",
      "True negative:  172\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  89.25619834710744\n",
      "F1 Score:  0.8876080691642652\n",
      "Precision:  0.9005847953216374\n",
      "Recall:  0.875\n",
      "Specificity:  0.9090909090909091\n",
      "Sensitivity:  0.875\n",
      "True positive:  154\n",
      "False positive:  17\n",
      "True negative:  170\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  94.21487603305785\n",
      "F1 Score:  0.9405099150141643\n",
      "Precision:  0.9431818181818182\n",
      "Recall:  0.9378531073446328\n",
      "Specificity:  0.946236559139785\n",
      "Sensitivity:  0.9378531073446328\n",
      "True positive:  166\n",
      "False positive:  10\n",
      "True negative:  176\n",
      "False negative:  11\n",
      "-------------\n",
      "Accuracy:  93.66391184573003\n",
      "F1 Score:  0.9355742296918768\n",
      "Precision:  0.9653179190751445\n",
      "Recall:  0.907608695652174\n",
      "Specificity:  0.9664804469273743\n",
      "Sensitivity:  0.907608695652174\n",
      "True positive:  167\n",
      "False positive:  6\n",
      "True negative:  173\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  91.18457300275482\n",
      "F1 Score:  0.9106145251396649\n",
      "Precision:  0.9261363636363636\n",
      "Recall:  0.8956043956043956\n",
      "Specificity:  0.9281767955801105\n",
      "Sensitivity:  0.8956043956043956\n",
      "True positive:  163\n",
      "False positive:  13\n",
      "True negative:  168\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.646408839779\n",
      "F1 Score:  0.9399477806788512\n",
      "Precision:  0.9424083769633508\n",
      "Recall:  0.9375\n",
      "Specificity:  0.9352941176470588\n",
      "Sensitivity:  0.9375\n",
      "True positive:  180\n",
      "False positive:  11\n",
      "True negative:  159\n",
      "False negative:  12\n",
      "-------------\n",
      "Accuracy:  93.37016574585635\n",
      "F1 Score:  0.9393939393939393\n",
      "Precision:  0.9441624365482234\n",
      "Recall:  0.9346733668341709\n",
      "Specificity:  0.9325153374233128\n",
      "Sensitivity:  0.9346733668341709\n",
      "True positive:  186\n",
      "False positive:  11\n",
      "True negative:  152\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  93.37016574585635\n",
      "F1 Score:  0.9298245614035088\n",
      "Precision:  0.9085714285714286\n",
      "Recall:  0.9520958083832335\n",
      "Specificity:  0.9179487179487179\n",
      "Sensitivity:  0.9520958083832335\n",
      "True positive:  159\n",
      "False positive:  16\n",
      "True negative:  179\n",
      "False negative:  8\n",
      "-------------\n",
      "Accuracy:  92.5414364640884\n",
      "F1 Score:  0.9247910863509751\n",
      "Precision:  0.9378531073446328\n",
      "Recall:  0.9120879120879121\n",
      "Specificity:  0.9388888888888889\n",
      "Sensitivity:  0.9120879120879121\n",
      "True positive:  166\n",
      "False positive:  11\n",
      "True negative:  169\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  93.38842975206612\n",
      "F1 Score:  0.9351351351351351\n",
      "Precision:  0.9664804469273743\n",
      "Recall:  0.9057591623036649\n",
      "Specificity:  0.9651162790697675\n",
      "Sensitivity:  0.9057591623036649\n",
      "True positive:  173\n",
      "False positive:  6\n",
      "True negative:  166\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  91.46005509641874\n",
      "F1 Score:  0.9173333333333333\n",
      "Precision:  0.9197860962566845\n",
      "Recall:  0.9148936170212766\n",
      "Specificity:  0.9142857142857143\n",
      "Sensitivity:  0.9148936170212766\n",
      "True positive:  172\n",
      "False positive:  15\n",
      "True negative:  160\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  93.66391184573003\n",
      "F1 Score:  0.9317507418397627\n",
      "Precision:  0.9345238095238095\n",
      "Recall:  0.9289940828402367\n",
      "Specificity:  0.9432989690721649\n",
      "Sensitivity:  0.9289940828402367\n",
      "True positive:  157\n",
      "False positive:  11\n",
      "True negative:  183\n",
      "False negative:  12\n",
      "-------------\n",
      "Accuracy:  93.1129476584022\n",
      "F1 Score:  0.9347258485639687\n",
      "Precision:  0.9572192513368984\n",
      "Recall:  0.9132653061224489\n",
      "Specificity:  0.9520958083832335\n",
      "Sensitivity:  0.9132653061224489\n",
      "True positive:  179\n",
      "False positive:  8\n",
      "True negative:  159\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  91.18457300275482\n",
      "F1 Score:  0.909090909090909\n",
      "Precision:  0.8695652173913043\n",
      "Recall:  0.9523809523809523\n",
      "Specificity:  0.8769230769230769\n",
      "Sensitivity:  0.9523809523809523\n",
      "True positive:  160\n",
      "False positive:  24\n",
      "True negative:  171\n",
      "False negative:  8\n",
      "-------------\n",
      "Accuracy:  92.56198347107438\n",
      "F1 Score:  0.9272237196765498\n",
      "Precision:  0.9197860962566845\n",
      "Recall:  0.9347826086956522\n",
      "Specificity:  0.9162011173184358\n",
      "Sensitivity:  0.9347826086956522\n",
      "True positive:  172\n",
      "False positive:  15\n",
      "True negative:  164\n",
      "False negative:  12\n",
      "-------------\n",
      "Accuracy:  93.37016574585635\n",
      "F1 Score:  0.9325842696629213\n",
      "Precision:  0.9651162790697675\n",
      "Recall:  0.9021739130434783\n",
      "Specificity:  0.9662921348314607\n",
      "Sensitivity:  0.9021739130434783\n",
      "True positive:  166\n",
      "False positive:  6\n",
      "True negative:  172\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  91.71270718232044\n",
      "F1 Score:  0.9157303370786517\n",
      "Precision:  0.9157303370786517\n",
      "Recall:  0.9157303370786517\n",
      "Specificity:  0.9184782608695652\n",
      "Sensitivity:  0.9157303370786517\n",
      "True positive:  163\n",
      "False positive:  15\n",
      "True negative:  169\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  93.646408839779\n",
      "F1 Score:  0.9337175792507205\n",
      "Precision:  0.9418604651162791\n",
      "Recall:  0.9257142857142857\n",
      "Specificity:  0.946524064171123\n",
      "Sensitivity:  0.9257142857142857\n",
      "True positive:  162\n",
      "False positive:  10\n",
      "True negative:  177\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  93.37016574585635\n",
      "F1 Score:  0.9318181818181819\n",
      "Precision:  0.9534883720930233\n",
      "Recall:  0.9111111111111111\n",
      "Specificity:  0.9560439560439561\n",
      "Sensitivity:  0.9111111111111111\n",
      "True positive:  164\n",
      "False positive:  8\n",
      "True negative:  174\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  90.633608815427\n",
      "F1 Score:  0.9022988505747127\n",
      "Precision:  0.9289940828402367\n",
      "Recall:  0.8770949720670391\n",
      "Specificity:  0.9347826086956522\n",
      "Sensitivity:  0.8770949720670391\n",
      "True positive:  157\n",
      "False positive:  12\n",
      "True negative:  172\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  91.18457300275482\n",
      "F1 Score:  0.9069767441860466\n",
      "Precision:  0.9122807017543859\n",
      "Recall:  0.9017341040462428\n",
      "Specificity:  0.9210526315789473\n",
      "Sensitivity:  0.9017341040462428\n",
      "True positive:  156\n",
      "False positive:  15\n",
      "True negative:  175\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  94.21487603305785\n",
      "F1 Score:  0.9462915601023019\n",
      "Precision:  0.9390862944162437\n",
      "Recall:  0.9536082474226805\n",
      "Specificity:  0.9289940828402367\n",
      "Sensitivity:  0.9536082474226805\n",
      "True positive:  185\n",
      "False positive:  12\n",
      "True negative:  157\n",
      "False negative:  9\n",
      "-------------\n",
      "Accuracy:  92.01101928374655\n",
      "F1 Score:  0.9205479452054794\n",
      "Precision:  0.9281767955801105\n",
      "Recall:  0.9130434782608695\n",
      "Specificity:  0.9273743016759777\n",
      "Sensitivity:  0.9130434782608695\n",
      "True positive:  168\n",
      "False positive:  13\n",
      "True negative:  166\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  90.35812672176309\n",
      "F1 Score:  0.8985507246376812\n",
      "Precision:  0.8908045977011494\n",
      "Recall:  0.9064327485380117\n",
      "Specificity:  0.9010416666666666\n",
      "Sensitivity:  0.9064327485380117\n",
      "True positive:  155\n",
      "False positive:  19\n",
      "True negative:  173\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  93.38842975206612\n",
      "F1 Score:  0.9344262295081968\n",
      "Precision:  0.9193548387096774\n",
      "Recall:  0.95\n",
      "Specificity:  0.9180327868852459\n",
      "Sensitivity:  0.95\n",
      "True positive:  171\n",
      "False positive:  15\n",
      "True negative:  168\n",
      "False negative:  9\n",
      "-------------\n",
      "Accuracy:  91.9889502762431\n",
      "F1 Score:  0.9226666666666669\n",
      "Precision:  0.9202127659574468\n",
      "Recall:  0.9251336898395722\n",
      "Specificity:  0.9142857142857143\n",
      "Sensitivity:  0.9251336898395722\n",
      "True positive:  173\n",
      "False positive:  15\n",
      "True negative:  160\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  93.646408839779\n",
      "F1 Score:  0.9373297002724795\n",
      "Precision:  0.945054945054945\n",
      "Recall:  0.9297297297297298\n",
      "Specificity:  0.943502824858757\n",
      "Sensitivity:  0.9297297297297298\n",
      "True positive:  172\n",
      "False positive:  10\n",
      "True negative:  167\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  93.646408839779\n",
      "F1 Score:  0.9352112676056339\n",
      "Precision:  0.9651162790697675\n",
      "Recall:  0.907103825136612\n",
      "Specificity:  0.9664804469273743\n",
      "Sensitivity:  0.907103825136612\n",
      "True positive:  166\n",
      "False positive:  6\n",
      "True negative:  173\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  92.5414364640884\n",
      "F1 Score:  0.9217391304347826\n",
      "Precision:  0.9464285714285714\n",
      "Recall:  0.8983050847457628\n",
      "Specificity:  0.9513513513513514\n",
      "Sensitivity:  0.8983050847457628\n",
      "True positive:  159\n",
      "False positive:  9\n",
      "True negative:  176\n",
      "False negative:  18\n",
      "-------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  92.01101928374655\n",
      "F1 Score:  0.9201101928374656\n",
      "Precision:  0.9175824175824175\n",
      "Recall:  0.9226519337016574\n",
      "Specificity:  0.9175824175824175\n",
      "Sensitivity:  0.9226519337016574\n",
      "True positive:  167\n",
      "False positive:  15\n",
      "True negative:  167\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  92.56198347107438\n",
      "F1 Score:  0.9221902017291066\n",
      "Precision:  0.9195402298850575\n",
      "Recall:  0.9248554913294798\n",
      "Specificity:  0.9263157894736842\n",
      "Sensitivity:  0.9248554913294798\n",
      "True positive:  160\n",
      "False positive:  14\n",
      "True negative:  176\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  94.21487603305785\n",
      "F1 Score:  0.9433962264150944\n",
      "Precision:  0.9408602150537635\n",
      "Recall:  0.9459459459459459\n",
      "Specificity:  0.9382022471910112\n",
      "Sensitivity:  0.9459459459459459\n",
      "True positive:  175\n",
      "False positive:  11\n",
      "True negative:  167\n",
      "False negative:  10\n",
      "-------------\n",
      "Accuracy:  92.8374655647383\n",
      "F1 Score:  0.9226190476190477\n",
      "Precision:  0.9281437125748503\n",
      "Recall:  0.9171597633136095\n",
      "Specificity:  0.9381443298969072\n",
      "Sensitivity:  0.9171597633136095\n",
      "True positive:  155\n",
      "False positive:  12\n",
      "True negative:  182\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  92.28650137741047\n",
      "F1 Score:  0.9267015706806283\n",
      "Precision:  0.9516129032258065\n",
      "Recall:  0.9030612244897959\n",
      "Specificity:  0.9461077844311377\n",
      "Sensitivity:  0.9030612244897959\n",
      "True positive:  177\n",
      "False positive:  9\n",
      "True negative:  158\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.66391184573003\n",
      "F1 Score:  0.9386666666666668\n",
      "Precision:  0.9513513513513514\n",
      "Recall:  0.9263157894736842\n",
      "Specificity:  0.9479768786127167\n",
      "Sensitivity:  0.9263157894736842\n",
      "True positive:  176\n",
      "False positive:  9\n",
      "True negative:  164\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  91.16022099447514\n",
      "F1 Score:  0.9120879120879122\n",
      "Precision:  0.9273743016759777\n",
      "Recall:  0.8972972972972973\n",
      "Specificity:  0.9265536723163842\n",
      "Sensitivity:  0.8972972972972973\n",
      "True positive:  166\n",
      "False positive:  13\n",
      "True negative:  164\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.646408839779\n",
      "F1 Score:  0.9309309309309309\n",
      "Precision:  0.9451219512195121\n",
      "Recall:  0.9171597633136095\n",
      "Specificity:  0.9533678756476683\n",
      "Sensitivity:  0.9171597633136095\n",
      "True positive:  155\n",
      "False positive:  9\n",
      "True negative:  184\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  92.26519337016575\n",
      "F1 Score:  0.9243243243243244\n",
      "Precision:  0.9447513812154696\n",
      "Recall:  0.9047619047619048\n",
      "Specificity:  0.9421965317919075\n",
      "Sensitivity:  0.9047619047619048\n",
      "True positive:  171\n",
      "False positive:  10\n",
      "True negative:  163\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  91.71270718232044\n",
      "F1 Score:  0.9157303370786517\n",
      "Precision:  0.9055555555555556\n",
      "Recall:  0.9261363636363636\n",
      "Specificity:  0.9086021505376344\n",
      "Sensitivity:  0.9261363636363636\n",
      "True positive:  163\n",
      "False positive:  17\n",
      "True negative:  169\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  90.9090909090909\n",
      "F1 Score:  0.9095890410958904\n",
      "Precision:  0.9222222222222223\n",
      "Recall:  0.8972972972972973\n",
      "Specificity:  0.9213483146067416\n",
      "Sensitivity:  0.8972972972972973\n",
      "True positive:  166\n",
      "False positive:  14\n",
      "True negative:  164\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.93939393939394\n",
      "F1 Score:  0.9414893617021277\n",
      "Precision:  0.9516129032258065\n",
      "Recall:  0.9315789473684211\n",
      "Specificity:  0.9479768786127167\n",
      "Sensitivity:  0.9315789473684211\n",
      "True positive:  177\n",
      "False positive:  9\n",
      "True negative:  164\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  94.21487603305785\n",
      "F1 Score:  0.9418282548476454\n",
      "Precision:  0.9497206703910615\n",
      "Recall:  0.9340659340659341\n",
      "Specificity:  0.9502762430939227\n",
      "Sensitivity:  0.9340659340659341\n",
      "True positive:  170\n",
      "False positive:  9\n",
      "True negative:  172\n",
      "False negative:  12\n",
      "-------------\n",
      "Accuracy:  93.66391184573003\n",
      "F1 Score:  0.934844192634561\n",
      "Precision:  0.9593023255813954\n",
      "Recall:  0.9116022099447514\n",
      "Specificity:  0.9615384615384616\n",
      "Sensitivity:  0.9116022099447514\n",
      "True positive:  165\n",
      "False positive:  7\n",
      "True negative:  175\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  92.8374655647383\n",
      "F1 Score:  0.9297297297297297\n",
      "Precision:  0.9347826086956522\n",
      "Recall:  0.9247311827956989\n",
      "Specificity:  0.9322033898305084\n",
      "Sensitivity:  0.9247311827956989\n",
      "True positive:  172\n",
      "False positive:  12\n",
      "True negative:  165\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  93.93939393939394\n",
      "F1 Score:  0.9395604395604396\n",
      "Precision:  0.95\n",
      "Recall:  0.9293478260869565\n",
      "Specificity:  0.9497206703910615\n",
      "Sensitivity:  0.9293478260869565\n",
      "True positive:  171\n",
      "False positive:  9\n",
      "True negative:  170\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  92.5414364640884\n",
      "F1 Score:  0.9164086687306502\n",
      "Precision:  0.9079754601226994\n",
      "Recall:  0.925\n",
      "Specificity:  0.9257425742574258\n",
      "Sensitivity:  0.925\n",
      "True positive:  148\n",
      "False positive:  15\n",
      "True negative:  187\n",
      "False negative:  12\n",
      "-------------\n",
      "Accuracy:  88.67403314917127\n",
      "F1 Score:  0.8864265927977839\n",
      "Precision:  0.9142857142857143\n",
      "Recall:  0.8602150537634409\n",
      "Specificity:  0.9147727272727273\n",
      "Sensitivity:  0.8602150537634409\n",
      "True positive:  160\n",
      "False positive:  15\n",
      "True negative:  161\n",
      "False negative:  26\n",
      "-------------\n",
      "Accuracy:  94.1988950276243\n",
      "F1 Score:  0.9454545454545454\n",
      "Precision:  0.9430051813471503\n",
      "Recall:  0.9479166666666666\n",
      "Specificity:  0.9352941176470588\n",
      "Sensitivity:  0.9479166666666666\n",
      "True positive:  182\n",
      "False positive:  11\n",
      "True negative:  159\n",
      "False negative:  10\n",
      "-------------\n",
      "Accuracy:  93.0939226519337\n",
      "F1 Score:  0.927536231884058\n",
      "Precision:  0.898876404494382\n",
      "Recall:  0.9580838323353293\n",
      "Specificity:  0.9076923076923077\n",
      "Sensitivity:  0.9580838323353293\n",
      "True positive:  160\n",
      "False positive:  18\n",
      "True negative:  177\n",
      "False negative:  7\n",
      "-------------\n",
      "Accuracy:  93.1129476584022\n",
      "F1 Score:  0.9299719887955182\n",
      "Precision:  0.9378531073446328\n",
      "Recall:  0.9222222222222223\n",
      "Specificity:  0.9398907103825137\n",
      "Sensitivity:  0.9222222222222223\n",
      "True positive:  166\n",
      "False positive:  11\n",
      "True negative:  172\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  93.38842975206612\n",
      "F1 Score:  0.9306358381502892\n",
      "Precision:  0.936046511627907\n",
      "Recall:  0.9252873563218391\n",
      "Specificity:  0.9417989417989417\n",
      "Sensitivity:  0.9252873563218391\n",
      "True positive:  161\n",
      "False positive:  11\n",
      "True negative:  178\n",
      "False negative:  13\n",
      "-------------\n",
      "Accuracy:  91.73553719008265\n",
      "F1 Score:  0.9152542372881356\n",
      "Precision:  0.9310344827586207\n",
      "Recall:  0.9\n",
      "Specificity:  0.9344262295081968\n",
      "Sensitivity:  0.9\n",
      "True positive:  162\n",
      "False positive:  12\n",
      "True negative:  171\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  90.35812672176309\n",
      "F1 Score:  0.8991354466858789\n",
      "Precision:  0.8813559322033898\n",
      "Recall:  0.9176470588235294\n",
      "Specificity:  0.8911917098445595\n",
      "Sensitivity:  0.9176470588235294\n",
      "True positive:  156\n",
      "False positive:  21\n",
      "True negative:  172\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  95.31680440771349\n",
      "F1 Score:  0.9526462395543176\n",
      "Precision:  0.9606741573033708\n",
      "Recall:  0.9447513812154696\n",
      "Specificity:  0.9615384615384616\n",
      "Sensitivity:  0.9447513812154696\n",
      "True positive:  171\n",
      "False positive:  7\n",
      "True negative:  175\n",
      "False negative:  10\n",
      "-------------\n",
      "Accuracy:  92.01101928374655\n",
      "F1 Score:  0.911854103343465\n",
      "Precision:  0.9316770186335404\n",
      "Recall:  0.8928571428571429\n",
      "Specificity:  0.9435897435897436\n",
      "Sensitivity:  0.8928571428571429\n",
      "True positive:  150\n",
      "False positive:  11\n",
      "True negative:  184\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  91.4364640883978\n",
      "F1 Score:  0.9168900804289545\n",
      "Precision:  0.9193548387096774\n",
      "Recall:  0.9144385026737968\n",
      "Specificity:  0.9142857142857143\n",
      "Sensitivity:  0.9144385026737968\n",
      "True positive:  171\n",
      "False positive:  15\n",
      "True negative:  160\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  90.33149171270718\n",
      "F1 Score:  0.904109589041096\n",
      "Precision:  0.9116022099447514\n",
      "Recall:  0.8967391304347826\n",
      "Specificity:  0.9101123595505618\n",
      "Sensitivity:  0.8967391304347826\n",
      "True positive:  165\n",
      "False positive:  16\n",
      "True negative:  162\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.37016574585635\n",
      "F1 Score:  0.9408866995073892\n",
      "Precision:  0.9597989949748744\n",
      "Recall:  0.9227053140096618\n",
      "Specificity:  0.9483870967741935\n",
      "Sensitivity:  0.9227053140096618\n",
      "True positive:  191\n",
      "False positive:  8\n",
      "True negative:  147\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  93.92265193370166\n",
      "F1 Score:  0.9395604395604396\n",
      "Precision:  0.9395604395604396\n",
      "Recall:  0.9395604395604396\n",
      "Specificity:  0.9388888888888889\n",
      "Sensitivity:  0.9395604395604396\n",
      "True positive:  171\n",
      "False positive:  11\n",
      "True negative:  169\n",
      "False negative:  11\n",
      "-------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  94.49035812672176\n",
      "F1 Score:  0.9470899470899472\n",
      "Precision:  0.93717277486911\n",
      "Recall:  0.9572192513368984\n",
      "Specificity:  0.9318181818181818\n",
      "Sensitivity:  0.9572192513368984\n",
      "True positive:  179\n",
      "False positive:  12\n",
      "True negative:  164\n",
      "False negative:  8\n",
      "-------------\n",
      "Accuracy:  94.76584022038568\n",
      "F1 Score:  0.9449275362318841\n",
      "Precision:  0.9532163742690059\n",
      "Recall:  0.9367816091954023\n",
      "Specificity:  0.9576719576719577\n",
      "Sensitivity:  0.9367816091954023\n",
      "True positive:  163\n",
      "False positive:  8\n",
      "True negative:  181\n",
      "False negative:  11\n",
      "-------------\n",
      "Accuracy:  94.21487603305785\n",
      "F1 Score:  0.9424657534246575\n",
      "Precision:  0.9662921348314607\n",
      "Recall:  0.9197860962566845\n",
      "Specificity:  0.9659090909090909\n",
      "Sensitivity:  0.9197860962566845\n",
      "True positive:  172\n",
      "False positive:  6\n",
      "True negative:  170\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  87.87878787878788\n",
      "F1 Score:  0.8757062146892656\n",
      "Precision:  0.8959537572254336\n",
      "Recall:  0.856353591160221\n",
      "Specificity:  0.9010989010989011\n",
      "Sensitivity:  0.856353591160221\n",
      "True positive:  155\n",
      "False positive:  18\n",
      "True negative:  164\n",
      "False negative:  26\n",
      "-------------\n",
      "Accuracy:  93.1129476584022\n",
      "F1 Score:  0.9240121580547112\n",
      "Precision:  0.9101796407185628\n",
      "Recall:  0.9382716049382716\n",
      "Specificity:  0.9253731343283582\n",
      "Sensitivity:  0.9382716049382716\n",
      "True positive:  152\n",
      "False positive:  15\n",
      "True negative:  186\n",
      "False negative:  10\n",
      "-------------\n",
      "Accuracy:  91.46005509641874\n",
      "F1 Score:  0.9146005509641872\n",
      "Precision:  0.9120879120879121\n",
      "Recall:  0.9171270718232044\n",
      "Specificity:  0.9120879120879121\n",
      "Sensitivity:  0.9171270718232044\n",
      "True positive:  166\n",
      "False positive:  16\n",
      "True negative:  166\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  94.47513812154696\n",
      "F1 Score:  0.9411764705882353\n",
      "Precision:  0.935672514619883\n",
      "Recall:  0.9467455621301775\n",
      "Specificity:  0.9430051813471503\n",
      "Sensitivity:  0.9467455621301775\n",
      "True positive:  160\n",
      "False positive:  11\n",
      "True negative:  182\n",
      "False negative:  9\n",
      "-------------\n",
      "Accuracy:  92.26519337016575\n",
      "F1 Score:  0.9209039548022598\n",
      "Precision:  0.9367816091954023\n",
      "Recall:  0.9055555555555556\n",
      "Specificity:  0.9395604395604396\n",
      "Sensitivity:  0.9055555555555556\n",
      "True positive:  163\n",
      "False positive:  11\n",
      "True negative:  171\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  90.33149171270718\n",
      "F1 Score:  0.906166219839142\n",
      "Precision:  0.9086021505376344\n",
      "Recall:  0.9037433155080213\n",
      "Specificity:  0.9028571428571428\n",
      "Sensitivity:  0.9037433155080213\n",
      "True positive:  169\n",
      "False positive:  17\n",
      "True negative:  158\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  91.9889502762431\n",
      "F1 Score:  0.9273182957393484\n",
      "Precision:  0.9536082474226805\n",
      "Recall:  0.9024390243902439\n",
      "Specificity:  0.9426751592356688\n",
      "Sensitivity:  0.9024390243902439\n",
      "True positive:  185\n",
      "False positive:  9\n",
      "True negative:  148\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  93.38842975206612\n",
      "F1 Score:  0.934065934065934\n",
      "Precision:  0.9444444444444444\n",
      "Recall:  0.9239130434782609\n",
      "Specificity:  0.9441340782122905\n",
      "Sensitivity:  0.9239130434782609\n",
      "True positive:  170\n",
      "False positive:  10\n",
      "True negative:  169\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  94.76584022038568\n",
      "F1 Score:  0.9482288828337876\n",
      "Precision:  0.9405405405405406\n",
      "Recall:  0.9560439560439561\n",
      "Specificity:  0.9392265193370166\n",
      "Sensitivity:  0.9560439560439561\n",
      "True positive:  174\n",
      "False positive:  11\n",
      "True negative:  170\n",
      "False negative:  8\n",
      "-------------\n",
      "Accuracy:  92.28650137741047\n",
      "F1 Score:  0.9204545454545455\n",
      "Precision:  0.9642857142857143\n",
      "Recall:  0.8804347826086957\n",
      "Specificity:  0.9664804469273743\n",
      "Sensitivity:  0.8804347826086957\n",
      "True positive:  162\n",
      "False positive:  6\n",
      "True negative:  173\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  88.42975206611571\n",
      "F1 Score:  0.8793103448275862\n",
      "Precision:  0.864406779661017\n",
      "Recall:  0.8947368421052632\n",
      "Specificity:  0.875\n",
      "Sensitivity:  0.8947368421052632\n",
      "True positive:  153\n",
      "False positive:  24\n",
      "True negative:  168\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  93.93939393939394\n",
      "F1 Score:  0.9364161849710982\n",
      "Precision:  0.9473684210526315\n",
      "Recall:  0.9257142857142857\n",
      "Specificity:  0.9521276595744681\n",
      "Sensitivity:  0.9257142857142857\n",
      "True positive:  162\n",
      "False positive:  9\n",
      "True negative:  179\n",
      "False negative:  13\n",
      "-------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-135-ebdb64641964>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mlogistic_regression_model_undersampled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mlogistic_regression_model_undersampled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_undersampled\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_undersampled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mprediction_undersampled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression_model_undersampled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_undersampled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1415\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1416\u001b[0m                       sample_weight=sample_weight)\n\u001b[1;32m-> 1417\u001b[1;33m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[0;32m   1418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[0;32m    758\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"L-BFGS-B\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 760\u001b[1;33m                 \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"iprint\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0miprint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"gtol\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"maxiter\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    761\u001b[0m             )\n\u001b[0;32m    762\u001b[0m             n_iter_i = _check_optimize_result(\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'l-bfgs-b'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m--> 618\u001b[1;33m                                 callback=callback, **options)\n\u001b[0m\u001b[0;32m    619\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[1;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnfev\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;34m\"\"\" returns the the function value \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin200918\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_logistic_loss_and_grad\u001b[1;34m(w, X, y, alpha, sample_weight)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlog_logistic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m     \u001b[0mz0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_undersampled = df_undersampled.drop(\"label_spam\",axis = 1).values\n",
    "y_undersampled = df_undersampled[\"label_spam\"].values\n",
    "X_train_undersampled,X_test_undersampled, y_train_undersampled, y_test_undersampled = train_test_split(X_undersampled,y_undersampled,test_size = 0.2, random_state = 1)\n",
    "\n",
    "kf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "\n",
    "for train_index,test_index in kf.split(X_undersampled):\n",
    "    X_train_undersampled,X_test_undersampled=X_undersampled[train_index],X_undersampled[test_index]\n",
    "    y_train_undersampled,y_test_undersampled=y_undersampled[train_index],y_undersampled[test_index]\n",
    "    \n",
    "    logistic_regression_model_undersampled = LogisticRegression(max_iter = 3000)\n",
    "    logistic_regression_model_undersampled.fit(X_train_undersampled,y_train_undersampled)\n",
    "    prediction_undersampled = logistic_regression_model_undersampled.predict(X_test_undersampled)\n",
    "    \n",
    "    conf_matrix_undersampled = confusion_matrix(y_true=y_test_undersampled,y_pred=prediction_undersampled)\n",
    "    TP_undersampled = conf_matrix_undersampled[1,1]\n",
    "    TN_undersampled = conf_matrix_undersampled[0,0]\n",
    "    FP_undersampled = conf_matrix_undersampled[0,1]\n",
    "    FN_undersampled = conf_matrix_undersampled[1,0]\n",
    "    sensitivity_undersampled = TP_undersampled/(TP_undersampled+FN_undersampled)\n",
    "    specificity_undersampled = TN_undersampled/(TN_undersampled+FP_undersampled)\n",
    "    accuracy_undersampled = accuracy_score(y_test_undersampled,prediction_undersampled)*100\n",
    "    f1_undersampled = f1_score(y_test_undersampled, prediction_undersampled)\n",
    "    precision_undersampled = precision_score(y_test_undersampled, prediction_undersampled)\n",
    "    recall_undersampled = recall_score(y_test_undersampled, prediction_undersampled)\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy_undersampled)\n",
    "    print(\"F1 Score: \", f1_undersampled)\n",
    "    print(\"Precision: \", precision_undersampled)\n",
    "    print(\"Recall: \", recall_undersampled)\n",
    "    print(\"Specificity: \", specificity_undersampled)\n",
    "    print(\"Sensitivity: \", sensitivity_undersampled)\n",
    "    print(\"True positive: \", TP_undersampled)\n",
    "    print(\"False positive: \", FP_undersampled)\n",
    "    print(\"True negative: \", TN_undersampled)\n",
    "    print(\"False negative: \", FN_undersampled)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  91.39784946236558\n",
      "F1 Score:  0.9124087591240877\n",
      "Precision:  0.8992805755395683\n",
      "Recall:  0.9259259259259259\n",
      "Specificity:  0.9027777777777778\n",
      "Sensitivity:  0.9259259259259259\n",
      "True positive:  250\n",
      "False positive:  28\n",
      "True negative:  260\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  94.6236559139785\n",
      "F1 Score:  0.9477351916376306\n",
      "Precision:  0.9477351916376306\n",
      "Recall:  0.9477351916376306\n",
      "Specificity:  0.9446494464944649\n",
      "Sensitivity:  0.9477351916376306\n",
      "True positive:  272\n",
      "False positive:  15\n",
      "True negative:  256\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  91.93548387096774\n",
      "F1 Score:  0.918918918918919\n",
      "Precision:  0.9239130434782609\n",
      "Recall:  0.9139784946236559\n",
      "Specificity:  0.9247311827956989\n",
      "Sensitivity:  0.9139784946236559\n",
      "True positive:  255\n",
      "False positive:  21\n",
      "True negative:  258\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  93.01075268817203\n",
      "F1 Score:  0.9233791748526522\n",
      "Precision:  0.94\n",
      "Recall:  0.9073359073359073\n",
      "Specificity:  0.9498327759197325\n",
      "Sensitivity:  0.9073359073359073\n",
      "True positive:  235\n",
      "False positive:  15\n",
      "True negative:  284\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  93.9068100358423\n",
      "F1 Score:  0.9417808219178082\n",
      "Precision:  0.935374149659864\n",
      "Recall:  0.9482758620689655\n",
      "Specificity:  0.9291044776119403\n",
      "Sensitivity:  0.9482758620689655\n",
      "True positive:  275\n",
      "False positive:  19\n",
      "True negative:  249\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  91.21863799283155\n",
      "F1 Score:  0.9090909090909091\n",
      "Precision:  0.9315589353612167\n",
      "Recall:  0.8876811594202898\n",
      "Specificity:  0.9361702127659575\n",
      "Sensitivity:  0.8876811594202898\n",
      "True positive:  245\n",
      "False positive:  18\n",
      "True negative:  264\n",
      "False negative:  31\n",
      "-------------\n",
      "Accuracy:  92.99820466786356\n",
      "F1 Score:  0.9307282415630551\n",
      "Precision:  0.9290780141843972\n",
      "Recall:  0.9323843416370107\n",
      "Specificity:  0.927536231884058\n",
      "Sensitivity:  0.9323843416370107\n",
      "True positive:  262\n",
      "False positive:  20\n",
      "True negative:  256\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  94.7935368043088\n",
      "F1 Score:  0.9473684210526316\n",
      "Precision:  0.9525547445255474\n",
      "Recall:  0.9422382671480144\n",
      "Specificity:  0.9535714285714286\n",
      "Sensitivity:  0.9422382671480144\n",
      "True positive:  261\n",
      "False positive:  13\n",
      "True negative:  267\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  91.92100538599641\n",
      "F1 Score:  0.9217391304347825\n",
      "Precision:  0.9330985915492958\n",
      "Recall:  0.9106529209621993\n",
      "Specificity:  0.9285714285714286\n",
      "Sensitivity:  0.9106529209621993\n",
      "True positive:  265\n",
      "False positive:  19\n",
      "True negative:  247\n",
      "False negative:  26\n",
      "-------------\n",
      "Accuracy:  93.17773788150808\n",
      "F1 Score:  0.9304029304029303\n",
      "Precision:  0.9477611940298507\n",
      "Recall:  0.9136690647482014\n",
      "Specificity:  0.9498207885304659\n",
      "Sensitivity:  0.9136690647482014\n",
      "True positive:  254\n",
      "False positive:  14\n",
      "True negative:  265\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  93.9068100358423\n",
      "F1 Score:  0.9356060606060606\n",
      "Precision:  0.9427480916030534\n",
      "Recall:  0.9285714285714286\n",
      "Specificity:  0.9486301369863014\n",
      "Sensitivity:  0.9285714285714286\n",
      "True positive:  247\n",
      "False positive:  15\n",
      "True negative:  277\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.9068100358423\n",
      "F1 Score:  0.9401408450704226\n",
      "Precision:  0.956989247311828\n",
      "Recall:  0.9238754325259516\n",
      "Specificity:  0.9553903345724907\n",
      "Sensitivity:  0.9238754325259516\n",
      "True positive:  267\n",
      "False positive:  12\n",
      "True negative:  257\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  93.36917562724014\n",
      "F1 Score:  0.9352014010507881\n",
      "Precision:  0.9303135888501742\n",
      "Recall:  0.9401408450704225\n",
      "Specificity:  0.927007299270073\n",
      "Sensitivity:  0.9401408450704225\n",
      "True positive:  267\n",
      "False positive:  20\n",
      "True negative:  254\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  90.86021505376344\n",
      "F1 Score:  0.9057301293900184\n",
      "Precision:  0.8909090909090909\n",
      "Recall:  0.9210526315789473\n",
      "Specificity:  0.8972602739726028\n",
      "Sensitivity:  0.9210526315789473\n",
      "True positive:  245\n",
      "False positive:  30\n",
      "True negative:  262\n",
      "False negative:  21\n",
      "-------------\n",
      "Accuracy:  93.9068100358423\n",
      "F1 Score:  0.9341085271317829\n",
      "Precision:  0.926923076923077\n",
      "Recall:  0.94140625\n",
      "Specificity:  0.9370860927152318\n",
      "Sensitivity:  0.94140625\n",
      "True positive:  241\n",
      "False positive:  19\n",
      "True negative:  283\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  92.47311827956989\n",
      "F1 Score:  0.9273356401384083\n",
      "Precision:  0.9436619718309859\n",
      "Recall:  0.9115646258503401\n",
      "Specificity:  0.9393939393939394\n",
      "Sensitivity:  0.9115646258503401\n",
      "True positive:  268\n",
      "False positive:  16\n",
      "True negative:  248\n",
      "False negative:  26\n",
      "-------------\n",
      "Accuracy:  91.02333931777379\n",
      "F1 Score:  0.9056603773584906\n",
      "Precision:  0.9230769230769231\n",
      "Recall:  0.8888888888888888\n",
      "Specificity:  0.9303135888501742\n",
      "Sensitivity:  0.8888888888888888\n",
      "True positive:  240\n",
      "False positive:  20\n",
      "True negative:  267\n",
      "False negative:  30\n",
      "-------------\n",
      "Accuracy:  92.99820466786356\n",
      "F1 Score:  0.934453781512605\n",
      "Precision:  0.9391891891891891\n",
      "Recall:  0.9297658862876255\n",
      "Specificity:  0.9302325581395349\n",
      "Sensitivity:  0.9297658862876255\n",
      "True positive:  278\n",
      "False positive:  18\n",
      "True negative:  240\n",
      "False negative:  21\n",
      "-------------\n",
      "Accuracy:  93.53680430879713\n",
      "F1 Score:  0.9368421052631579\n",
      "Precision:  0.956989247311828\n",
      "Recall:  0.9175257731958762\n",
      "Specificity:  0.9548872180451128\n",
      "Sensitivity:  0.9175257731958762\n",
      "True positive:  267\n",
      "False positive:  12\n",
      "True negative:  254\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  92.45960502692998\n",
      "F1 Score:  0.9230769230769231\n",
      "Precision:  0.9230769230769231\n",
      "Recall:  0.9230769230769231\n",
      "Specificity:  0.926056338028169\n",
      "Sensitivity:  0.9230769230769231\n",
      "True positive:  252\n",
      "False positive:  21\n",
      "True negative:  263\n",
      "False negative:  21\n",
      "-------------\n",
      "Accuracy:  92.29390681003584\n",
      "F1 Score:  0.9254766031195841\n",
      "Precision:  0.9303135888501742\n",
      "Recall:  0.9206896551724137\n",
      "Specificity:  0.9253731343283582\n",
      "Sensitivity:  0.9206896551724137\n",
      "True positive:  267\n",
      "False positive:  20\n",
      "True negative:  248\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  91.21863799283155\n",
      "F1 Score:  0.9077212806026365\n",
      "Precision:  0.926923076923077\n",
      "Recall:  0.8892988929889298\n",
      "Specificity:  0.9337979094076655\n",
      "Sensitivity:  0.8892988929889298\n",
      "True positive:  241\n",
      "False positive:  19\n",
      "True negative:  268\n",
      "False negative:  30\n",
      "-------------\n",
      "Accuracy:  94.44444444444444\n",
      "F1 Score:  0.9437386569872959\n",
      "Precision:  0.9386281588447654\n",
      "Recall:  0.948905109489051\n",
      "Specificity:  0.9401408450704225\n",
      "Sensitivity:  0.948905109489051\n",
      "True positive:  260\n",
      "False positive:  17\n",
      "True negative:  267\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  91.57706093189965\n",
      "F1 Score:  0.9179755671902269\n",
      "Precision:  0.926056338028169\n",
      "Recall:  0.9100346020761245\n",
      "Specificity:  0.9219330855018587\n",
      "Sensitivity:  0.9100346020761245\n",
      "True positive:  263\n",
      "False positive:  21\n",
      "True negative:  248\n",
      "False negative:  26\n",
      "-------------\n",
      "Accuracy:  94.08602150537635\n",
      "F1 Score:  0.9385474860335195\n",
      "Precision:  0.9618320610687023\n",
      "Recall:  0.9163636363636364\n",
      "Specificity:  0.9646643109540636\n",
      "Sensitivity:  0.9163636363636364\n",
      "True positive:  252\n",
      "False positive:  10\n",
      "True negative:  273\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  92.29390681003584\n",
      "F1 Score:  0.9236234458259325\n",
      "Precision:  0.948905109489051\n",
      "Recall:  0.8996539792387543\n",
      "Specificity:  0.9479553903345725\n",
      "Sensitivity:  0.8996539792387543\n",
      "True positive:  260\n",
      "False positive:  14\n",
      "True negative:  255\n",
      "False negative:  29\n",
      "-------------\n",
      "Accuracy:  91.74147217235189\n",
      "F1 Score:  0.9144981412639405\n",
      "Precision:  0.9077490774907749\n",
      "Recall:  0.9213483146067416\n",
      "Specificity:  0.9137931034482759\n",
      "Sensitivity:  0.9213483146067416\n",
      "True positive:  246\n",
      "False positive:  25\n",
      "True negative:  265\n",
      "False negative:  21\n",
      "-------------\n",
      "Accuracy:  93.53680430879713\n",
      "F1 Score:  0.9357142857142857\n",
      "Precision:  0.9290780141843972\n",
      "Recall:  0.9424460431654677\n",
      "Specificity:  0.9283154121863799\n",
      "Sensitivity:  0.9424460431654677\n",
      "True positive:  262\n",
      "False positive:  20\n",
      "True negative:  259\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  92.99820466786356\n",
      "F1 Score:  0.9271028037383178\n",
      "Precision:  0.9358490566037736\n",
      "Recall:  0.9185185185185185\n",
      "Specificity:  0.9407665505226481\n",
      "Sensitivity:  0.9185185185185185\n",
      "True positive:  248\n",
      "False positive:  17\n",
      "True negative:  270\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  93.89587073608617\n",
      "F1 Score:  0.9407665505226481\n",
      "Precision:  0.9342560553633218\n",
      "Recall:  0.9473684210526315\n",
      "Specificity:  0.9301470588235294\n",
      "Sensitivity:  0.9473684210526315\n",
      "True positive:  270\n",
      "False positive:  19\n",
      "True negative:  253\n",
      "False negative:  15\n",
      "-------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  92.1146953405018\n",
      "F1 Score:  0.9197080291970803\n",
      "Precision:  0.9197080291970803\n",
      "Recall:  0.9197080291970803\n",
      "Specificity:  0.9225352112676056\n",
      "Sensitivity:  0.9197080291970803\n",
      "True positive:  252\n",
      "False positive:  22\n",
      "True negative:  262\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  93.36917562724014\n",
      "F1 Score:  0.9369676320272573\n",
      "Precision:  0.9548611111111112\n",
      "Recall:  0.919732441471572\n",
      "Specificity:  0.9498069498069498\n",
      "Sensitivity:  0.919732441471572\n",
      "True positive:  275\n",
      "False positive:  13\n",
      "True negative:  246\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  93.72759856630825\n",
      "F1 Score:  0.9328214971209214\n",
      "Precision:  0.9274809160305344\n",
      "Recall:  0.9382239382239382\n",
      "Specificity:  0.9364548494983278\n",
      "Sensitivity:  0.9382239382239382\n",
      "True positive:  243\n",
      "False positive:  19\n",
      "True negative:  280\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  92.47311827956989\n",
      "F1 Score:  0.9290540540540541\n",
      "Precision:  0.9581881533101045\n",
      "Recall:  0.9016393442622951\n",
      "Specificity:  0.9525691699604744\n",
      "Sensitivity:  0.9016393442622951\n",
      "True positive:  275\n",
      "False positive:  12\n",
      "True negative:  241\n",
      "False negative:  30\n",
      "-------------\n",
      "Accuracy:  91.75627240143369\n",
      "F1 Score:  0.9154411764705883\n",
      "Precision:  0.9360902255639098\n",
      "Recall:  0.89568345323741\n",
      "Specificity:  0.9392857142857143\n",
      "Sensitivity:  0.89568345323741\n",
      "True positive:  249\n",
      "False positive:  17\n",
      "True negative:  263\n",
      "False negative:  29\n",
      "-------------\n",
      "Accuracy:  92.831541218638\n",
      "F1 Score:  0.9295774647887325\n",
      "Precision:  0.9103448275862069\n",
      "Recall:  0.9496402877697842\n",
      "Specificity:  0.9071428571428571\n",
      "Sensitivity:  0.9496402877697842\n",
      "True positive:  264\n",
      "False positive:  26\n",
      "True negative:  254\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  91.56193895870736\n",
      "F1 Score:  0.91500904159132\n",
      "Precision:  0.9133574007220217\n",
      "Recall:  0.9166666666666666\n",
      "Specificity:  0.9145907473309609\n",
      "Sensitivity:  0.9166666666666666\n",
      "True positive:  253\n",
      "False positive:  24\n",
      "True negative:  257\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  94.43447037701975\n",
      "F1 Score:  0.9457092819614711\n",
      "Precision:  0.9540636042402827\n",
      "Recall:  0.9375\n",
      "Specificity:  0.9516728624535316\n",
      "Sensitivity:  0.9375\n",
      "True positive:  270\n",
      "False positive:  13\n",
      "True negative:  256\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  92.10053859964094\n",
      "F1 Score:  0.9147286821705426\n",
      "Precision:  0.9147286821705426\n",
      "Recall:  0.9147286821705426\n",
      "Specificity:  0.9264214046822743\n",
      "Sensitivity:  0.9147286821705426\n",
      "True positive:  236\n",
      "False positive:  22\n",
      "True negative:  277\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  93.89587073608617\n",
      "F1 Score:  0.9375\n",
      "Precision:  0.940959409594096\n",
      "Recall:  0.9340659340659341\n",
      "Specificity:  0.9436619718309859\n",
      "Sensitivity:  0.9340659340659341\n",
      "True positive:  255\n",
      "False positive:  16\n",
      "True negative:  268\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  92.29390681003584\n",
      "F1 Score:  0.9264957264957264\n",
      "Precision:  0.928082191780822\n",
      "Recall:  0.9249146757679181\n",
      "Specificity:  0.9207547169811321\n",
      "Sensitivity:  0.9249146757679181\n",
      "True positive:  271\n",
      "False positive:  21\n",
      "True negative:  244\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  92.831541218638\n",
      "F1 Score:  0.9270072992700731\n",
      "Precision:  0.9338235294117647\n",
      "Recall:  0.9202898550724637\n",
      "Specificity:  0.9361702127659575\n",
      "Sensitivity:  0.9202898550724637\n",
      "True positive:  254\n",
      "False positive:  18\n",
      "True negative:  264\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  91.75627240143369\n",
      "F1 Score:  0.9195804195804196\n",
      "Precision:  0.9163763066202091\n",
      "Recall:  0.9228070175438596\n",
      "Specificity:  0.9120879120879121\n",
      "Sensitivity:  0.9228070175438596\n",
      "True positive:  263\n",
      "False positive:  24\n",
      "True negative:  249\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  92.831541218638\n",
      "F1 Score:  0.9224806201550387\n",
      "Precision:  0.937007874015748\n",
      "Recall:  0.9083969465648855\n",
      "Specificity:  0.9459459459459459\n",
      "Sensitivity:  0.9083969465648855\n",
      "True positive:  238\n",
      "False positive:  16\n",
      "True negative:  280\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  93.01075268817203\n",
      "F1 Score:  0.9328743545611016\n",
      "Precision:  0.954225352112676\n",
      "Recall:  0.9124579124579124\n",
      "Specificity:  0.9501915708812261\n",
      "Sensitivity:  0.9124579124579124\n",
      "True positive:  271\n",
      "False positive:  13\n",
      "True negative:  248\n",
      "False negative:  26\n",
      "-------------\n",
      "Accuracy:  93.01075268817203\n",
      "F1 Score:  0.9259962049335864\n",
      "Precision:  0.9277566539923955\n",
      "Recall:  0.9242424242424242\n",
      "Specificity:  0.935374149659864\n",
      "Sensitivity:  0.9242424242424242\n",
      "True positive:  244\n",
      "False positive:  19\n",
      "True negative:  275\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  93.35727109515261\n",
      "F1 Score:  0.9278752436647173\n",
      "Precision:  0.9407114624505929\n",
      "Recall:  0.9153846153846154\n",
      "Specificity:  0.9494949494949495\n",
      "Sensitivity:  0.9153846153846154\n",
      "True positive:  238\n",
      "False positive:  15\n",
      "True negative:  282\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  92.28007181328546\n",
      "F1 Score:  0.9262435677530018\n",
      "Precision:  0.9278350515463918\n",
      "Recall:  0.9246575342465754\n",
      "Specificity:  0.9207547169811321\n",
      "Sensitivity:  0.9246575342465754\n",
      "True positive:  270\n",
      "False positive:  21\n",
      "True negative:  244\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  94.97307001795332\n",
      "F1 Score:  0.950530035335689\n",
      "Precision:  0.9607142857142857\n",
      "Recall:  0.9405594405594405\n",
      "Specificity:  0.959409594095941\n",
      "Sensitivity:  0.9405594405594405\n",
      "True positive:  269\n",
      "False positive:  11\n",
      "True negative:  260\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  93.53680430879713\n",
      "F1 Score:  0.9345454545454545\n",
      "Precision:  0.927797833935018\n",
      "Recall:  0.9413919413919414\n",
      "Specificity:  0.9295774647887324\n",
      "Sensitivity:  0.9413919413919414\n",
      "True positive:  257\n",
      "False positive:  20\n",
      "True negative:  264\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  93.36917562724014\n",
      "F1 Score:  0.9330922242314648\n",
      "Precision:  0.9555555555555556\n",
      "Recall:  0.911660777385159\n",
      "Specificity:  0.9563636363636364\n",
      "Sensitivity:  0.911660777385159\n",
      "True positive:  258\n",
      "False positive:  12\n",
      "True negative:  263\n",
      "False negative:  25\n",
      "-------------\n",
      "Accuracy:  94.26523297491039\n",
      "F1 Score:  0.943661971830986\n",
      "Precision:  0.9469964664310954\n",
      "Recall:  0.9403508771929825\n",
      "Specificity:  0.945054945054945\n",
      "Sensitivity:  0.9403508771929825\n",
      "True positive:  268\n",
      "False positive:  15\n",
      "True negative:  258\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  91.75627240143369\n",
      "F1 Score:  0.9169675090252708\n",
      "Precision:  0.9039145907473309\n",
      "Recall:  0.9304029304029304\n",
      "Specificity:  0.9052631578947369\n",
      "Sensitivity:  0.9304029304029304\n",
      "True positive:  254\n",
      "False positive:  27\n",
      "True negative:  258\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  94.08602150537635\n",
      "F1 Score:  0.9433962264150942\n",
      "Precision:  0.9450171821305842\n",
      "Recall:  0.9417808219178082\n",
      "Specificity:  0.9398496240601504\n",
      "Sensitivity:  0.9417808219178082\n",
      "True positive:  275\n",
      "False positive:  16\n",
      "True negative:  250\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  94.26523297491039\n",
      "F1 Score:  0.9379844961240311\n",
      "Precision:  0.937984496124031\n",
      "Recall:  0.937984496124031\n",
      "Specificity:  0.9466666666666667\n",
      "Sensitivity:  0.937984496124031\n",
      "True positive:  242\n",
      "False positive:  16\n",
      "True negative:  284\n",
      "False negative:  16\n",
      "-------------\n",
      "Accuracy:  94.6236559139785\n",
      "F1 Score:  0.9468085106382979\n",
      "Precision:  0.9535714285714286\n",
      "Recall:  0.9401408450704225\n",
      "Specificity:  0.9525547445255474\n",
      "Sensitivity:  0.9401408450704225\n",
      "True positive:  267\n",
      "False positive:  13\n",
      "True negative:  261\n",
      "False negative:  17\n",
      "-------------\n",
      "Accuracy:  91.74147217235189\n",
      "F1 Score:  0.9175627240143369\n",
      "Precision:  0.9045936395759717\n",
      "Recall:  0.9309090909090909\n",
      "Specificity:  0.9042553191489362\n",
      "Sensitivity:  0.9309090909090909\n",
      "True positive:  256\n",
      "False positive:  27\n",
      "True negative:  255\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.35727109515261\n",
      "F1 Score:  0.9360967184801382\n",
      "Precision:  0.9442508710801394\n",
      "Recall:  0.928082191780822\n",
      "Specificity:  0.939622641509434\n",
      "Sensitivity:  0.928082191780822\n",
      "True positive:  271\n",
      "False positive:  16\n",
      "True negative:  249\n",
      "False negative:  21\n",
      "-------------\n",
      "Accuracy:  92.10053859964094\n",
      "F1 Score:  0.9176029962546817\n",
      "Precision:  0.928030303030303\n",
      "Recall:  0.9074074074074074\n",
      "Specificity:  0.9337979094076655\n",
      "Sensitivity:  0.9074074074074074\n",
      "True positive:  245\n",
      "False positive:  19\n",
      "True negative:  268\n",
      "False negative:  25\n",
      "-------------\n",
      "Accuracy:  88.68940754039497\n",
      "F1 Score:  0.88268156424581\n",
      "Precision:  0.9080459770114943\n",
      "Recall:  0.8586956521739131\n",
      "Specificity:  0.9145907473309609\n",
      "Sensitivity:  0.8586956521739131\n",
      "True positive:  237\n",
      "False positive:  24\n",
      "True negative:  257\n",
      "False negative:  39\n",
      "-------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  92.1146953405018\n",
      "F1 Score:  0.9194139194139194\n",
      "Precision:  0.9261992619926199\n",
      "Recall:  0.9127272727272727\n",
      "Specificity:  0.9293286219081273\n",
      "Sensitivity:  0.9127272727272727\n",
      "True positive:  251\n",
      "False positive:  20\n",
      "True negative:  263\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  92.1146953405018\n",
      "F1 Score:  0.92\n",
      "Precision:  0.9440298507462687\n",
      "Recall:  0.8971631205673759\n",
      "Specificity:  0.9456521739130435\n",
      "Sensitivity:  0.8971631205673759\n",
      "True positive:  253\n",
      "False positive:  15\n",
      "True negative:  261\n",
      "False negative:  29\n",
      "-------------\n",
      "Accuracy:  92.831541218638\n",
      "F1 Score:  0.9224806201550387\n",
      "Precision:  0.937007874015748\n",
      "Recall:  0.9083969465648855\n",
      "Specificity:  0.9459459459459459\n",
      "Sensitivity:  0.9083969465648855\n",
      "True positive:  238\n",
      "False positive:  16\n",
      "True negative:  280\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  92.47311827956989\n",
      "F1 Score:  0.9268292682926829\n",
      "Precision:  0.9333333333333333\n",
      "Recall:  0.9204152249134948\n",
      "Specificity:  0.929368029739777\n",
      "Sensitivity:  0.9204152249134948\n",
      "True positive:  266\n",
      "False positive:  19\n",
      "True negative:  250\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  94.44444444444444\n",
      "F1 Score:  0.9451327433628319\n",
      "Precision:  0.9434628975265018\n",
      "Recall:  0.9468085106382979\n",
      "Specificity:  0.9420289855072463\n",
      "Sensitivity:  0.9468085106382979\n",
      "True positive:  267\n",
      "False positive:  16\n",
      "True negative:  260\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  94.80286738351255\n",
      "F1 Score:  0.9488536155202822\n",
      "Precision:  0.9676258992805755\n",
      "Recall:  0.9307958477508651\n",
      "Specificity:  0.966542750929368\n",
      "Sensitivity:  0.9307958477508651\n",
      "True positive:  269\n",
      "False positive:  9\n",
      "True negative:  260\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  94.25493716337523\n",
      "F1 Score:  0.937984496124031\n",
      "Precision:  0.9343629343629344\n",
      "Recall:  0.9416342412451362\n",
      "Specificity:  0.9433333333333334\n",
      "Sensitivity:  0.9416342412451362\n",
      "True positive:  242\n",
      "False positive:  17\n",
      "True negative:  283\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  91.74147217235189\n",
      "F1 Score:  0.917562724014337\n",
      "Precision:  0.920863309352518\n",
      "Recall:  0.9142857142857143\n",
      "Specificity:  0.9205776173285198\n",
      "Sensitivity:  0.9142857142857143\n",
      "True positive:  256\n",
      "False positive:  22\n",
      "True negative:  255\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  92.28007181328546\n",
      "F1 Score:  0.9228007181328547\n",
      "Precision:  0.9244604316546763\n",
      "Recall:  0.921146953405018\n",
      "Specificity:  0.9244604316546763\n",
      "Sensitivity:  0.921146953405018\n",
      "True positive:  257\n",
      "False positive:  21\n",
      "True negative:  257\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  91.92100538599641\n",
      "F1 Score:  0.9235993208828525\n",
      "Precision:  0.918918918918919\n",
      "Recall:  0.9283276450511946\n",
      "Specificity:  0.9090909090909091\n",
      "Sensitivity:  0.9283276450511946\n",
      "True positive:  272\n",
      "False positive:  24\n",
      "True negative:  240\n",
      "False negative:  21\n",
      "-------------\n",
      "Accuracy:  92.29390681003584\n",
      "F1 Score:  0.9211009174311927\n",
      "Precision:  0.9330855018587361\n",
      "Recall:  0.9094202898550725\n",
      "Specificity:  0.9361702127659575\n",
      "Sensitivity:  0.9094202898550725\n",
      "True positive:  251\n",
      "False positive:  18\n",
      "True negative:  264\n",
      "False negative:  25\n",
      "-------------\n",
      "Accuracy:  91.93548387096774\n",
      "F1 Score:  0.9192100538599641\n",
      "Precision:  0.9110320284697508\n",
      "Recall:  0.927536231884058\n",
      "Specificity:  0.9113475177304965\n",
      "Sensitivity:  0.927536231884058\n",
      "True positive:  256\n",
      "False positive:  25\n",
      "True negative:  257\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  92.65232974910394\n",
      "F1 Score:  0.9239332096474954\n",
      "Precision:  0.9188191881918819\n",
      "Recall:  0.9291044776119403\n",
      "Specificity:  0.9241379310344827\n",
      "Sensitivity:  0.9291044776119403\n",
      "True positive:  249\n",
      "False positive:  22\n",
      "True negative:  268\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  93.72759856630825\n",
      "F1 Score:  0.935064935064935\n",
      "Precision:  0.9230769230769231\n",
      "Recall:  0.9473684210526315\n",
      "Specificity:  0.928082191780822\n",
      "Sensitivity:  0.9473684210526315\n",
      "True positive:  252\n",
      "False positive:  21\n",
      "True negative:  271\n",
      "False negative:  14\n",
      "-------------\n",
      "Accuracy:  94.08602150537635\n",
      "F1 Score:  0.9413854351687388\n",
      "Precision:  0.9498207885304659\n",
      "Recall:  0.9330985915492958\n",
      "Specificity:  0.948905109489051\n",
      "Sensitivity:  0.9330985915492958\n",
      "True positive:  265\n",
      "False positive:  14\n",
      "True negative:  260\n",
      "False negative:  19\n",
      "-------------\n",
      "Accuracy:  92.29390681003584\n",
      "F1 Score:  0.9262435677530018\n",
      "Precision:  0.9574468085106383\n",
      "Recall:  0.8970099667774086\n",
      "Specificity:  0.953307392996109\n",
      "Sensitivity:  0.8970099667774086\n",
      "True positive:  270\n",
      "False positive:  12\n",
      "True negative:  245\n",
      "False negative:  31\n",
      "-------------\n",
      "Accuracy:  92.28007181328546\n",
      "F1 Score:  0.9211009174311927\n",
      "Precision:  0.916058394160584\n",
      "Recall:  0.9261992619926199\n",
      "Specificity:  0.9195804195804196\n",
      "Sensitivity:  0.9261992619926199\n",
      "True positive:  251\n",
      "False positive:  23\n",
      "True negative:  263\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  93.35727109515261\n",
      "F1 Score:  0.9330922242314647\n",
      "Precision:  0.945054945054945\n",
      "Recall:  0.9214285714285714\n",
      "Specificity:  0.9458483754512635\n",
      "Sensitivity:  0.9214285714285714\n",
      "True positive:  258\n",
      "False positive:  15\n",
      "True negative:  262\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  92.99820466786356\n",
      "F1 Score:  0.9307282415630549\n",
      "Precision:  0.9390681003584229\n",
      "Recall:  0.9225352112676056\n",
      "Specificity:  0.9377289377289377\n",
      "Sensitivity:  0.9225352112676056\n",
      "True positive:  262\n",
      "False positive:  17\n",
      "True negative:  256\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  93.71633752244165\n",
      "F1 Score:  0.9369369369369369\n",
      "Precision:  0.9523809523809523\n",
      "Recall:  0.9219858156028369\n",
      "Specificity:  0.9527272727272728\n",
      "Sensitivity:  0.9219858156028369\n",
      "True positive:  260\n",
      "False positive:  13\n",
      "True negative:  262\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  93.54838709677419\n",
      "F1 Score:  0.9333333333333335\n",
      "Precision:  0.9402985074626866\n",
      "Recall:  0.9264705882352942\n",
      "Specificity:  0.9440559440559441\n",
      "Sensitivity:  0.9264705882352942\n",
      "True positive:  252\n",
      "False positive:  16\n",
      "True negative:  270\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  93.36917562724014\n",
      "F1 Score:  0.928709055876686\n",
      "Precision:  0.9450980392156862\n",
      "Recall:  0.9128787878787878\n",
      "Specificity:  0.9523809523809523\n",
      "Sensitivity:  0.9128787878787878\n",
      "True positive:  241\n",
      "False positive:  14\n",
      "True negative:  280\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  92.1146953405018\n",
      "F1 Score:  0.9228070175438596\n",
      "Precision:  0.9359430604982206\n",
      "Recall:  0.9100346020761245\n",
      "Specificity:  0.9330855018587361\n",
      "Sensitivity:  0.9100346020761245\n",
      "True positive:  263\n",
      "False positive:  18\n",
      "True negative:  251\n",
      "False negative:  26\n",
      "-------------\n",
      "Accuracy:  93.72759856630825\n",
      "F1 Score:  0.9369369369369368\n",
      "Precision:  0.9285714285714286\n",
      "Recall:  0.9454545454545454\n",
      "Specificity:  0.9293286219081273\n",
      "Sensitivity:  0.9454545454545454\n",
      "True positive:  260\n",
      "False positive:  20\n",
      "True negative:  263\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  91.93548387096774\n",
      "F1 Score:  0.9203539823008849\n",
      "Precision:  0.9252669039145908\n",
      "Recall:  0.9154929577464789\n",
      "Specificity:  0.9233576642335767\n",
      "Sensitivity:  0.9154929577464789\n",
      "True positive:  260\n",
      "False positive:  21\n",
      "True negative:  253\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  92.831541218638\n",
      "F1 Score:  0.9315068493150686\n",
      "Precision:  0.9411764705882353\n",
      "Recall:  0.9220338983050848\n",
      "Specificity:  0.935361216730038\n",
      "Sensitivity:  0.9220338983050848\n",
      "True positive:  272\n",
      "False positive:  17\n",
      "True negative:  246\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  90.30520646319569\n",
      "F1 Score:  0.9032258064516129\n",
      "Precision:  0.8904593639575972\n",
      "Recall:  0.9163636363636364\n",
      "Specificity:  0.8900709219858156\n",
      "Sensitivity:  0.9163636363636364\n",
      "True positive:  252\n",
      "False positive:  31\n",
      "True negative:  251\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  95.15260323159784\n",
      "F1 Score:  0.9497206703910613\n",
      "Precision:  0.9550561797752809\n",
      "Recall:  0.9444444444444444\n",
      "Specificity:  0.9581881533101045\n",
      "Sensitivity:  0.9444444444444444\n",
      "True positive:  255\n",
      "False positive:  12\n",
      "True negative:  275\n",
      "False negative:  15\n",
      "-------------\n",
      "Accuracy:  92.99820466786356\n",
      "F1 Score:  0.9357495881383856\n",
      "Precision:  0.9403973509933775\n",
      "Recall:  0.9311475409836065\n",
      "Specificity:  0.9285714285714286\n",
      "Sensitivity:  0.9311475409836065\n",
      "True positive:  284\n",
      "False positive:  18\n",
      "True negative:  234\n",
      "False negative:  21\n",
      "-------------\n",
      "Accuracy:  92.99820466786356\n",
      "F1 Score:  0.9236790606653621\n",
      "Precision:  0.9365079365079365\n",
      "Recall:  0.9111969111969112\n",
      "Specificity:  0.9463087248322147\n",
      "Sensitivity:  0.9111969111969112\n",
      "True positive:  236\n",
      "False positive:  16\n",
      "True negative:  282\n",
      "False negative:  23\n",
      "-------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  91.75627240143369\n",
      "F1 Score:  0.915129151291513\n",
      "Precision:  0.9117647058823529\n",
      "Recall:  0.9185185185185185\n",
      "Specificity:  0.9166666666666666\n",
      "Sensitivity:  0.9185185185185185\n",
      "True positive:  248\n",
      "False positive:  24\n",
      "True negative:  264\n",
      "False negative:  22\n",
      "-------------\n",
      "Accuracy:  90.68100358422939\n",
      "F1 Score:  0.9040590405904059\n",
      "Precision:  0.8844765342960289\n",
      "Recall:  0.9245283018867925\n",
      "Specificity:  0.8907849829351536\n",
      "Sensitivity:  0.9245283018867925\n",
      "True positive:  245\n",
      "False positive:  32\n",
      "True negative:  261\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  93.36917562724014\n",
      "F1 Score:  0.937394247038917\n",
      "Precision:  0.9358108108108109\n",
      "Recall:  0.9389830508474576\n",
      "Specificity:  0.9277566539923955\n",
      "Sensitivity:  0.9389830508474576\n",
      "True positive:  277\n",
      "False positive:  19\n",
      "True negative:  244\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  94.26523297491039\n",
      "F1 Score:  0.9372549019607843\n",
      "Precision:  0.9637096774193549\n",
      "Recall:  0.9122137404580153\n",
      "Specificity:  0.9695945945945946\n",
      "Sensitivity:  0.9122137404580153\n",
      "True positive:  239\n",
      "False positive:  9\n",
      "True negative:  287\n",
      "False negative:  23\n",
      "-------------\n",
      "Accuracy:  92.831541218638\n",
      "F1 Score:  0.9267399267399268\n",
      "Precision:  0.9547169811320755\n",
      "Recall:  0.900355871886121\n",
      "Specificity:  0.9566787003610109\n",
      "Sensitivity:  0.900355871886121\n",
      "True positive:  253\n",
      "False positive:  12\n",
      "True negative:  265\n",
      "False negative:  28\n",
      "-------------\n",
      "Accuracy:  93.01075268817203\n",
      "F1 Score:  0.9297297297297297\n",
      "Precision:  0.9247311827956989\n",
      "Recall:  0.9347826086956522\n",
      "Specificity:  0.925531914893617\n",
      "Sensitivity:  0.9347826086956522\n",
      "True positive:  258\n",
      "False positive:  21\n",
      "True negative:  261\n",
      "False negative:  18\n",
      "-------------\n",
      "Accuracy:  92.99820466786356\n",
      "F1 Score:  0.9302325581395349\n",
      "Precision:  0.931899641577061\n",
      "Recall:  0.9285714285714286\n",
      "Specificity:  0.9314079422382672\n",
      "Sensitivity:  0.9285714285714286\n",
      "True positive:  260\n",
      "False positive:  19\n",
      "True negative:  258\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  93.71633752244165\n",
      "F1 Score:  0.9380530973451328\n",
      "Precision:  0.9464285714285714\n",
      "Recall:  0.9298245614035088\n",
      "Specificity:  0.9448529411764706\n",
      "Sensitivity:  0.9298245614035088\n",
      "True positive:  265\n",
      "False positive:  15\n",
      "True negative:  257\n",
      "False negative:  20\n",
      "-------------\n",
      "Accuracy:  93.53680430879713\n",
      "F1 Score:  0.9352517985611511\n",
      "Precision:  0.9558823529411765\n",
      "Recall:  0.9154929577464789\n",
      "Specificity:  0.9560439560439561\n",
      "Sensitivity:  0.9154929577464789\n",
      "True positive:  260\n",
      "False positive:  12\n",
      "True negative:  261\n",
      "False negative:  24\n",
      "-------------\n",
      "Accuracy:  92.99820466786356\n",
      "F1 Score:  0.9326424870466321\n",
      "Precision:  0.9342560553633218\n",
      "Recall:  0.9310344827586207\n",
      "Specificity:  0.9288389513108615\n",
      "Sensitivity:  0.9310344827586207\n",
      "True positive:  270\n",
      "False positive:  19\n",
      "True negative:  248\n",
      "False negative:  20\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "X_upsampled = df_upsampled.drop(\"label_spam\",axis = 1).values\n",
    "y_upsampled = df_upsampled[\"label_spam\"].values\n",
    "X_train_upsampled,X_test_upsampled, y_train_upsampled, y_test_upsampled = train_test_split(X_upsampled,y_upsampled,test_size = 0.2, random_state = 1)\n",
    "\n",
    "kf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "\n",
    "for train_index,test_index in kf.split(X_upsampled):\n",
    "    X_train_upsampled,X_test_upsampled=X_upsampled[train_index],X_upsampled[test_index]\n",
    "    y_train_upsampled,y_test_upsampled=y_upsampled[train_index],y_upsampled[test_index]\n",
    "    \n",
    "    logistic_regression_model_upsampled = LogisticRegression(max_iter = 3000)\n",
    "    logistic_regression_model_upsampled.fit(X_train_upsampled,y_train_upsampled)\n",
    "    prediction_upsampled = logistic_regression_model_upsampled.predict(X_test_upsampled)\n",
    "    \n",
    "    conf_matrix_upsampled = confusion_matrix(y_true=y_test_upsampled,y_pred=prediction_upsampled)\n",
    "    TP_upsampled = conf_matrix_upsampled[1,1]\n",
    "    TN_upsampled = conf_matrix_upsampled[0,0]\n",
    "    FP_upsampled = conf_matrix_upsampled[0,1]\n",
    "    FN_upsampled = conf_matrix_upsampled[1,0]\n",
    "    sensitivity_upsampled = TP_upsampled/(TP_upsampled+FN_upsampled)\n",
    "    specificity_upsampled = TN_upsampled/(TN_upsampled+FP_upsampled)\n",
    "    accuracy_upsampled = accuracy_score(y_test_upsampled,prediction_upsampled)*100\n",
    "    f1_upsampled = f1_score(y_test_upsampled, prediction_upsampled)\n",
    "    precision_upsampled = precision_score(y_test_upsampled, prediction_upsampled)\n",
    "    recall_upsampled = recall_score(y_test_upsampled, prediction_upsampled)\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy_upsampled)\n",
    "    print(\"F1 Score: \", f1_upsampled)\n",
    "    print(\"Precision: \", precision_upsampled)\n",
    "    print(\"Recall: \", recall_upsampled)\n",
    "    print(\"Specificity: \", specificity_upsampled)\n",
    "    print(\"Sensitivity: \", sensitivity_upsampled)\n",
    "    print(\"True positive: \", TP_upsampled)\n",
    "    print(\"False positive: \", FP_upsampled)\n",
    "    print(\"True negative: \", TN_upsampled)\n",
    "    print(\"False negative: \", FN_upsampled)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NaiveBayes\n",
    "X_upsampled = df_upsampled.drop(\"label_spam\",axis = 1).values\n",
    "y_upsampled = df_upsampled[\"label_spam\"].values\n",
    "X_train_upsampled,X_test_upsampled, y_train_upsampled, y_test_upsampled = train_test_split(X_upsampled,y_upsampled,test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train_upsampled,y_train_upsampled)\n",
    "prediction_upsampled = model.predict(X_test_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_upsampled = confusion_matrix(y_true=y_test_upsampled,y_pred=prediction_upsampled)\n",
    "TP_upsampled = conf_matrix_upsampled[1,1]\n",
    "TN_upsampled = conf_matrix_upsampled[0,0]\n",
    "FP_upsampled = conf_matrix_upsampled[0,1]\n",
    "FN_upsampled = conf_matrix_upsampled[1,0]\n",
    "sensitivity_upsampled = TP_upsampled/(TP_upsampled+FN_upsampled)\n",
    "specificity_upsampled = TN_upsampled/(TN_upsampled+FP_upsampled)\n",
    "accuracy_upsampled = accuracy_score(y_test_upsampled,prediction_upsampled)*100\n",
    "f1_upsampled = f1_score(y_test_upsampled, prediction_upsampled)\n",
    "precision_upsampled = precision_score(y_test_upsampled, prediction_upsampled)\n",
    "recall_upsampled = recall_score(y_test_upsampled, prediction_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  78.31541218637993\n",
      "F1 Score:  0.7708333333333334\n",
      "Precision:  0.8156312625250501\n",
      "Recall:  0.7307001795332136\n",
      "Specificity:  0.8354203935599285\n",
      "Sensitivity:  0.7307001795332136\n",
      "True positive:  407\n",
      "False positive:  92\n",
      "True negative:  467\n",
      "False negative:  150\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_upsampled)\n",
    "print(\"F1 Score: \", f1_upsampled)\n",
    "print(\"Precision: \", precision_upsampled)\n",
    "print(\"Recall: \", recall_upsampled)\n",
    "print(\"Specificity: \", specificity_upsampled)\n",
    "print(\"Sensitivity: \", sensitivity_upsampled)\n",
    "print(\"True positive: \", TP_upsampled)\n",
    "print(\"False positive: \", FP_upsampled)\n",
    "print(\"True negative: \", TN_upsampled)\n",
    "print(\"False negative: \", FN_upsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_upsampled = df_upsampled.drop(\"label_spam\",axis = 1).values\n",
    "y_upsampled = df_upsampled[\"label_spam\"].values\n",
    "X_train_upsampled,X_test_upsampled, y_train_upsampled, y_test_upsampled = train_test_split(X_upsampled,y_upsampled,test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest\n",
    "random_forest = RandomForestClassifier(max_depth=25, random_state=0)\n",
    "random_forest.fit(X_train_upsampled,y_train_upsampled)\n",
    "prediction_upsampled = random_forest.predict(X_test_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_upsampled = confusion_matrix(y_true=y_test_upsampled,y_pred=prediction_upsampled)\n",
    "TP_upsampled = conf_matrix_upsampled[1,1]\n",
    "TN_upsampled = conf_matrix_upsampled[0,0]\n",
    "FP_upsampled = conf_matrix_upsampled[0,1]\n",
    "FN_upsampled = conf_matrix_upsampled[1,0]\n",
    "sensitivity_upsampled = TP_upsampled/(TP_upsampled+FN_upsampled)\n",
    "specificity_upsampled = TN_upsampled/(TN_upsampled+FP_upsampled)\n",
    "accuracy_upsampled = accuracy_score(y_test_upsampled,prediction_upsampled)*100\n",
    "f1_upsampled = f1_score(y_test_upsampled, prediction_upsampled)\n",
    "precision_upsampled = precision_score(y_test_upsampled, prediction_upsampled)\n",
    "recall_upsampled = recall_score(y_test_upsampled, prediction_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  96.95340501792114\n",
      "F1 Score:  0.9699115044247787\n",
      "Precision:  0.956369982547993\n",
      "Recall:  0.9838420107719928\n",
      "Specificity:  0.9552772808586762\n",
      "Sensitivity:  0.9838420107719928\n",
      "True positive:  548\n",
      "False positive:  25\n",
      "True negative:  534\n",
      "False negative:  9\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_upsampled)\n",
    "print(\"F1 Score: \", f1_upsampled)\n",
    "print(\"Precision: \", precision_upsampled)\n",
    "print(\"Recall: \", recall_upsampled)\n",
    "print(\"Specificity: \", specificity_upsampled)\n",
    "print(\"Sensitivity: \", sensitivity_upsampled)\n",
    "print(\"True positive: \", TP_upsampled)\n",
    "print(\"False positive: \", FP_upsampled)\n",
    "print(\"True negative: \", TN_upsampled)\n",
    "print(\"False negative: \", FN_upsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k - NEAREST NEIGHBORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn\n",
    "X_upsampled = df_upsampled.drop(\"label_spam\",axis = 1).values\n",
    "y_upsampled = df_upsampled[\"label_spam\"].values\n",
    "X_train_upsampled,X_test_upsampled, y_train_upsampled, y_test_upsampled = train_test_split(X_upsampled,y_upsampled,test_size = 0.2, random_state = 1)\n",
    "\n",
    "X_train_upsampled, X_test_upsampled, y_train_upsampled, y_test_upsampled = train_test_split(X_upsampled, y_upsampled, test_size=0.20)\n",
    "scaler = StandardScaler()\n",
    "X_train_upsampled = scaler.fit_transform(X_train_upsampled)\n",
    "X_test_upsampled = scaler.transform(X_test_upsampled)\n",
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier.fit(X_train_upsampled,y_train_upsampled)\n",
    "prediction_upsampled = classifier.predict(X_test_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_upsampled = confusion_matrix(y_true=y_test_upsampled,y_pred=prediction_upsampled)\n",
    "TP_upsampled = conf_matrix_upsampled[1,1]\n",
    "TN_upsampled = conf_matrix_upsampled[0,0]\n",
    "FP_upsampled = conf_matrix_upsampled[0,1]\n",
    "FN_upsampled = conf_matrix_upsampled[1,0]\n",
    "sensitivity_upsampled = TP_upsampled/(TP_upsampled+FN_upsampled)\n",
    "specificity_upsampled = TN_upsampled/(TN_upsampled+FP_upsampled)\n",
    "accuracy_upsampled = accuracy_score(y_test_upsampled,prediction_upsampled)*100\n",
    "f1_upsampled = f1_score(y_test_upsampled, prediction_upsampled)\n",
    "precision_upsampled = precision_score(y_test_upsampled, prediction_upsampled)\n",
    "recall_upsampled = recall_score(y_test_upsampled, prediction_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  92.29390681003584\n",
      "F1 Score:  0.9223826714801444\n",
      "Precision:  0.9012345679012346\n",
      "Recall:  0.944547134935305\n",
      "Specificity:  0.9026086956521739\n",
      "Sensitivity:  0.944547134935305\n",
      "True positive:  511\n",
      "False positive:  56\n",
      "True negative:  519\n",
      "False negative:  30\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_upsampled)\n",
    "print(\"F1 Score: \", f1_upsampled)\n",
    "print(\"Precision: \", precision_upsampled)\n",
    "print(\"Recall: \", recall_upsampled)\n",
    "print(\"Specificity: \", specificity_upsampled)\n",
    "print(\"Sensitivity: \", sensitivity_upsampled)\n",
    "print(\"True positive: \", TP_upsampled)\n",
    "print(\"False positive: \", FP_upsampled)\n",
    "print(\"True negative: \", TN_upsampled)\n",
    "print(\"False negative: \", FN_upsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmi za redukciju dimenzionalnosti\n",
    "### RECURSIVE FEATURE ELIMINATION - RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_freq_remove\n",
      "word_freq_000\n",
      "word_freq_george\n",
      "word_freq_cs\n",
      "char_freq_$\n"
     ]
    }
   ],
   "source": [
    "#RFE feature reduction over logistic regression model\n",
    "X = df_undersampled.drop(\"label_spam\", axis = 1)\n",
    "y = df_undersampled[\"label_spam\"]\n",
    "model = LogisticRegression(max_iter = 3000)\n",
    "rfe_model = RFE(model, n_features_to_select = 5)\n",
    "rfe_prediction = rfe_model.fit(X,y)\n",
    "feature_list = rfe_prediction.get_support(indices=True)\n",
    "key_columns = []\n",
    "for feature in feature_list:\n",
    "    print(df_undersampled.columns[feature])\n",
    "    key_columns.append(df_undersampled.columns[feature])\n",
    "key_columns.append(\"label_spam\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_000</th>\n",
       "      <th>word_freq_george</th>\n",
       "      <th>word_freq_cs</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>label_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3795</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4025</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4329</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3437</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_freq_remove  word_freq_000  word_freq_george  word_freq_cs  \\\n",
       "3795               0.0            0.0              0.00           0.0   \n",
       "4025               0.0            0.0              1.03           0.0   \n",
       "4329               0.0            0.0              0.00           0.0   \n",
       "3437               0.0            0.0              0.00           0.0   \n",
       "2371               0.0            0.0              0.00           0.0   \n",
       "\n",
       "      char_freq_$  label_spam  \n",
       "3795          0.0           0  \n",
       "4025          0.0           0  \n",
       "4329          0.0           0  \n",
       "3437          0.0           0  \n",
       "2371          0.0           0  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_remove = []\n",
    "df_rfe = df_undersampled.copy(deep=True)\n",
    "for column in df_rfe.columns:\n",
    "    if column not in key_columns:\n",
    "        columns_to_remove.append(column)\n",
    "        \n",
    "rfe_removed = df_rfe.drop(columns_to_remove, axis=1)\n",
    "rfe_removed.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rfe_removed.drop(\"label_spam\",axis = 1).values\n",
    "y = rfe_removed[\"label_spam\"].values\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 1)\n",
    "\n",
    "rfe_logistic_regression_model = LogisticRegression(max_iter = 3000)\n",
    "rfe_logistic_regression_model.fit(X_train,y_train)\n",
    "rfe_prediction = rfe_logistic_regression_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_true=y_test,y_pred=rfe_prediction)\n",
    "TP = conf_matrix[1,1]\n",
    "TN = conf_matrix[0,0]\n",
    "FP = conf_matrix[0,1]\n",
    "FN = conf_matrix[1,0]\n",
    "sensitivity = TP/(TP+FN)\n",
    "specificity = TN/(TN+FP)\n",
    "accuracy = accuracy_score(y_test,rfe_prediction)*100\n",
    "f1 = f1_score(y_test, rfe_prediction)\n",
    "precision = precision_score(y_test, rfe_prediction)\n",
    "recall = recall_score(y_test, rfe_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  83.60881542699724\n",
      "F1 Score:  0.8120063191153238\n",
      "Precision:  0.921146953405018\n",
      "Recall:  0.7259887005649718\n",
      "Specificity:  0.9408602150537635\n",
      "Sensitivity:  0.7259887005649718\n",
      "True positive:  257\n",
      "False positive:  22\n",
      "True negative:  350\n",
      "False negative:  97\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"F1 Score: \", f1)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"Specificity: \", specificity)\n",
    "print(\"Sensitivity: \", sensitivity)\n",
    "print(\"True positive: \", TP)\n",
    "print(\"False positive: \", FP)\n",
    "print(\"True negative: \", TN)\n",
    "print(\"False negative: \", FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_freq_free\n",
      "word_freq_you\n",
      "word_freq_your\n",
      "word_freq_hp\n",
      "word_freq_george\n"
     ]
    }
   ],
   "source": [
    "#Lasso\n",
    "X = df_undersampled.drop(\"label_spam\", axis = 1)\n",
    "y = df_undersampled[\"label_spam\"]\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 1)\n",
    "lasso = LassoCV(cv=3)\n",
    "select_from_model = SelectFromModel(lasso,threshold = 0.01)\n",
    "select_from_model.fit(X, y)\n",
    "feature_list = select_from_model.get_support(indices=True)\n",
    "key_columns = []\n",
    "for feature in feature_list:\n",
    "    print(df_undersampled.columns[feature])\n",
    "    key_columns.append(df_undersampled.columns[feature])\n",
    "    \n",
    "key_columns.append(\"label_spam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_free</th>\n",
       "      <th>word_freq_you</th>\n",
       "      <th>word_freq_your</th>\n",
       "      <th>word_freq_hp</th>\n",
       "      <th>word_freq_george</th>\n",
       "      <th>label_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3795</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4025</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1.03</td>\n",
       "      <td>2.06</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4329</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3437</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_freq_free  word_freq_you  word_freq_your  word_freq_hp  \\\n",
       "3795             0.0           6.00            0.00          0.00   \n",
       "4025             0.0           2.41            1.03          2.06   \n",
       "4329             0.0           2.96            0.00          0.00   \n",
       "3437             0.0           2.00            8.00          0.00   \n",
       "2371             0.0           1.72            0.00          0.00   \n",
       "\n",
       "      word_freq_george  label_spam  \n",
       "3795              0.00           0  \n",
       "4025              1.03           0  \n",
       "4329              0.00           0  \n",
       "3437              0.00           0  \n",
       "2371              0.00           0  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_remove = []\n",
    "df_lasso = df_undersampled.copy(deep=True)\n",
    "for column in df_lasso.columns:\n",
    "    if column not in key_columns:\n",
    "        columns_to_remove.append(column)\n",
    "        \n",
    "lasso_removed = df_lasso.drop(columns_to_remove, axis=1)\n",
    "lasso_removed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lasso_removed.drop(\"label_spam\",axis = 1).values\n",
    "y = lasso_removed[\"label_spam\"].values\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 1)\n",
    "\n",
    "lasso_logistic_regression_model = LogisticRegression(max_iter = 3000)\n",
    "lasso_logistic_regression_model.fit(X_train,y_train)\n",
    "lasso_prediction = lasso_logistic_regression_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_true=y_test,y_pred=lasso_prediction)\n",
    "TP = conf_matrix[1,1]\n",
    "TN = conf_matrix[0,0]\n",
    "FP = conf_matrix[0,1]\n",
    "FN = conf_matrix[1,0]\n",
    "sensitivity = TP/(TP+FN)\n",
    "specificity = TN/(TN+FP)\n",
    "accuracy = accuracy_score(y_test,lasso_prediction)*100\n",
    "f1 = f1_score(y_test, lasso_prediction)\n",
    "precision = precision_score(y_test, lasso_prediction)\n",
    "recall = recall_score(y_test, lasso_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  83.7465564738292\n",
      "F1 Score:  0.8289855072463769\n",
      "Precision:  0.8511904761904762\n",
      "Recall:  0.807909604519774\n",
      "Specificity:  0.8655913978494624\n",
      "Sensitivity:  0.807909604519774\n",
      "True positive:  286\n",
      "False positive:  50\n",
      "True negative:  322\n",
      "False negative:  68\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"F1 Score: \", f1)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"Specificity: \", specificity)\n",
    "print(\"Sensitivity: \", sensitivity)\n",
    "print(\"True positive: \", TP)\n",
    "print(\"False positive: \", FP)\n",
    "print(\"True negative: \", TN)\n",
    "print(\"False negative: \", FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRA TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_freq_make\n",
      "word_freq_address\n",
      "word_freq_all\n",
      "word_freq_3d\n",
      "word_freq_our\n"
     ]
    }
   ],
   "source": [
    "et_classifier = ExtraTreesClassifier()\n",
    "et_classifier.fit(X,y)\n",
    "select_from_model = SelectFromModel(et_classifier, threshold = 0.05)\n",
    "select_from_model.fit(X,y)\n",
    "feature_list = select_from_model.get_support(indices=True)\n",
    "for feature in feature_list:\n",
    "    print(df_undersampled.columns[feature])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRINCIPAL COMPONENT ANALYSIS - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57259956, 0.19844974, 0.11957735, 0.06789583, 0.04147752])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA feature variances\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "pca = PCA()\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "feature_variances = pca.explained_variance_ratio_\n",
    "feature_variances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kombinacija logističke regresije sa PCA algoritmom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA decomposition over LogisticRegression\n",
    "X_upsampled = df_upsampled.drop(\"label_spam\",axis = 1).values\n",
    "y_upsampled = df_upsampled[\"label_spam\"].values\n",
    "X_train_upsampled,X_test_upsampled, y_train, y_test = train_test_split(X_upsampled,y_upsampled,test_size = 0.2, random_state = 1)\n",
    "\n",
    "pca = PCA(n_components = 20)\n",
    "X_train_upsampled = pca.fit_transform(X_train_upsampled)\n",
    "X_test_upsampled = pca.transform(X_test_upsampled)\n",
    "logmodel=LogisticRegression(max_iter = 3000)\n",
    "logmodel.fit(X_train_upsampled, y_train)\n",
    "prediction = logmodel.predict(X_test_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_true=y_test,y_pred=prediction)\n",
    "TP = conf_matrix[1,1]\n",
    "TN = conf_matrix[0,0]\n",
    "FP = conf_matrix[0,1]\n",
    "FN = conf_matrix[1,0]\n",
    "sensitivity = TP/(TP+FN)\n",
    "specificity = TN/(TN+FP)\n",
    "accuracy = accuracy_score(y_test,prediction)*100\n",
    "f1 = f1_score(y_test, prediction)\n",
    "precision = precision_score(y_test, prediction)\n",
    "recall = recall_score(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  90.68100358422939\n",
      "F1 Score:  0.9092495636998256\n",
      "Precision:  0.8845500848896435\n",
      "Recall:  0.9353680430879713\n",
      "Specificity:  0.8783542039355993\n",
      "Sensitivity:  0.9353680430879713\n",
      "True positive:  521\n",
      "False positive:  68\n",
      "True negative:  491\n",
      "False negative:  36\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"F1 Score: \", f1)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"Specificity: \", specificity)\n",
    "print(\"Sensitivity: \", sensitivity)\n",
    "print(\"True positive: \", TP)\n",
    "print(\"False positive: \", FP)\n",
    "print(\"True negative: \", TN)\n",
    "print(\"False negative: \", FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RIDGE REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set errors:\n",
      "0.3324841506999164\n",
      "0.5578170692148967\n",
      "Test set errors:\n",
      "0.32474779391130537\n",
      "0.5781541265670814\n"
     ]
    }
   ],
   "source": [
    "#Ridge\n",
    "#error over train set \n",
    "X_upsampled = df_upsampled.drop(\"label_spam\",axis = 1).values\n",
    "y_upsampled = df_upsampled[\"label_spam\"].values\n",
    "X_train_upsampled,X_test_upsampled, y_train, y_test = train_test_split(X_upsampled,y_upsampled,test_size = 0.2, random_state = 1)\n",
    "\n",
    "model = Ridge(alpha = 0.04, normalize = True)\n",
    "model_fit = model.fit(X_train_upsampled, y_train)\n",
    "prediction_train = model_fit.predict(X_train_upsampled)\n",
    "print(\"Train set errors:\")\n",
    "print(np.sqrt(mean_squared_error(y_train,prediction_train)))\n",
    "print(r2_score(y_train, prediction_train))\n",
    "\n",
    "#error over test set\n",
    "prediction_test = model_fit.predict(X_test_upsampled)\n",
    "print(\"Test set errors:\")\n",
    "print(np.sqrt(mean_squared_error(y_test,prediction_test)))\n",
    "print(r2_score(y_test, prediction_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_upsampled = df_upsampled.drop(\"label_spam\",axis = 1).values\n",
    "y_upsampled = df_upsampled[\"label_spam\"].values\n",
    "X_train_upsampled,X_test_upsampled, y_train, y_test = train_test_split(X_upsampled,y_upsampled,test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train=sc.fit_transform(X_train_upsampled)\n",
    "X_test_upsampled=sc.transform(X_test_upsampled)\n",
    "logmodel=LogisticRegression(max_iter = 3000)\n",
    "y_score = logmodel2.fit(X_train_upsampled, y_train).decision_function(X_test_upsampled)\n",
    "logmodel.fit(X_train_upsampled,y_train)\n",
    "prediction = logmodel.predict(X_test_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(y_test, prediction)\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
